{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7764110b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\flatbuffers\\compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  'hamming': pil_image.HAMMING,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  'box': pil_image.BOX,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  'lanczos': pil_image.LANCZOS,\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e4ab220",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(358)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdd14b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_num_actions = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6470ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.gymlibrary.ml/environments/box2d/lunar_lander/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f235a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OU Noise\n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n",
    "\n",
    "def get_actor(num_states, num_actions=1, upper_bound=1, continuous=True, layer1=400, layer2=300):\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(layer1, activation=\"relu\")(inputs)\n",
    "    out = layers.Dense(layer2, activation=\"relu\")(out)\n",
    "    \n",
    "    # Different output activation based on discrete or continous version\n",
    "    if continuous:\n",
    "        outputs = layers.Dense(num_actions, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "    else:\n",
    "        outputs = layers.Dense(1, activation=\"softmax\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Multiply to fill the whole action space which should be equal around 0\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def get_critic(num_states, num_actions=1, layer1=400, layer2=300):\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(layer1, activation=\"relu\")(concat)\n",
    "    out = layers.Dense(layer2, activation=\"relu\")(out)\n",
    "\n",
    "    outputs = layers.Dense(num_actions)(out)\n",
    "\n",
    "    # Make it into a keras model\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "# This updates the weights in a slow manner which keeps stability\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "3d09a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions=1, lower_bound=1, upper_bound=1, continuous=True,\n",
    "            buffer_capacity=50000, batch_size=64, std_dev=0.2, critic_lr=0.002,\n",
    "            actor_lr=0.001, gamma=0.99, tau=0.005, epsilon=0.2, adam_critic_eps=1e-07,\n",
    "            adam_actor_eps=1e-07):\n",
    "        \n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # For methods\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        self.continuous = continuous\n",
    "\n",
    "        # This is used to make sure we only sample from used buffer space\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "        # have to change type for gradient calculation\n",
    "        self.done_buffer = np.zeros((self.buffer_capacity, 1)).astype(np.float32)\n",
    "        \n",
    "        self.std_dev = std_dev\n",
    "        self.critic_lr = critic_lr\n",
    "        self.actor_lr = actor_lr\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.actor_model = get_actor(num_states, num_actions, upper_bound, continuous=continuous, layer1=400, layer2=300)\n",
    "        self.critic_model = get_critic(num_states, num_actions, layer1=400, layer2=300)\n",
    "\n",
    "        self.target_actor = get_actor(num_states, num_actions, upper_bound, continuous=continuous, layer1=400, layer2=300)\n",
    "        self.target_critic = get_critic(num_states, num_actions, layer1=400, layer2=300)\n",
    "        \n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=critic_lr,beta_1=0.9,beta_2=0.999,epsilon=adam_critic_eps\n",
    "        )\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=actor_lr,beta_1=0.9,beta_2=0.999,epsilon=adam_actor_eps\n",
    "        )\n",
    "        \n",
    "        # Making the weights equal initially\n",
    "        self.target_actor.set_weights(self.actor_model.get_weights())\n",
    "        self.target_critic.set_weights(self.critic_model.get_weights())\n",
    "        \n",
    "        self.ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "    \n",
    "    # Makes a record of the outputted (s,a,r,s') obervation tuple + terminal state\n",
    "    def record(self, obs_tuple):\n",
    "        # Reuse the same buffer replacing old entries\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        \n",
    "        self.done_buffer[index] = obs_tuple[4]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "    \n",
    "    # Move the update and learn function from buffer to Agent to \"decrease\" scope\n",
    "    @tf.function\n",
    "    def update(self, state_batch, action_batch, reward_batch, next_state_batch, done_batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor(next_state_batch, training=True)\n",
    "            # Add done_batch to y function for terminal state\n",
    "            y = reward_batch + done_batch * self.gamma * self.target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = self.critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
    "        \n",
    "        # Gradient clipping to avoid exploding and vanishing gradients\n",
    "        critic_gvd = zip(critic_grad, self.critic_model.trainable_variables)\n",
    "        critic_capped_grad = [(tf.clip_by_global_norm(grad, 1), var) for grad, var in critic_gvd]\n",
    "        \n",
    "        self.critic_optimizer.apply_gradients(critic_capped_grad)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor_model(state_batch, training=True)\n",
    "            critic_value = self.critic_model([state_batch, actions], training=True)\n",
    "\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
    "        # clip actor too\n",
    "        actor_gvd = zip(actor_grad, self.actor_model.trainable_variables)\n",
    "        actor_capped_grad = [(tf.clip_by_global_norm(grad, 1), var) for grad, var in actor_gvd]\n",
    "        \n",
    "        self.actor_optimizer.apply_gradients(actor_capped_grad)\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Sample only valid data\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        # Add done_batch for terminal state\n",
    "        done_batch = tf.convert_to_tensor(self.done_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
    "        \n",
    "    def policy(self, state, noise_object=0, use_noise=True, noise_mult=1):\n",
    "        # Default noise_object to 0 for when it is not needed\n",
    "        # For doing actions without added noise\n",
    "        if not use_noise:    \n",
    "            if self.continuous:\n",
    "                sampled_actions = tf.squeeze(self.actor_model(state)).numpy()\n",
    "                legal_action = np.clip(sampled_actions, self.lower_bound, self.upper_bound)\n",
    "                return [np.squeeze(legal_action)]\n",
    "            else:\n",
    "                sampled_actions = tf.squeeze(self.actor_model(state)).numpy()\n",
    "                legal_action = np.clip(sampled_actions, self.lower_bound, self.upper_bound)\n",
    "                return int(legal_action)\n",
    "        else:\n",
    "            if self.continuous:\n",
    "                sampled_actions = tf.squeeze(self.actor_model(state))\n",
    "                noise = noise_object()\n",
    "                # Adding noise to action\n",
    "                sampled_actions = sampled_actions.numpy() + noise * noise_mult\n",
    "\n",
    "                # We make sure action is within bounds\n",
    "                legal_action = np.clip(sampled_actions, self.lower_bound, self.upper_bound)\n",
    "                \n",
    "                return [np.squeeze(legal_action)]\n",
    "            \n",
    "            else:\n",
    "                if (rng.random() < self.epsilon):\n",
    "                    #random move\n",
    "                    #gets random non-nan index\n",
    "                    return rng.choice(np.arange(0, disc_num_actions, 1, dtype=int))\n",
    "                else:\n",
    "                    sampled_actions = tf.squeeze(self.actor_model(state)).numpy()\n",
    "                    legal_action = np.clip(sampled_actions, self.lower_bound, self.upper_bound)\n",
    "                    return max(0, min(disc_num_actions, int(legal_action)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c3f522c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed(x, episode):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "42f9f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(total_trials=1, total_episodes=100, \n",
    "            buffer_capacity=50000, batch_size=64, std_dev=0.3, critic_lr=0.003, render=False,\n",
    "            actor_lr=0.002, gamma=0.99, tau=0.005, noise_mult=1, save_weights=True, \n",
    "            directory='Weights/', actor_name='actor', critic_name='critic',\n",
    "            gamma_func=fixed, tau_func=fixed, critic_lr_func=fixed, actor_lr_func=fixed,\n",
    "            noise_mult_func=fixed, std_dev_func=fixed, mean_number=20, output=True,\n",
    "            return_rewards=False, total_time=True, use_guide=False, solved=200,\n",
    "            continuous=True, environment='LunarLander-v2', seed=1453, start_steps=0,\n",
    "            gravity=-10.0, enable_wind=False, wind_power=15.0, turbulence_power=1.5,\n",
    "            epsilon=0.2, epsilon_func=fixed, adam_critic_eps=1e-07, adam_actor_eps=1e-07):\n",
    "    tot_time = time.time()\n",
    "    \n",
    "    if environment == 'LunarLander-v2':\n",
    "        env = gym.make(\n",
    "            \"LunarLander-v2\",\n",
    "            continuous=continuous,\n",
    "            gravity=gravity,\n",
    "            enable_wind=enable_wind,\n",
    "            wind_power=wind_power,\n",
    "            turbulence_power=turbulence_power\n",
    "        )\n",
    "    else:\n",
    "        env = gym.make(environment)\n",
    "        \n",
    "    # Apply the seed\n",
    "    _ = env.reset(seed=seed)\n",
    "        \n",
    "    # This is needed to get the input size for the NN\n",
    "    num_states = env.observation_space.low.shape[0]\n",
    "    if continuous:\n",
    "        num_actions = env.action_space.shape[0]\n",
    "    else:\n",
    "        num_actions = 1\n",
    "\n",
    "    # Normalize action space according to https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
    "    action_space = spaces.Box(low=-1, high=1, shape=(num_actions,), dtype='float32')\n",
    "\n",
    "    # This is needed to clip the actions within the legal boundaries\n",
    "    upper_bound = action_space.high[0]\n",
    "    lower_bound = action_space.low[0]\n",
    "    \n",
    "    # To store reward history of each episode\n",
    "    ep_reward_list = []\n",
    "    # To store average reward history of last few episodes\n",
    "    avg_reward_list = []\n",
    "    # To separate assisted reward structures from the \"true\"\n",
    "    true_reward_list = []\n",
    "    true_avg_reward_list = []\n",
    "    \n",
    "    for trial in range(total_trials):\n",
    "        \n",
    "        # Stepcount for random start\n",
    "        step = 0\n",
    "\n",
    "        # add sublists for each trial\n",
    "        avg_reward_list.append([])\n",
    "        ep_reward_list.append([])\n",
    "        \n",
    "        true_reward_list.append([])\n",
    "        true_avg_reward_list.append([])\n",
    "        \n",
    "        agent = Agent(num_states=num_states, num_actions=num_actions, lower_bound=lower_bound, \n",
    "                upper_bound=upper_bound, continuous=continuous, buffer_capacity=buffer_capacity, \n",
    "                batch_size=batch_size, std_dev=std_dev, critic_lr=critic_lr, actor_lr=actor_lr, \n",
    "                gamma=gamma, tau=tau, epsilon=epsilon, adam_critic_eps=adam_critic_eps, adam_actor_eps=adam_actor_eps)\n",
    "\n",
    "        for ep in range(total_episodes):\n",
    "            # functions for different parameters\n",
    "            agent.gamma = gamma_func(gamma, ep)\n",
    "            agent.tau = tau_func(tau, ep)\n",
    "            agent.critic_lr = critic_lr_func(critic_lr, ep)\n",
    "            agent.actor_lr = actor_lr_func(actor_lr, ep)\n",
    "            agent.noise_mult = noise_mult_func(noise_mult, ep)\n",
    "            agent.std_dev = std_dev_func(std_dev, ep)\n",
    "            \n",
    "            agent.epsilon = epsilon_func(epsilon, ep)\n",
    "            \n",
    "            # Used for time benchmarking\n",
    "            before = time.time()\n",
    "\n",
    "            prev_state = env.reset()\n",
    "            episodic_reward = 0\n",
    "            true_reward = 0\n",
    "\n",
    "            while True:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                \n",
    "                tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "                if step >= start_steps:\n",
    "                    action = agent.policy(state=tf_prev_state, noise_object=agent.ou_noise, noise_mult=noise_mult)\n",
    "                    # To get the right format\n",
    "                    if continuous:\n",
    "                        action = action[0]\n",
    "                else:\n",
    "                    action = env.action_space.sample()\n",
    "                \n",
    "                step += 1\n",
    "                \n",
    "                # Recieve state and reward from environment.\n",
    "                state, reward, done, info = env.step(action)\n",
    "                \n",
    "                # Add this before eventual reward modification\n",
    "                true_reward += reward\n",
    "                \n",
    "                # Reward modification\n",
    "                if use_guide:\n",
    "                    # giving penalty for straying far from flags and having high speed\n",
    "                    # x max\n",
    "#                     reward -= int(abs(state[0]) > 0.15) * 2 * abs(state[0])\n",
    "#                     # y top\n",
    "#                     reward -= int(state[1] > 1) * state[1] / 2\n",
    "#                     # horizontal speed\n",
    "#                     reward -= int(abs(state[2]) > 1) * abs(state[2])\n",
    "#                     # down speed\n",
    "#                     reward -= int(state[3] <  -1) * abs(state[3])\n",
    "#                     # up speed\n",
    "#                     reward -= int(state[3] > 0.1) * 3 * state[3]\n",
    "                    reward -= abs(state[2]/2) + abs(state[3]) + (abs(state[0])) + (abs(state[1])/2)\n",
    "\n",
    "                # Add terminal state for when it has landed. Just look at legs on the ground\n",
    "                terminal_state = int(state[6] and state[7])\n",
    "                \n",
    "                agent.record((prev_state, action, reward, state, terminal_state))\n",
    "                episodic_reward += reward\n",
    "\n",
    "                agent.learn()\n",
    "                update_target(agent.target_actor.variables, agent.actor_model.variables, agent.tau)\n",
    "                update_target(agent.target_critic.variables, agent.critic_model.variables, agent.tau)\n",
    "\n",
    "                # End this episode if en episode is done\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                prev_state = state\n",
    "\n",
    "            ep_reward_list[trial].append(episodic_reward)\n",
    "            \n",
    "            true_reward_list[trial].append(true_reward)\n",
    "            \n",
    "            true_avg_reward = np.mean(true_reward_list[trial][-mean_number:])\n",
    "            true_avg_reward_list[trial].append(true_avg_reward)\n",
    "\n",
    "            # Mean of last x episodes\n",
    "            avg_reward = np.mean(ep_reward_list[trial][-mean_number:])\n",
    "            if output:\n",
    "                print(\"Ep {} * AvgReward {:.2f} * true AvgReward {:.2f} * Reward {:.2f} * True Reward {:.2f} * time {:.2f} * step {}\"\n",
    "                  .format(ep, avg_reward, true_avg_reward, episodic_reward, true_reward, (time.time() - before), step))\n",
    "            avg_reward_list[trial].append(avg_reward)\n",
    "            \n",
    "            # stop if avg is solved\n",
    "            if true_avg_reward >= solved:\n",
    "                break\n",
    "\n",
    "        # Save weights naming\n",
    "        now = datetime.datetime.now()\n",
    "        timestamp = \"{}.{}.{}.{}.{}.{}\".format(now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "        save_name = \"{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}\".format(\n",
    "            environment, total_episodes, \n",
    "            buffer_capacity, batch_size, \n",
    "            std_dev, critic_lr, actor_lr, \n",
    "            gamma, tau, noise_mult, \n",
    "            gamma_func.__name__, tau_func.__name__, \n",
    "            critic_lr_func.__name__, actor_lr_func.__name__, \n",
    "            noise_mult_func.__name__, std_dev_func.__name__, \n",
    "            mean_number, use_guide, \n",
    "            solved, continuous, \n",
    "            start_steps, gravity, \n",
    "            enable_wind, wind_power,\n",
    "            turbulence_power, \n",
    "            epsilon, epsilon_func,\n",
    "            timestamp,\n",
    "        )\n",
    "        if save_weights:\n",
    "            try:\n",
    "                agent.actor_model.save_weights(directory + actor_name + '-trial' + str(trial) + '_' + save_name + '.h5')\n",
    "            except:\n",
    "                print('actor save fail')\n",
    "            try:\n",
    "                agent.critic_model.save_weights(directory + critic_name + '-trial' + str(trial) + '_' + save_name + '.h5')\n",
    "            except:\n",
    "                print('critic save fail')\n",
    "    \n",
    "    # Plotting graph\n",
    "    for idx, p in enumerate(true_avg_reward_list):\n",
    "        plt.plot(p, label=str(idx))\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True Avg. Epsiodic Reward (\" + str(mean_number) + \")\")\n",
    "    plt.legend()\n",
    "    plt.savefig('Graphs/' + save_name + '.png')\n",
    "    plt.show()\n",
    "    \n",
    "    print('total time:',time.time() - tot_time, 's')\n",
    "    \n",
    "    # Return to be able to make graphs etc. later, or use the data for other stuff\n",
    "    if return_rewards:\n",
    "        return true_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a57bcf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(total_episodes=10, actor_weights='Weights/actor-trial0.h5', render=False,\n",
    "        environment=\"LunarLander-v2\", continuous=True, gravity=-10.0, enable_wind=False,\n",
    "        wind_power=15.0, turbulence_power=1.5, seed=1453):\n",
    "    rewards = []\n",
    "    \n",
    "    env = gym.make(\n",
    "        environment,\n",
    "        continuous=continuous,\n",
    "        gravity=gravity,\n",
    "        enable_wind=enable_wind,\n",
    "        wind_power=wind_power,\n",
    "        turbulence_power=turbulence_power\n",
    "    )\n",
    "    \n",
    "    # This is needed to get the input size for the NN\n",
    "    num_states = env.observation_space.low.shape[0]\n",
    "    if continuous:\n",
    "        num_actions = env.action_space.shape[0]\n",
    "    else:\n",
    "        num_actions = 1\n",
    "\n",
    "    # Normalize action space according to https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
    "    action_space = spaces.Box(low=-1, high=1, shape=(num_actions,), dtype='float32')\n",
    "\n",
    "    # This is needed to clip the actions within the legal boundaries\n",
    "    upper_bound = action_space.high[0]\n",
    "    lower_bound = action_space.low[0]\n",
    "    \n",
    "    # Apply the seed\n",
    "    _ = env.reset(seed=seed)\n",
    "    \n",
    "    for ep in range(total_episodes):\n",
    "        ep_reward = 0\n",
    "        \n",
    "        # Used for time benchmarking\n",
    "        before = time.time()\n",
    "        \n",
    "        prev_state = env.reset()\n",
    "        agent = Agent(num_states=num_states, num_actions=num_actions, lower_bound=lower_bound, \n",
    "                upper_bound=upper_bound, continuous=continuous, buffer_capacity=0, batch_size=0, \n",
    "                std_dev=0, critic_lr=0, actor_lr=0, gamma=0, tau=0, epsilon=0)\n",
    "        agent.actor_model.load_weights(actor_weights)\n",
    "        \n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "            action = agent.policy(state=tf_prev_state, use_noise=False)\n",
    "            if continuous:\n",
    "                action = action[0]\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            ep_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(str(time.time() - before) + 's')\n",
    "                rewards.append(ep_reward)\n",
    "                break\n",
    "\n",
    "            prev_state = state\n",
    "            \n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "1a90fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random(total_episodes=10, render=False, environment=\"LunarLander-v2\", continuous=True,\n",
    "        gravity=-10.0, enable_wind=False, wind_power=15.0, turbulence_power=1.5, seed=1453):\n",
    "    rewards = []\n",
    "    \n",
    "    env = gym.make(\n",
    "        environment,\n",
    "        continuous=continuous,\n",
    "        gravity=gravity,\n",
    "        enable_wind=enable_wind,\n",
    "        wind_power=wind_power,\n",
    "        turbulence_power=turbulence_power,\n",
    "    )\n",
    "    \n",
    "    # Apply the seed\n",
    "    _ = env.reset(seed=seed)\n",
    "    \n",
    "    for ep in range(total_episodes):\n",
    "        ep_reward = 0\n",
    "        \n",
    "        # Used for time benchmarking\n",
    "        before = time.time()\n",
    "        \n",
    "        prev_state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = env.action_space.sample()\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(str(time.time() - before) + 's')\n",
    "                rewards.append(ep_reward)\n",
    "                break\n",
    "\n",
    "            prev_state = state\n",
    "            \n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83b8ba",
   "metadata": {},
   "source": [
    "---\n",
    "# Runs and tests\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3d14ddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xax = [x for x in range(-600,250)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "4cf6c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decreasing_std(x, episode):\n",
    "    return max(0, min(0.2, 0.2 - (x+500)*(0.2/700)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "058e4b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decreasing_alr(x, episode):\n",
    "    return max(0, min(0.001, 0.001 - (x+500)*(0.001/700)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "890d4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decreasing_clr(x, episode):\n",
    "    return max(0, min(0.002, 0.002 - (x+500)*(0.002/700)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a30c827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decreasing_eps(x, episode):\n",
    "    return max(0, min(0.2, 0.2 - (x+500)*(0.2/700)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ab85e9ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x194244c4850>]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xax,[decreasing_std(x,1) for x in range(-600,250)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "1adcbbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 0 * AvgReward -348.24 * true AvgReward -348.24 * Reward -348.24 * True Reward -348.24 * time 1.51 * step 141\n",
      "Ep 1 * AvgReward -409.85 * true AvgReward -409.85 * Reward -471.46 * True Reward -471.46 * time 1.02 * step 320\n",
      "Ep 2 * AvgReward -325.27 * true AvgReward -325.27 * Reward -156.12 * True Reward -156.12 * time 0.57 * step 420\n",
      "Ep 3 * AvgReward -275.83 * true AvgReward -275.83 * Reward -127.50 * True Reward -127.50 * time 0.51 * step 509\n",
      "Ep 4 * AvgReward -239.83 * true AvgReward -239.83 * Reward -95.86 * True Reward -95.86 * time 0.43 * step 586\n",
      "Ep 5 * AvgReward -267.86 * true AvgReward -267.86 * Reward -408.02 * True Reward -408.02 * time 0.52 * step 675\n",
      "Ep 6 * AvgReward -315.38 * true AvgReward -315.38 * Reward -600.44 * True Reward -600.44 * time 1.02 * step 849\n",
      "Ep 7 * AvgReward -292.22 * true AvgReward -292.22 * Reward -130.14 * True Reward -130.14 * time 0.68 * step 962\n",
      "Ep 8 * AvgReward -288.76 * true AvgReward -288.76 * Reward -261.04 * True Reward -261.04 * time 0.56 * step 1055\n",
      "Ep 9 * AvgReward -288.31 * true AvgReward -288.31 * Reward -284.33 * True Reward -284.33 * time 0.66 * step 1160\n",
      "Ep 10 * AvgReward -310.82 * true AvgReward -310.82 * Reward -535.91 * True Reward -535.91 * time 0.71 * step 1277\n",
      "Ep 11 * AvgReward -306.12 * true AvgReward -306.12 * Reward -254.34 * True Reward -254.34 * time 0.76 * step 1388\n",
      "Ep 12 * AvgReward -303.30 * true AvgReward -303.30 * Reward -269.46 * True Reward -269.46 * time 0.64 * step 1487\n",
      "Ep 13 * AvgReward -323.07 * true AvgReward -323.07 * Reward -580.05 * True Reward -580.05 * time 1.00 * step 1628\n",
      "Ep 14 * AvgReward -318.46 * true AvgReward -318.46 * Reward -253.99 * True Reward -253.99 * time 0.63 * step 1728\n",
      "Ep 15 * AvgReward -319.58 * true AvgReward -319.58 * Reward -336.41 * True Reward -336.41 * time 0.55 * step 1817\n",
      "Ep 16 * AvgReward -314.55 * true AvgReward -314.55 * Reward -234.01 * True Reward -234.01 * time 0.80 * step 1941\n",
      "Ep 17 * AvgReward -313.70 * true AvgReward -313.70 * Reward -299.27 * True Reward -299.27 * time 0.60 * step 2033\n",
      "Ep 18 * AvgReward -302.25 * true AvgReward -302.25 * Reward -96.23 * True Reward -96.23 * time 0.40 * step 2099\n",
      "Ep 19 * AvgReward -298.57 * true AvgReward -298.57 * Reward -228.48 * True Reward -228.48 * time 0.82 * step 2222\n",
      "Ep 20 * AvgReward -293.08 * true AvgReward -293.08 * Reward -238.51 * True Reward -238.51 * time 1.18 * step 2400\n",
      "Ep 21 * AvgReward -291.99 * true AvgReward -291.99 * Reward -449.78 * True Reward -449.78 * time 0.63 * step 2496\n",
      "Ep 22 * AvgReward -308.87 * true AvgReward -308.87 * Reward -493.68 * True Reward -493.68 * time 0.89 * step 2625\n",
      "Ep 23 * AvgReward -312.72 * true AvgReward -312.72 * Reward -204.41 * True Reward -204.41 * time 0.73 * step 2735\n",
      "Ep 24 * AvgReward -331.92 * true AvgReward -331.92 * Reward -479.80 * True Reward -479.80 * time 0.85 * step 2852\n",
      "Ep 25 * AvgReward -329.82 * true AvgReward -329.82 * Reward -366.13 * True Reward -366.13 * time 0.60 * step 2939\n",
      "Ep 26 * AvgReward -314.87 * true AvgReward -314.87 * Reward -301.34 * True Reward -301.34 * time 1.20 * step 3106\n",
      "Ep 27 * AvgReward -312.36 * true AvgReward -312.36 * Reward -80.11 * True Reward -80.11 * time 0.55 * step 3190\n",
      "Ep 28 * AvgReward -313.01 * true AvgReward -313.01 * Reward -273.99 * True Reward -273.99 * time 0.72 * step 3298\n",
      "Ep 29 * AvgReward -303.44 * true AvgReward -303.44 * Reward -92.80 * True Reward -92.80 * time 0.53 * step 3378\n",
      "Ep 30 * AvgReward -301.80 * true AvgReward -301.80 * Reward -503.19 * True Reward -503.19 * time 1.22 * step 3551\n",
      "Ep 31 * AvgReward -310.01 * true AvgReward -310.01 * Reward -418.61 * True Reward -418.61 * time 0.89 * step 3686\n",
      "Ep 32 * AvgReward -319.14 * true AvgReward -319.14 * Reward -451.97 * True Reward -451.97 * time 0.82 * step 3809\n",
      "Ep 33 * AvgReward -299.66 * true AvgReward -299.66 * Reward -190.49 * True Reward -190.49 * time 0.74 * step 3913\n",
      "Ep 34 * AvgReward -297.61 * true AvgReward -297.61 * Reward -213.04 * True Reward -213.04 * time 0.53 * step 3990\n",
      "Ep 35 * AvgReward -299.07 * true AvgReward -299.07 * Reward -365.63 * True Reward -365.63 * time 0.66 * step 4088\n",
      "Ep 36 * AvgReward -304.78 * true AvgReward -304.78 * Reward -348.20 * True Reward -348.20 * time 0.61 * step 4177\n",
      "Ep 37 * AvgReward -294.06 * true AvgReward -294.06 * Reward -84.86 * True Reward -84.86 * time 0.58 * step 4252\n",
      "Ep 38 * AvgReward -304.35 * true AvgReward -304.35 * Reward -301.92 * True Reward -301.92 * time 0.57 * step 4333\n",
      "Ep 39 * AvgReward -297.63 * true AvgReward -297.63 * Reward -94.24 * True Reward -94.24 * time 0.96 * step 4467\n",
      "Ep 40 * AvgReward -299.53 * true AvgReward -299.53 * Reward -276.38 * True Reward -276.38 * time 0.64 * step 4552\n",
      "Ep 41 * AvgReward -296.61 * true AvgReward -296.61 * Reward -391.45 * True Reward -391.45 * time 0.77 * step 4655\n",
      "Ep 42 * AvgReward -288.37 * true AvgReward -288.37 * Reward -328.82 * True Reward -328.82 * time 0.93 * step 4786\n",
      "Ep 43 * AvgReward -286.38 * true AvgReward -286.38 * Reward -164.65 * True Reward -164.65 * time 0.76 * step 4895\n",
      "Ep 44 * AvgReward -281.91 * true AvgReward -281.91 * Reward -390.43 * True Reward -390.43 * time 0.57 * step 4980\n",
      "Ep 45 * AvgReward -271.76 * true AvgReward -271.76 * Reward -163.00 * True Reward -163.00 * time 1.19 * step 5151\n",
      "Ep 46 * AvgReward -273.05 * true AvgReward -273.05 * Reward -327.23 * True Reward -327.23 * time 0.78 * step 5257\n",
      "Ep 47 * AvgReward -283.71 * true AvgReward -283.71 * Reward -293.37 * True Reward -293.37 * time 0.89 * step 5375\n",
      "Ep 48 * AvgReward -283.33 * true AvgReward -283.33 * Reward -266.24 * True Reward -266.24 * time 0.65 * step 5470\n",
      "Ep 49 * AvgReward -287.02 * true AvgReward -287.02 * Reward -166.73 * True Reward -166.73 * time 0.49 * step 5540\n",
      "Ep 50 * AvgReward -268.80 * true AvgReward -268.80 * Reward -138.75 * True Reward -138.75 * time 0.48 * step 5613\n",
      "Ep 51 * AvgReward -274.75 * true AvgReward -274.75 * Reward -537.51 * True Reward -537.51 * time 0.73 * step 5714\n",
      "Ep 52 * AvgReward -272.08 * true AvgReward -272.08 * Reward -398.64 * True Reward -398.64 * time 0.61 * step 5803\n",
      "Ep 53 * AvgReward -272.28 * true AvgReward -272.28 * Reward -194.51 * True Reward -194.51 * time 0.52 * step 5881\n",
      "Ep 54 * AvgReward -265.63 * true AvgReward -265.63 * Reward -79.97 * True Reward -79.97 * time 0.52 * step 5955\n",
      "Ep 55 * AvgReward -254.46 * true AvgReward -254.46 * Reward -142.21 * True Reward -142.21 * time 0.86 * step 6081\n",
      "Ep 56 * AvgReward -254.44 * true AvgReward -254.44 * Reward -347.83 * True Reward -347.83 * time 0.62 * step 6174\n",
      "Ep 57 * AvgReward -256.07 * true AvgReward -256.07 * Reward -117.54 * True Reward -117.54 * time 1.14 * step 6330\n",
      "Ep 58 * AvgReward -258.32 * true AvgReward -258.32 * Reward -346.84 * True Reward -346.84 * time 0.68 * step 6420\n",
      "Ep 59 * AvgReward -278.96 * true AvgReward -278.96 * Reward -507.09 * True Reward -507.09 * time 0.81 * step 6528\n",
      "Ep 60 * AvgReward -283.13 * true AvgReward -283.13 * Reward -359.77 * True Reward -359.77 * time 0.81 * step 6631\n",
      "Ep 61 * AvgReward -269.24 * true AvgReward -269.24 * Reward -113.74 * True Reward -113.74 * time 0.76 * step 6733\n",
      "Ep 62 * AvgReward -275.83 * true AvgReward -275.83 * Reward -460.55 * True Reward -460.55 * time 0.79 * step 6836\n",
      "Ep 63 * AvgReward -270.27 * true AvgReward -270.27 * Reward -53.52 * True Reward -53.52 * time 0.58 * step 6916\n",
      "Ep 64 * AvgReward -256.02 * true AvgReward -256.02 * Reward -105.30 * True Reward -105.30 * time 0.84 * step 7033\n",
      "Ep 65 * AvgReward -250.75 * true AvgReward -250.75 * Reward -57.68 * True Reward -57.68 * time 0.61 * step 7118\n",
      "Ep 66 * AvgReward -244.83 * true AvgReward -244.83 * Reward -208.77 * True Reward -208.77 * time 1.05 * step 7273\n",
      "Ep 67 * AvgReward -243.63 * true AvgReward -243.63 * Reward -269.40 * True Reward -269.40 * time 0.70 * step 7374\n",
      "Ep 68 * AvgReward -237.12 * true AvgReward -237.12 * Reward -136.03 * True Reward -136.03 * time 0.84 * step 7494\n",
      "Ep 69 * AvgReward -232.05 * true AvgReward -232.05 * Reward -65.29 * True Reward -65.29 * time 0.46 * step 7563\n",
      "Ep 70 * AvgReward -228.99 * true AvgReward -228.99 * Reward -77.59 * True Reward -77.59 * time 1.10 * step 7718\n",
      "Ep 71 * AvgReward -217.97 * true AvgReward -217.97 * Reward -317.20 * True Reward -317.20 * time 0.55 * step 7798\n",
      "Ep 72 * AvgReward -206.23 * true AvgReward -206.23 * Reward -163.69 * True Reward -163.69 * time 0.86 * step 7920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 73 * AvgReward -214.12 * true AvgReward -214.12 * Reward -352.45 * True Reward -352.45 * time 0.64 * step 8009\n",
      "Ep 74 * AvgReward -213.92 * true AvgReward -213.92 * Reward -75.94 * True Reward -75.94 * time 0.47 * step 8074\n",
      "Ep 75 * AvgReward -222.01 * true AvgReward -222.01 * Reward -303.99 * True Reward -303.99 * time 0.67 * step 8166\n",
      "Ep 76 * AvgReward -217.05 * true AvgReward -217.05 * Reward -248.53 * True Reward -248.53 * time 0.65 * step 8254\n",
      "Ep 77 * AvgReward -224.03 * true AvgReward -224.03 * Reward -257.16 * True Reward -257.16 * time 0.84 * step 8362\n",
      "Ep 78 * AvgReward -212.54 * true AvgReward -212.54 * Reward -117.18 * True Reward -117.18 * time 0.59 * step 8437\n",
      "Ep 79 * AvgReward -191.64 * true AvgReward -191.64 * Reward -89.08 * True Reward -89.08 * time 0.83 * step 8547\n",
      "Ep 80 * AvgReward -183.84 * true AvgReward -183.84 * Reward -203.73 * True Reward -203.73 * time 0.67 * step 8639\n",
      "Ep 81 * AvgReward -196.11 * true AvgReward -196.11 * Reward -359.02 * True Reward -359.02 * time 0.75 * step 8742\n",
      "Ep 82 * AvgReward -194.87 * true AvgReward -194.87 * Reward -435.89 * True Reward -435.89 * time 0.78 * step 8850\n",
      "Ep 83 * AvgReward -212.86 * true AvgReward -212.86 * Reward -413.37 * True Reward -413.37 * time 0.79 * step 8961\n",
      "Ep 84 * AvgReward -218.55 * true AvgReward -218.55 * Reward -219.01 * True Reward -219.01 * time 0.82 * step 9072\n",
      "Ep 85 * AvgReward -235.87 * true AvgReward -235.87 * Reward -404.00 * True Reward -404.00 * time 0.89 * step 9196\n",
      "Ep 86 * AvgReward -238.40 * true AvgReward -238.40 * Reward -259.49 * True Reward -259.49 * time 0.70 * step 9288\n",
      "Ep 87 * AvgReward -242.02 * true AvgReward -242.02 * Reward -341.82 * True Reward -341.82 * time 0.75 * step 9382\n",
      "Ep 88 * AvgReward -242.17 * true AvgReward -242.17 * Reward -138.99 * True Reward -138.99 * time 0.72 * step 9465\n",
      "Ep 89 * AvgReward -240.63 * true AvgReward -240.63 * Reward -34.42 * True Reward -34.42 * time 0.99 * step 9585\n",
      "Ep 90 * AvgReward -248.76 * true AvgReward -248.76 * Reward -240.21 * True Reward -240.21 * time 0.62 * step 9670\n",
      "Ep 91 * AvgReward -235.91 * true AvgReward -235.91 * Reward -60.16 * True Reward -60.16 * time 0.51 * step 9740\n",
      "Ep 92 * AvgReward -228.35 * true AvgReward -228.35 * Reward -12.62 * True Reward -12.62 * time 0.74 * step 9840\n",
      "Ep 93 * AvgReward -230.47 * true AvgReward -230.47 * Reward -394.72 * True Reward -394.72 * time 0.63 * step 9926\n",
      "Ep 94 * AvgReward -234.75 * true AvgReward -234.75 * Reward -161.62 * True Reward -161.62 * time 0.55 * step 10003\n",
      "Ep 95 * AvgReward -259.32 * true AvgReward -259.32 * Reward -795.44 * True Reward -795.44 * time 0.74 * step 10085\n",
      "Ep 96 * AvgReward -282.78 * true AvgReward -282.78 * Reward -717.72 * True Reward -717.72 * time 0.74 * step 10165\n",
      "Ep 97 * AvgReward -294.38 * true AvgReward -294.38 * Reward -489.07 * True Reward -489.07 * time 0.66 * step 10241\n",
      "Ep 98 * AvgReward -311.06 * true AvgReward -311.06 * Reward -450.75 * True Reward -450.75 * time 0.55 * step 10303\n",
      "Ep 99 * AvgReward -329.46 * true AvgReward -329.46 * Reward -457.16 * True Reward -457.16 * time 0.49 * step 10360\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 73.47851538658142 s\n"
     ]
    }
   ],
   "source": [
    "run(total_trials=1, total_episodes=100, buffer_capacity=300000, tau=0.001, critic_lr=0.0002, \n",
    "    actor_lr=0.0001, start_steps=10000, continuous=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
