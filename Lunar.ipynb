{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7764110b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\flatbuffers\\compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  'hamming': pil_image.HAMMING,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  'box': pil_image.BOX,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  'lanczos': pil_image.LANCZOS,\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.math import argmax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e4ab220",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(358)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0662e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdd14b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_actions_num = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6470ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.gymlibrary.ml/environments/box2d/lunar_lander/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f235a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OU Noise\n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "    def __call__(self):\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n",
    "\n",
    "def get_actor(num_states, num_actions=1, upper_bound=1, continuous=True, layer1=400, layer2=300, \n",
    "              init_weights_min=-0.003, init_weights_max=0.003):\n",
    "    last_init = tf.random_uniform_initializer(minval=init_weights_min, maxval=init_weights_max)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(layer1, activation=\"relu\")(inputs)\n",
    "    out = layers.LayerNormalization(axis=1)(out)\n",
    "    out = layers.Dense(layer2, activation=\"relu\")(out)\n",
    "    out = layers.LayerNormalization(axis=1)(out)\n",
    "    \n",
    "    # Different output activation based on discrete or continous version\n",
    "    if continuous:\n",
    "        outputs = layers.Dense(num_actions, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "    else:\n",
    "        outputs = layers.Dense(disc_actions_num, activation=\"softmax\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Multiply to fill the whole action space which should be equal around 0\n",
    "    outputs = outputs * upper_bound\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "def get_critic(num_states, num_actions=1, continuous=True, layer1=400, layer2=300):\n",
    "    state_input = layers.Input(shape=(num_states,))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    if continuous:\n",
    "        action_input = layers.Input(shape=(num_actions,))\n",
    "    else:\n",
    "        action_input = layers.Input(shape=(disc_actions_num,))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(layer1, activation=\"relu\")(concat)\n",
    "    out = layers.LayerNormalization(axis=1)(out)\n",
    "    out = layers.Dense(layer2, activation=\"relu\")(out)\n",
    "    out = layers.LayerNormalization(axis=1)(out)\n",
    "    outputs = layers.Dense(num_actions)(out)\n",
    "\n",
    "    return tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "# This updates the weights in a slow manner which keeps stability\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d09a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions=1, lower_bound=-1, upper_bound=1, continuous=True,\n",
    "            buffer_capacity=50000, batch_size=64, std_dev=0.2, critic_lr=0.002,\n",
    "            actor_lr=0.001, gamma=0.99, tau=0.005, epsilon=0.2, adam_critic_eps=1e-07,\n",
    "            adam_actor_eps=1e-07, actor_amsgrad=False, critic_amsgrad=False, actor_layer_1=256, \n",
    "            actor_layer_2=256, critic_layer_1=256, critic_layer_2=256):\n",
    "        \n",
    "        self.continuous = continuous\n",
    "        \n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # For methods\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "\n",
    "        # This is used to make sure we only sample from used buffer space\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        if self.continuous:\n",
    "            self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        else:\n",
    "            self.action_buffer = np.zeros((self.buffer_capacity, disc_actions_num))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "        # Also keep track if it is in terminal state (legs on ground)\n",
    "        self.done_buffer = np.zeros((self.buffer_capacity, 1)).astype(np.float32)\n",
    "        \n",
    "        self.std_dev = std_dev\n",
    "        self.critic_lr = critic_lr\n",
    "        self.actor_lr = actor_lr\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        # Epsilon in epsilon-greedy\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.actor_model = get_actor(\n",
    "            num_states, num_actions, upper_bound, continuous=continuous, layer1=actor_layer_1, layer2=actor_layer_2\n",
    "        )\n",
    "        self.critic_model = get_critic(\n",
    "            num_states, num_actions, continuous=continuous, layer1=critic_layer_1, layer2=critic_layer_2\n",
    "        )\n",
    "        \n",
    "        self.target_actor = get_actor(\n",
    "            num_states, num_actions, upper_bound, continuous=continuous, layer1=actor_layer_1, layer2=actor_layer_2\n",
    "        )\n",
    "        self.target_critic = get_critic(\n",
    "            num_states, num_actions, continuous=continuous, layer1=critic_layer_1, layer2=critic_layer_2\n",
    "        )\n",
    "        \n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=actor_lr, beta_1=0.9, beta_2=0.999, epsilon=adam_actor_eps, amsgrad=actor_amsgrad,\n",
    "        )\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=critic_lr, beta_1=0.9, beta_2=0.999, epsilon=adam_critic_eps, amsgrad=critic_amsgrad,\n",
    "        )\n",
    "        # Making the weights equal initially\n",
    "        self.target_actor.set_weights(self.actor_model.get_weights())\n",
    "        self.target_critic.set_weights(self.critic_model.get_weights())\n",
    "        \n",
    "        self.ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "    \n",
    "    # Makes a record of the outputted (s,a,r,s') obervation tuple + terminal state\n",
    "    def record(self, obs_tuple):\n",
    "        # Reuse the same buffer replacing old entries\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        self.done_buffer[index] = obs_tuple[4]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "    \n",
    "    # Calculation of loss and gradients\n",
    "    @tf.function\n",
    "    def update(self, state_batch, action_batch, reward_batch, next_state_batch, done_batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor(next_state_batch, training=True)\n",
    "            # Add done_batch to y function for terminal state\n",
    "            y = reward_batch + done_batch * self.gamma * self.target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = self.critic_model([state_batch, action_batch], training=True)\n",
    "            l = losses.MeanAbsoluteError()\n",
    "            critic_loss = l(y, critic_value)\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
    "        \n",
    "        # Gradient clipping to avoid exploding and vanishing gradients\n",
    "        critic_gvd = zip(critic_grad, self.critic_model.trainable_variables)\n",
    "        critic_capped_grad = [(tf.clip_by_value(grad, clip_value_min=-1, clip_value_max=1), var) for grad, var in critic_gvd]\n",
    "        \n",
    "        self.critic_optimizer.apply_gradients(critic_capped_grad)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor_model(state_batch, training=True)\n",
    "            critic_value = self.critic_model([state_batch, actions], training=True)\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
    "        # clip actor too\n",
    "        actor_gvd = zip(actor_grad, self.actor_model.trainable_variables)\n",
    "        actor_capped_grad = [(tf.clip_by_value(grad, clip_value_min=-1, clip_value_max=1), var) for grad, var in actor_gvd]\n",
    "        \n",
    "        self.actor_optimizer.apply_gradients(actor_capped_grad)\n",
    "\n",
    "    def learn(self):\n",
    "        # Sample only valid data\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        done_batch = tf.convert_to_tensor(self.done_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
    "        \n",
    "    def policy(self, state, noise_object=0, use_noise=True, noise_mult=1):\n",
    "        # Default noise_object to 0 for when it is not needed\n",
    "        # For doing actions without added noise\n",
    "        if not use_noise:    \n",
    "            if self.continuous:\n",
    "                sampled_actions = tf.squeeze(self.actor_model(state)).numpy()\n",
    "                legal_action = np.clip(sampled_actions, self.lower_bound, self.upper_bound)\n",
    "                return [np.squeeze(legal_action)][0]\n",
    "            else:\n",
    "                return self.actor_model(state)\n",
    "        else:\n",
    "            if self.continuous:\n",
    "                sampled_actions = tf.squeeze(self.actor_model(state))\n",
    "                \n",
    "                noise = noise_object()\n",
    "                # Adding noise to action\n",
    "                sampled_actions = sampled_actions.numpy() + noise * noise_mult\n",
    "\n",
    "                # We make sure action is within bounds\n",
    "                legal_action = np.clip(sampled_actions, self.lower_bound, self.upper_bound)\n",
    "                return [np.squeeze(legal_action)][0]\n",
    "            else:\n",
    "                if (rng.random() < self.epsilon):\n",
    "                    #random move\n",
    "                    action = np.zeros(disc_actions_num)\n",
    "                    action[np.random.randint(0, disc_actions_num, 1)[0]] = 1\n",
    "                    return action\n",
    "                else:\n",
    "                    return self.actor_model(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3f522c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed(x, episode):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42f9f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(total_trials=1, total_episodes=100, \n",
    "            buffer_capacity=50000, batch_size=64, std_dev=0.3, critic_lr=0.003, render=False,\n",
    "            actor_lr=0.002, gamma=0.99, tau=0.005, noise_mult=1, save_weights=True, \n",
    "            directory='Weights/', actor_name='actor', critic_name='critic',\n",
    "            gamma_func=fixed, tau_func=fixed, critic_lr_func=fixed, actor_lr_func=fixed,\n",
    "            noise_mult_func=fixed, std_dev_func=fixed, mean_number=20, output=True,\n",
    "            return_rewards=False, total_time=True, use_guide=False, solved=200,\n",
    "            continuous=True, environment='LunarLander-v2', seed=1453, start_steps=0,\n",
    "            gravity=-10.0, enable_wind=False, wind_power=15.0, turbulence_power=1.5,\n",
    "            epsilon=0.2, epsilon_func=fixed, adam_critic_eps=1e-07, adam_actor_eps=1e-07,\n",
    "            actor_amsgrad=False, critic_amsgrad=False, actor_layer_1=256, actor_layer_2=256,\n",
    "            critic_layer_1=256, critic_layer_2=256):\n",
    "    tot_time = time.time()\n",
    "    \n",
    "    if environment == 'LunarLander-v2':\n",
    "        env = gym.make(\n",
    "            \"LunarLander-v2\",\n",
    "            continuous=continuous,\n",
    "            gravity=gravity,\n",
    "            enable_wind=enable_wind,\n",
    "            wind_power=wind_power,\n",
    "            turbulence_power=turbulence_power\n",
    "        )\n",
    "    else:\n",
    "        env = gym.make(environment)\n",
    "        \n",
    "    # Apply the seed\n",
    "    _ = env.reset(seed=seed)\n",
    "        \n",
    "    # This is needed to get the input size for the NN\n",
    "    num_states = env.observation_space.low.shape[0]\n",
    "    if continuous:\n",
    "        num_actions = env.action_space.shape[0]\n",
    "    else:\n",
    "        num_actions = 1\n",
    "\n",
    "    # Normalize action space according to https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
    "    action_space = spaces.Box(low=-1, high=1, shape=(num_actions,), dtype='float32')\n",
    "\n",
    "    # This is needed to clip the actions within the legal boundaries\n",
    "    upper_bound = action_space.high[0]\n",
    "    lower_bound = action_space.low[0]\n",
    "    \n",
    "    ep_reward_list = []\n",
    "    # To store average reward history of last few episodes\n",
    "    avg_reward_list = []\n",
    "    # To separate assisted reward structures from the \"true\"\n",
    "    true_reward_list = []\n",
    "    true_avg_reward_list = []\n",
    "    \n",
    "    for trial in range(total_trials):\n",
    "        # Stepcount used for random start\n",
    "        step = 0\n",
    "\n",
    "        # Add sublists for each trial\n",
    "        avg_reward_list.append([])\n",
    "        ep_reward_list.append([])\n",
    "        true_reward_list.append([])\n",
    "        true_avg_reward_list.append([])\n",
    "        \n",
    "        agent = Agent(num_states=num_states, num_actions=num_actions, lower_bound=lower_bound, \n",
    "                upper_bound=upper_bound, continuous=continuous, buffer_capacity=buffer_capacity, \n",
    "                batch_size=batch_size, std_dev=std_dev, critic_lr=critic_lr, actor_lr=actor_lr, \n",
    "                gamma=gamma, tau=tau, epsilon=epsilon, adam_critic_eps=adam_critic_eps, adam_actor_eps=adam_actor_eps,\n",
    "                actor_amsgrad=actor_amsgrad, critic_amsgrad=critic_amsgrad, actor_layer_1=actor_layer_1, \n",
    "                actor_layer_2=actor_layer_2, critic_layer_1=critic_layer_1, critic_layer_2=critic_layer_2)\n",
    "\n",
    "        for ep in range(total_episodes):\n",
    "            # functions for different parameters\n",
    "            agent.gamma = gamma_func(gamma, ep)\n",
    "            agent.tau = tau_func(tau, ep)\n",
    "            agent.critic_lr = critic_lr_func(critic_lr, ep)\n",
    "            agent.actor_lr = actor_lr_func(actor_lr, ep)\n",
    "            agent.noise_mult = noise_mult_func(noise_mult, ep)\n",
    "            agent.std_dev = std_dev_func(std_dev, ep)\n",
    "            agent.epsilon = epsilon_func(epsilon, ep)\n",
    "            \n",
    "            # Used for time benchmarking\n",
    "            before = time.time()\n",
    "\n",
    "            prev_state = env.reset()\n",
    "            episodic_reward = 0\n",
    "            true_reward = 0\n",
    "\n",
    "            while True:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                \n",
    "                tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "                if step >= start_steps:\n",
    "                    action = agent.policy(state=tf_prev_state, noise_object=agent.ou_noise, noise_mult=noise_mult)\n",
    "                else:\n",
    "                    action = env.action_space.sample()\n",
    "                \n",
    "                step += 1\n",
    "                \n",
    "                # Recieve state and reward from environment.\n",
    "                if continuous:\n",
    "                    state, reward, done, info = env.step(action)\n",
    "                else:\n",
    "                    state, reward, done, info = env.step(np.argmax(action))\n",
    "                \n",
    "                # Add this before eventual reward modification\n",
    "                true_reward += reward\n",
    "                \n",
    "                # Reward modification\n",
    "                if use_guide:\n",
    "                    reward -= abs(state[2]/2) + abs(state[3]) + (abs(state[0])) + (abs(state[1])/2)\n",
    "\n",
    "                # Add terminal state\n",
    "                terminal_state = int(not done)\n",
    "                agent.record((prev_state, action, reward, state, terminal_state))\n",
    "                episodic_reward += reward\n",
    "\n",
    "                agent.learn()\n",
    "                update_target(agent.target_actor.variables, agent.actor_model.variables, agent.tau)\n",
    "                update_target(agent.target_critic.variables, agent.critic_model.variables, agent.tau)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                prev_state = state\n",
    "\n",
    "            ep_reward_list[trial].append(episodic_reward)\n",
    "            true_reward_list[trial].append(true_reward)\n",
    "            \n",
    "            avg_reward = np.mean(ep_reward_list[trial][-mean_number:])\n",
    "            avg_reward_list[trial].append(avg_reward)\n",
    "            true_avg_reward = np.mean(true_reward_list[trial][-mean_number:])\n",
    "            true_avg_reward_list[trial].append(true_avg_reward)\n",
    "            \n",
    "            if output:\n",
    "                print(\"Ep {} * AvgReward {:.2f} * true AvgReward {:.2f} * Reward {:.2f} * True Reward {:.2f} * time {:.2f} * step {}\"\n",
    "                  .format(ep, avg_reward, true_avg_reward, episodic_reward, \n",
    "                          true_reward, (time.time() - before), step))\n",
    "            \n",
    "            # Stop if avg is above 'solved'\n",
    "            if true_avg_reward >= solved:\n",
    "                break\n",
    "\n",
    "        # Save weights\n",
    "        now = datetime.datetime.now()\n",
    "        timestamp = \"{}.{}.{}.{}.{}.{}\".format(now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "        save_name = \"{}_{}_{}\".format(\n",
    "            environment, continuous, \n",
    "            timestamp,\n",
    "        )\n",
    "        if save_weights:\n",
    "            try:\n",
    "                agent.actor_model.save_weights(directory + actor_name + '-trial' + str(trial) + '_' + save_name + '.h5')\n",
    "            except:\n",
    "                print('actor save fail')\n",
    "            try:\n",
    "                agent.critic_model.save_weights(directory + critic_name + '-trial' + str(trial) + '_' + save_name + '.h5')\n",
    "            except:\n",
    "                print('critic save fail')\n",
    "    \n",
    "    # Plotting graph\n",
    "    for idx, p in enumerate(true_avg_reward_list):\n",
    "        plt.plot(p, label=str(idx))\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True Avg. Epsiodic Reward (\" + str(mean_number) + \")\")\n",
    "    plt.legend()\n",
    "    try:\n",
    "        plt.savefig('Graphs/' + save_name + '.png')\n",
    "    except:\n",
    "        print('save fig fail')\n",
    "    plt.show()\n",
    "    \n",
    "    print('total time:', time.time() - tot_time, 's')\n",
    "    \n",
    "    # Return to be able to make graphs etc. later, or use the data for other stuff\n",
    "    if return_rewards:\n",
    "        return true_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a57bcf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(total_episodes=10, actor_weights='Weights/actor-trial0.h5', render=False,\n",
    "        environment=\"LunarLander-v2\", continuous=True, gravity=-10.0, enable_wind=False,\n",
    "        wind_power=15.0, turbulence_power=1.5, seed=1453):\n",
    "    rewards = []\n",
    "    \n",
    "    env = gym.make(\n",
    "        environment,\n",
    "        continuous=continuous,\n",
    "        gravity=gravity,\n",
    "        enable_wind=enable_wind,\n",
    "        wind_power=wind_power,\n",
    "        turbulence_power=turbulence_power\n",
    "    )\n",
    "    \n",
    "    # This is needed to get the input size for the NN\n",
    "    num_states = env.observation_space.low.shape[0]\n",
    "    if continuous:\n",
    "        num_actions = env.action_space.shape[0]\n",
    "    else:\n",
    "        num_actions = 1\n",
    "\n",
    "    # Normalize action space according to https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
    "    action_space = spaces.Box(low=-1, high=1, shape=(num_actions,), dtype='float32')\n",
    "\n",
    "    # This is needed to clip the actions within the legal boundaries\n",
    "    upper_bound = action_space.high[0]\n",
    "    lower_bound = action_space.low[0]\n",
    "    \n",
    "    # Apply the seed\n",
    "    _ = env.reset(seed=seed)\n",
    "    \n",
    "    for ep in range(total_episodes):\n",
    "        ep_reward = 0\n",
    "        \n",
    "        # Used for time benchmarking\n",
    "        before = time.time()\n",
    "        \n",
    "        prev_state = env.reset()\n",
    "        agent = Agent(num_states=num_states, num_actions=num_actions, lower_bound=lower_bound, \n",
    "                upper_bound=upper_bound, continuous=continuous, buffer_capacity=0, batch_size=0, \n",
    "                std_dev=0, critic_lr=0, actor_lr=0, gamma=0, tau=0, epsilon=0)\n",
    "        agent.actor_model.load_weights(actor_weights)\n",
    "        \n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "            action = agent.policy(state=tf_prev_state, use_noise=False)\n",
    "\n",
    "            if continuous:\n",
    "                state, reward, done, info = env.step(action)\n",
    "            else:\n",
    "                state, reward, done, info = env.step(np.argmax(action))\n",
    "            \n",
    "            ep_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(str(time.time() - before) + 's')\n",
    "                rewards.append(ep_reward)\n",
    "                break\n",
    "\n",
    "            prev_state = state\n",
    "            \n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a90fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random(total_episodes=10, render=False, environment=\"LunarLander-v2\", continuous=True,\n",
    "        gravity=-10.0, enable_wind=False, wind_power=15.0, turbulence_power=1.5, seed=1453):\n",
    "    \n",
    "    rewards = []\n",
    "    \n",
    "    env = gym.make(\n",
    "        environment,\n",
    "        continuous=continuous,\n",
    "        gravity=gravity,\n",
    "        enable_wind=enable_wind,\n",
    "        wind_power=wind_power,\n",
    "        turbulence_power=turbulence_power,\n",
    "    )\n",
    "    \n",
    "    # Apply the seed\n",
    "    _ = env.reset(seed=seed)\n",
    "    \n",
    "    for ep in range(total_episodes):\n",
    "        ep_reward = 0\n",
    "        \n",
    "        before = time.time()\n",
    "        \n",
    "        prev_state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = env.action_space.sample()\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(str(time.time() - before) + 's')\n",
    "                rewards.append(ep_reward)\n",
    "                break\n",
    "\n",
    "            prev_state = state\n",
    "            \n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83b8ba",
   "metadata": {},
   "source": [
    "---\n",
    "# Runs and tests\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d14ddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xax = [x for x in range(-500,250)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cf6c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decreasing_std(x, episode):\n",
    "    return max(0, min(0.4, 0.4 - (x+400)*(0.4/600)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "058e4b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decreasing_alr(x, episode):\n",
    "    return max(0, min(0.0002, 0.0002 - (x+400)*(0.0002/600)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "890d4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decreasing_clr(x, episode):\n",
    "    return max(0, min(0.0004, 0.0004 - (x+400)*(0.0004/600)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3197773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decreasing_tau(x, episode):\n",
    "    return max(0, min(0.002, 0.002 - (x+400)*(0.002/600)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab85e9ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a028d6ce50>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xax,[decreasing_std(x,1) for x in range(-500,250)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b04643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 0 * AvgReward -352.85 * true AvgReward -352.85 * Reward -352.85 * True Reward -352.85 * time 3.11 * step 143\n",
      "Ep 1 * AvgReward -402.60 * true AvgReward -402.60 * Reward -452.35 * True Reward -452.35 * time 0.97 * step 244\n",
      "Ep 2 * AvgReward -310.44 * true AvgReward -310.44 * Reward -126.13 * True Reward -126.13 * time 1.01 * step 352\n",
      "Ep 3 * AvgReward -247.01 * true AvgReward -247.01 * Reward -56.70 * True Reward -56.70 * time 0.80 * step 434\n",
      "Ep 4 * AvgReward -255.60 * true AvgReward -255.60 * Reward -289.97 * True Reward -289.97 * time 1.16 * step 558\n",
      "Ep 5 * AvgReward -227.49 * true AvgReward -227.49 * Reward -86.94 * True Reward -86.94 * time 0.70 * step 629\n",
      "Ep 6 * AvgReward -205.39 * true AvgReward -205.39 * Reward -72.77 * True Reward -72.77 * time 0.79 * step 712\n",
      "Ep 7 * AvgReward -202.36 * true AvgReward -202.36 * Reward -181.17 * True Reward -181.17 * time 1.04 * step 820\n",
      "Ep 8 * AvgReward -214.16 * true AvgReward -214.16 * Reward -308.53 * True Reward -308.53 * time 1.00 * step 925\n",
      "Ep 9 * AvgReward -199.35 * true AvgReward -199.35 * Reward -66.09 * True Reward -66.09 * time 0.77 * step 1003\n",
      "Ep 10 * AvgReward -183.34 * true AvgReward -183.34 * Reward -23.27 * True Reward -23.27 * time 0.79 * step 1085\n",
      "Ep 11 * AvgReward -190.54 * true AvgReward -190.54 * Reward -269.73 * True Reward -269.73 * time 0.92 * step 1181\n",
      "Ep 12 * AvgReward -182.93 * true AvgReward -182.93 * Reward -91.59 * True Reward -91.59 * time 0.93 * step 1276\n",
      "Ep 13 * AvgReward -178.32 * true AvgReward -178.32 * Reward -118.37 * True Reward -118.37 * time 0.89 * step 1360\n",
      "Ep 14 * AvgReward -186.41 * true AvgReward -186.41 * Reward -299.67 * True Reward -299.67 * time 1.13 * step 1472\n",
      "Ep 15 * AvgReward -191.20 * true AvgReward -191.20 * Reward -263.00 * True Reward -263.00 * time 1.77 * step 1652\n",
      "Ep 16 * AvgReward -194.56 * true AvgReward -194.56 * Reward -248.39 * True Reward -248.39 * time 1.63 * step 1818\n",
      "Ep 17 * AvgReward -185.94 * true AvgReward -185.94 * Reward -39.38 * True Reward -39.38 * time 1.14 * step 1925\n",
      "Ep 18 * AvgReward -195.90 * true AvgReward -195.90 * Reward -375.19 * True Reward -375.19 * time 0.89 * step 2016\n",
      "Ep 19 * AvgReward -201.39 * true AvgReward -201.39 * Reward -305.75 * True Reward -305.75 * time 1.50 * step 2168\n",
      "Ep 20 * AvgReward -198.18 * true AvgReward -198.18 * Reward -288.51 * True Reward -288.51 * time 1.05 * step 2274\n",
      "Ep 21 * AvgReward -188.62 * true AvgReward -188.62 * Reward -261.14 * True Reward -261.14 * time 0.79 * step 2355\n",
      "Ep 22 * AvgReward -190.89 * true AvgReward -190.89 * Reward -171.63 * True Reward -171.63 * time 0.66 * step 2421\n",
      "Ep 23 * AvgReward -193.22 * true AvgReward -193.22 * Reward -103.30 * True Reward -103.30 * time 0.78 * step 2499\n",
      "Ep 24 * AvgReward -182.41 * true AvgReward -182.41 * Reward -73.82 * True Reward -73.82 * time 0.74 * step 2576\n",
      "Ep 25 * AvgReward -192.33 * true AvgReward -192.33 * Reward -285.35 * True Reward -285.35 * time 1.18 * step 2694\n",
      "Ep 26 * AvgReward -196.47 * true AvgReward -196.47 * Reward -155.47 * True Reward -155.47 * time 1.55 * step 2850\n",
      "Ep 27 * AvgReward -199.51 * true AvgReward -199.51 * Reward -241.93 * True Reward -241.93 * time 1.68 * step 3016\n",
      "Ep 28 * AvgReward -189.44 * true AvgReward -189.44 * Reward -107.18 * True Reward -107.18 * time 0.70 * step 3086\n",
      "Ep 29 * AvgReward -205.65 * true AvgReward -205.65 * Reward -390.36 * True Reward -390.36 * time 1.02 * step 3189\n",
      "Ep 30 * AvgReward -208.57 * true AvgReward -208.57 * Reward -81.70 * True Reward -81.70 * time 1.01 * step 3288\n",
      "Ep 31 * AvgReward -203.86 * true AvgReward -203.86 * Reward -175.38 * True Reward -175.38 * time 0.69 * step 3358\n",
      "Ep 32 * AvgReward -214.31 * true AvgReward -214.31 * Reward -300.66 * True Reward -300.66 * time 1.03 * step 3454\n",
      "Ep 33 * AvgReward -213.32 * true AvgReward -213.32 * Reward -98.62 * True Reward -98.62 * time 0.71 * step 3526\n",
      "Ep 34 * AvgReward -205.09 * true AvgReward -205.09 * Reward -135.05 * True Reward -135.05 * time 0.94 * step 3619\n",
      "Ep 35 * AvgReward -206.96 * true AvgReward -206.96 * Reward -300.31 * True Reward -300.31 * time 0.76 * step 3693\n",
      "Ep 36 * AvgReward -196.31 * true AvgReward -196.31 * Reward -35.37 * True Reward -35.37 * time 0.84 * step 3779\n",
      "Ep 37 * AvgReward -203.62 * true AvgReward -203.62 * Reward -185.58 * True Reward -185.58 * time 0.73 * step 3850\n",
      "Ep 38 * AvgReward -209.07 * true AvgReward -209.07 * Reward -484.21 * True Reward -484.21 * time 1.23 * step 3971\n",
      "Ep 39 * AvgReward -213.03 * true AvgReward -213.03 * Reward -385.02 * True Reward -385.02 * time 1.90 * step 4158\n",
      "Ep 40 * AvgReward -206.09 * true AvgReward -206.09 * Reward -149.80 * True Reward -149.80 * time 0.91 * step 4247\n",
      "Ep 41 * AvgReward -204.51 * true AvgReward -204.51 * Reward -229.45 * True Reward -229.45 * time 1.05 * step 4353\n",
      "Ep 42 * AvgReward -203.56 * true AvgReward -203.56 * Reward -152.69 * True Reward -152.69 * time 1.37 * step 4486\n",
      "Ep 43 * AvgReward -210.64 * true AvgReward -210.64 * Reward -244.88 * True Reward -244.88 * time 0.96 * step 4582\n",
      "Ep 44 * AvgReward -216.70 * true AvgReward -216.70 * Reward -194.97 * True Reward -194.97 * time 1.47 * step 4724\n",
      "Ep 45 * AvgReward -209.33 * true AvgReward -209.33 * Reward -137.91 * True Reward -137.91 * time 1.39 * step 4858\n",
      "Ep 46 * AvgReward -204.64 * true AvgReward -204.64 * Reward -61.70 * True Reward -61.70 * time 0.95 * step 4951\n",
      "Ep 47 * AvgReward -210.72 * true AvgReward -210.72 * Reward -363.62 * True Reward -363.62 * time 1.29 * step 5074\n",
      "Ep 48 * AvgReward -215.14 * true AvgReward -215.14 * Reward -195.50 * True Reward -195.50 * time 1.08 * step 5180\n",
      "Ep 49 * AvgReward -211.50 * true AvgReward -211.50 * Reward -317.59 * True Reward -317.59 * time 1.55 * step 5333\n",
      "Ep 50 * AvgReward -214.50 * true AvgReward -214.50 * Reward -141.70 * True Reward -141.70 * time 0.95 * step 5419\n",
      "Ep 51 * AvgReward -207.32 * true AvgReward -207.32 * Reward -31.70 * True Reward -31.70 * time 1.00 * step 5518\n",
      "Ep 52 * AvgReward -212.58 * true AvgReward -212.58 * Reward -406.01 * True Reward -406.01 * time 1.47 * step 5663\n",
      "Ep 53 * AvgReward -213.12 * true AvgReward -213.12 * Reward -109.29 * True Reward -109.29 * time 1.18 * step 5781\n",
      "Ep 54 * AvgReward -209.38 * true AvgReward -209.38 * Reward -60.27 * True Reward -60.27 * time 1.84 * step 5963\n",
      "Ep 55 * AvgReward -204.82 * true AvgReward -204.82 * Reward -209.19 * True Reward -209.19 * time 1.34 * step 6096\n",
      "Ep 56 * AvgReward -223.96 * true AvgReward -223.96 * Reward -418.11 * True Reward -418.11 * time 1.21 * step 6215\n",
      "Ep 57 * AvgReward -231.43 * true AvgReward -231.43 * Reward -335.01 * True Reward -335.01 * time 1.01 * step 6317\n",
      "Ep 58 * AvgReward -211.61 * true AvgReward -211.61 * Reward -87.82 * True Reward -87.82 * time 1.33 * step 6427\n",
      "Ep 59 * AvgReward -215.11 * true AvgReward -215.11 * Reward -455.08 * True Reward -455.08 * time 0.89 * step 6515\n",
      "Ep 60 * AvgReward -227.78 * true AvgReward -227.78 * Reward -403.18 * True Reward -403.18 * time 0.86 * step 6600\n",
      "Ep 61 * AvgReward -230.85 * true AvgReward -230.85 * Reward -290.80 * True Reward -290.80 * time 1.02 * step 6704\n",
      "Ep 62 * AvgReward -236.59 * true AvgReward -236.59 * Reward -267.47 * True Reward -267.47 * time 0.94 * step 6797\n",
      "Ep 63 * AvgReward -227.68 * true AvgReward -227.68 * Reward -66.63 * True Reward -66.63 * time 0.80 * step 6879\n",
      "Ep 64 * AvgReward -223.47 * true AvgReward -223.47 * Reward -110.87 * True Reward -110.87 * time 1.97 * step 7070\n",
      "Ep 65 * AvgReward -219.86 * true AvgReward -219.86 * Reward -65.67 * True Reward -65.67 * time 1.51 * step 7214\n",
      "Ep 66 * AvgReward -221.69 * true AvgReward -221.69 * Reward -98.21 * True Reward -98.21 * time 0.84 * step 7299\n",
      "Ep 67 * AvgReward -213.21 * true AvgReward -213.21 * Reward -194.18 * True Reward -194.18 * time 1.30 * step 7418\n",
      "Ep 68 * AvgReward -219.75 * true AvgReward -219.75 * Reward -326.23 * True Reward -326.23 * time 0.88 * step 7506\n",
      "Ep 69 * AvgReward -220.09 * true AvgReward -220.09 * Reward -324.33 * True Reward -324.33 * time 0.85 * step 7589\n",
      "Ep 70 * AvgReward -230.58 * true AvgReward -230.58 * Reward -351.64 * True Reward -351.64 * time 0.92 * step 7682\n",
      "Ep 71 * AvgReward -243.08 * true AvgReward -243.08 * Reward -281.58 * True Reward -281.58 * time 1.07 * step 7788\n",
      "Ep 72 * AvgReward -223.76 * true AvgReward -223.76 * Reward -19.57 * True Reward -19.57 * time 0.97 * step 7886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 73 * AvgReward -232.11 * true AvgReward -232.11 * Reward -276.33 * True Reward -276.33 * time 1.02 * step 7985\n",
      "Ep 74 * AvgReward -230.81 * true AvgReward -230.81 * Reward -34.25 * True Reward -34.25 * time 1.60 * step 8140\n",
      "Ep 75 * AvgReward -224.10 * true AvgReward -224.10 * Reward -75.02 * True Reward -75.02 * time 1.00 * step 8239\n",
      "Ep 76 * AvgReward -215.05 * true AvgReward -215.05 * Reward -237.03 * True Reward -237.03 * time 1.07 * step 8345\n",
      "Ep 77 * AvgReward -206.02 * true AvgReward -206.02 * Reward -154.60 * True Reward -154.60 * time 0.85 * step 8429\n",
      "Ep 78 * AvgReward -209.95 * true AvgReward -209.95 * Reward -166.43 * True Reward -166.43 * time 1.04 * step 8533\n",
      "Ep 79 * AvgReward -201.47 * true AvgReward -201.47 * Reward -285.44 * True Reward -285.44 * time 1.14 * step 8646\n",
      "Ep 80 * AvgReward -185.36 * true AvgReward -185.36 * Reward -81.02 * True Reward -81.02 * time 0.99 * step 8743\n",
      "Ep 81 * AvgReward -173.25 * true AvgReward -173.25 * Reward -48.51 * True Reward -48.51 * time 0.71 * step 8814\n",
      "Ep 82 * AvgReward -164.07 * true AvgReward -164.07 * Reward -83.93 * True Reward -83.93 * time 1.67 * step 8979\n",
      "Ep 83 * AvgReward -174.60 * true AvgReward -174.60 * Reward -277.14 * True Reward -277.14 * time 1.29 * step 9107\n"
     ]
    }
   ],
   "source": [
    "run(total_trials=1, total_episodes=1500, gamma=0.998, buffer_capacity=300000, continuous=True, \n",
    "    start_steps=10000, std_dev_func=decreasing_std, actor_lr_func=decreasing_alr,\n",
    "    critic_lr_func=decreasing_clr, tau_func=decreasing_tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9580415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(total_trials=1, total_episodes=1500, buffer_capacity=500000, tau=0.004, critic_lr=0.002, \n",
    "    actor_lr=0.001, start_steps=20000, continuous=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
