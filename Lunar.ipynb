{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7764110b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\flatbuffers\\compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  'hamming': pil_image.HAMMING,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  'box': pil_image.BOX,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  'lanczos': pil_image.LANCZOS,\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.math import argmax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e4ab220",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(358)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdd14b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_actions_num = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6470ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.gymlibrary.ml/environments/box2d/lunar_lander/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f235a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OU Noise\n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "    def __call__(self):\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n",
    "\n",
    "def get_actor(num_states, num_actions=1, upper_bound=1, continuous=True, layer1=400, layer2=300, \n",
    "              init_weights_min=-0.003, init_weights_max=0.003):\n",
    "    last_init = tf.random_uniform_initializer(minval=init_weights_min, maxval=init_weights_max)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(layer1, activation=\"relu\")(inputs)\n",
    "    out = layers.Dense(layer2, activation=\"relu\")(out)\n",
    "    \n",
    "    # Different output activation based on discrete or continous version\n",
    "    if continuous:\n",
    "        outputs = layers.Dense(num_actions, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "    else:\n",
    "        outputs = layers.Dense(disc_actions_num, activation=\"softmax\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Multiply to fill the whole action space which should be equal around 0\n",
    "    outputs = outputs * upper_bound\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "def get_critic(num_states, num_actions=1, continuous=True, layer1=400, layer2=300):\n",
    "    state_input = layers.Input(shape=(num_states,))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    if continuous:\n",
    "        action_input = layers.Input(shape=(num_actions,))\n",
    "    else:\n",
    "        action_input = layers.Input(shape=(disc_actions_num,))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(layer1, activation=\"relu\")(concat)\n",
    "    out = layers.Dense(layer2, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(num_actions)(out)\n",
    "#     if continuous:\n",
    "#         outputs = layers.Dense(num_actions)(out)\n",
    "#     else:\n",
    "#         outputs = layers.Dense(disc_actions_num)(out)\n",
    "\n",
    "    return tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "# This updates the weights in a slow manner which keeps stability\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d09a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions=1, lower_bound=-1, upper_bound=1, continuous=True,\n",
    "            buffer_capacity=50000, batch_size=64, std_dev=0.2, critic_lr=0.002,\n",
    "            actor_lr=0.001, gamma=0.99, tau=0.005, epsilon=0.2, adam_critic_eps=1e-07,\n",
    "            adam_actor_eps=1e-07, actor_amsgrad=False, critic_amsgrad=False, actor_layer_1=256, \n",
    "            actor_layer_2=256, critic_layer_1=256, critic_layer_2=256):\n",
    "        \n",
    "        self.continuous = continuous\n",
    "        \n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # For methods\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "\n",
    "        # This is used to make sure we only sample from used buffer space\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        if self.continuous:\n",
    "            self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        else:\n",
    "            self.action_buffer = np.zeros((self.buffer_capacity, disc_actions_num))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "        # Also keep track if it is in terminal state (legs on ground)\n",
    "        self.done_buffer = np.zeros((self.buffer_capacity, 1)).astype(np.float32)\n",
    "        \n",
    "        self.std_dev = std_dev\n",
    "        self.critic_lr = critic_lr\n",
    "        self.actor_lr = actor_lr\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        # Epsilon in epsilon-greedy\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.actor_model = get_actor(\n",
    "            num_states, num_actions, upper_bound, continuous=continuous, layer1=actor_layer_1, layer2=actor_layer_2\n",
    "        )\n",
    "        self.critic_model = get_critic(\n",
    "            num_states, num_actions, continuous=continuous, layer1=critic_layer_1, layer2=critic_layer_2\n",
    "        )\n",
    "        \n",
    "        self.target_actor = get_actor(\n",
    "            num_states, num_actions, upper_bound, continuous=continuous, layer1=actor_layer_1, layer2=actor_layer_2\n",
    "        )\n",
    "        self.target_critic = get_critic(\n",
    "            num_states, num_actions, continuous=continuous, layer1=critic_layer_1, layer2=critic_layer_2\n",
    "        )\n",
    "        \n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=actor_lr, beta_1=0.9, beta_2=0.999, epsilon=adam_actor_eps, amsgrad=actor_amsgrad,\n",
    "        )\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=critic_lr, beta_1=0.9, beta_2=0.999, epsilon=adam_critic_eps, amsgrad=critic_amsgrad,\n",
    "        )\n",
    "        # Making the weights equal initially\n",
    "        self.target_actor.set_weights(self.actor_model.get_weights())\n",
    "        self.target_critic.set_weights(self.critic_model.get_weights())\n",
    "        \n",
    "        self.ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "    \n",
    "    # Makes a record of the outputted (s,a,r,s') obervation tuple + terminal state\n",
    "    def record(self, obs_tuple):\n",
    "        # Reuse the same buffer replacing old entries\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        self.done_buffer[index] = obs_tuple[4]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "    \n",
    "    # Calculation of loss and gradients\n",
    "    @tf.function\n",
    "    def update(self, state_batch, action_batch, reward_batch, next_state_batch, done_batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor(next_state_batch, training=True)\n",
    "            # Add done_batch to y function for terminal state\n",
    "            y = reward_batch + done_batch * self.gamma * self.target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = self.critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
    "        \n",
    "        # Gradient clipping to avoid exploding and vanishing gradients\n",
    "        critic_gvd = zip(critic_grad, self.critic_model.trainable_variables)\n",
    "        critic_capped_grad = [(tf.clip_by_value(grad, clip_value_min=-2, clip_value_max=2), var) for grad, var in critic_gvd]\n",
    "        \n",
    "        self.critic_optimizer.apply_gradients(critic_capped_grad)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor_model(state_batch, training=True)\n",
    "            critic_value = self.critic_model([state_batch, actions], training=True)\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
    "        # clip actor too\n",
    "        actor_gvd = zip(actor_grad, self.actor_model.trainable_variables)\n",
    "        actor_capped_grad = [(tf.clip_by_value(grad, clip_value_min=-2, clip_value_max=2), var) for grad, var in actor_gvd]\n",
    "        \n",
    "        self.actor_optimizer.apply_gradients(actor_capped_grad)\n",
    "\n",
    "    def learn(self):\n",
    "        # Sample only valid data\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        done_batch = tf.convert_to_tensor(self.done_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
    "        \n",
    "    def policy(self, state, noise_object=0, use_noise=True, noise_mult=1):\n",
    "        # Default noise_object to 0 for when it is not needed\n",
    "        # For doing actions without added noise\n",
    "        if not use_noise:    \n",
    "            if self.continuous:\n",
    "                sampled_actions = tf.squeeze(self.actor_model(state)).numpy()\n",
    "                legal_action = np.clip(sampled_actions, self.lower_bound, self.upper_bound)\n",
    "                return [np.squeeze(legal_action)][0]\n",
    "            else:\n",
    "                return self.actor_model(state)\n",
    "        else:\n",
    "            if self.continuous:\n",
    "                sampled_actions = tf.squeeze(self.actor_model(state))\n",
    "                \n",
    "                noise = noise_object()\n",
    "                # Adding noise to action\n",
    "                sampled_actions = sampled_actions.numpy() + noise * noise_mult\n",
    "\n",
    "                # We make sure action is within bounds\n",
    "                legal_action = np.clip(sampled_actions, self.lower_bound, self.upper_bound)\n",
    "                return [np.squeeze(legal_action)][0]\n",
    "            else:\n",
    "                if (rng.random() < self.epsilon):\n",
    "                    #random move\n",
    "                    action = np.zeros(disc_actions_num)\n",
    "                    action[np.random.randint(0, disc_actions_num, 1)[0]] = 1\n",
    "                    return action\n",
    "                else:\n",
    "                    return self.actor_model(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1975650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6070693511849563"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3f522c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed(x, episode):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42f9f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(total_trials=1, total_episodes=100, \n",
    "            buffer_capacity=50000, batch_size=64, std_dev=0.3, critic_lr=0.003, render=False,\n",
    "            actor_lr=0.002, gamma=0.99, tau=0.005, noise_mult=1, save_weights=True, \n",
    "            directory='Weights/', actor_name='actor', critic_name='critic',\n",
    "            gamma_func=fixed, tau_func=fixed, critic_lr_func=fixed, actor_lr_func=fixed,\n",
    "            noise_mult_func=fixed, std_dev_func=fixed, mean_number=20, output=True,\n",
    "            return_rewards=False, total_time=True, use_guide=False, solved=200,\n",
    "            continuous=True, environment='LunarLander-v2', seed=1453, start_steps=0,\n",
    "            gravity=-10.0, enable_wind=False, wind_power=15.0, turbulence_power=1.5,\n",
    "            epsilon=0.2, epsilon_func=fixed, adam_critic_eps=1e-07, adam_actor_eps=1e-07,\n",
    "            actor_amsgrad=False, critic_amsgrad=False, actor_layer_1=256, actor_layer_2=256,\n",
    "            critic_layer_1=256, critic_layer_2=256):\n",
    "    tot_time = time.time()\n",
    "    \n",
    "    if environment == 'LunarLander-v2':\n",
    "        env = gym.make(\n",
    "            \"LunarLander-v2\",\n",
    "            continuous=continuous,\n",
    "            gravity=gravity,\n",
    "            enable_wind=enable_wind,\n",
    "            wind_power=wind_power,\n",
    "            turbulence_power=turbulence_power\n",
    "        )\n",
    "    else:\n",
    "        env = gym.make(environment)\n",
    "        \n",
    "    # Apply the seed\n",
    "    _ = env.reset(seed=seed)\n",
    "        \n",
    "    # This is needed to get the input size for the NN\n",
    "    num_states = env.observation_space.low.shape[0]\n",
    "    if continuous:\n",
    "        num_actions = env.action_space.shape[0]\n",
    "    else:\n",
    "        num_actions = 1\n",
    "\n",
    "    # Normalize action space according to https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
    "    action_space = spaces.Box(low=-1, high=1, shape=(num_actions,), dtype='float32')\n",
    "\n",
    "    # This is needed to clip the actions within the legal boundaries\n",
    "    upper_bound = action_space.high[0]\n",
    "    lower_bound = action_space.low[0]\n",
    "    \n",
    "    ep_reward_list = []\n",
    "    # To store average reward history of last few episodes\n",
    "    avg_reward_list = []\n",
    "    # To separate assisted reward structures from the \"true\"\n",
    "    true_reward_list = []\n",
    "    true_avg_reward_list = []\n",
    "    \n",
    "    for trial in range(total_trials):\n",
    "        # Stepcount used for random start\n",
    "        step = 0\n",
    "\n",
    "        # Add sublists for each trial\n",
    "        avg_reward_list.append([])\n",
    "        ep_reward_list.append([])\n",
    "        true_reward_list.append([])\n",
    "        true_avg_reward_list.append([])\n",
    "        \n",
    "        agent = Agent(num_states=num_states, num_actions=num_actions, lower_bound=lower_bound, \n",
    "                upper_bound=upper_bound, continuous=continuous, buffer_capacity=buffer_capacity, \n",
    "                batch_size=batch_size, std_dev=std_dev, critic_lr=critic_lr, actor_lr=actor_lr, \n",
    "                gamma=gamma, tau=tau, epsilon=epsilon, adam_critic_eps=adam_critic_eps, adam_actor_eps=adam_actor_eps,\n",
    "                actor_amsgrad=actor_amsgrad, critic_amsgrad=critic_amsgrad, actor_layer_1=actor_layer_1, \n",
    "                actor_layer_2=actor_layer_2, critic_layer_1=critic_layer_1, critic_layer_2=critic_layer_2)\n",
    "\n",
    "        for ep in range(total_episodes):\n",
    "            # functions for different parameters\n",
    "            agent.gamma = gamma_func(gamma, ep)\n",
    "            agent.tau = tau_func(tau, ep)\n",
    "            agent.critic_lr = critic_lr_func(critic_lr, ep)\n",
    "            agent.actor_lr = actor_lr_func(actor_lr, ep)\n",
    "            agent.noise_mult = noise_mult_func(noise_mult, ep)\n",
    "            agent.std_dev = std_dev_func(std_dev, ep)\n",
    "            agent.epsilon = epsilon_func(epsilon, ep)\n",
    "            \n",
    "            # Used for time benchmarking\n",
    "            before = time.time()\n",
    "\n",
    "            prev_state = env.reset()\n",
    "            episodic_reward = 0\n",
    "            true_reward = 0\n",
    "\n",
    "            while True:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                \n",
    "                tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "                if step >= start_steps:\n",
    "                    action = agent.policy(state=tf_prev_state, noise_object=agent.ou_noise, noise_mult=noise_mult)\n",
    "                else:\n",
    "                    action = env.action_space.sample()\n",
    "                \n",
    "                step += 1\n",
    "                \n",
    "                # Recieve state and reward from environment.\n",
    "                if continuous:\n",
    "                    state, reward, done, info = env.step(action)\n",
    "                else:\n",
    "                    state, reward, done, info = env.step(np.argmax(action))\n",
    "                \n",
    "                # Add this before eventual reward modification\n",
    "                true_reward += reward\n",
    "                \n",
    "                # Reward modification\n",
    "                if use_guide:\n",
    "                    # giving penalty for straying far from flags and having high speed\n",
    "                    # x max\n",
    "#                     reward -= int(abs(state[0]) > 0.15) * 2 * abs(state[0])\n",
    "#                     # y top\n",
    "#                     reward -= int(state[1] > 1) * state[1] / 2\n",
    "#                     # horizontal speed\n",
    "#                     reward -= int(abs(state[2]) > 1) * abs(state[2])\n",
    "#                     # down speed\n",
    "#                     reward -= int(state[3] <  -1) * abs(state[3])\n",
    "#                     # up speed\n",
    "#                     reward -= int(state[3] > 0.1) * 3 * state[3]\n",
    "                    reward -= abs(state[2]/2) + abs(state[3]) + (abs(state[0])) + (abs(state[1])/2)\n",
    "\n",
    "                # Add terminal state for when it has landed. Just look at legs on the ground.\n",
    "                terminal_state = int(not (state[6] and state[7]))\n",
    "                agent.record((prev_state, action, reward, state, terminal_state))\n",
    "                episodic_reward += reward\n",
    "\n",
    "                agent.learn()\n",
    "                update_target(agent.target_actor.variables, agent.actor_model.variables, agent.tau)\n",
    "                update_target(agent.target_critic.variables, agent.critic_model.variables, agent.tau)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                prev_state = state\n",
    "\n",
    "            ep_reward_list[trial].append(episodic_reward)\n",
    "            true_reward_list[trial].append(true_reward)\n",
    "            \n",
    "            avg_reward = np.mean(ep_reward_list[trial][-mean_number:])\n",
    "            avg_reward_list[trial].append(avg_reward)\n",
    "            true_avg_reward = np.mean(true_reward_list[trial][-mean_number:])\n",
    "            true_avg_reward_list[trial].append(true_avg_reward)\n",
    "            \n",
    "            if output:\n",
    "                print(\"Ep {} * AvgReward {:.2f} * true AvgReward {:.2f} * Reward {:.2f} * True Reward {:.2f} * time {:.2f} * step {}\"\n",
    "                  .format(ep, avg_reward, true_avg_reward, episodic_reward, \n",
    "                          true_reward, (time.time() - before), step))\n",
    "            \n",
    "            # Stop if avg is above 'solved'\n",
    "            if true_avg_reward >= solved:\n",
    "                break\n",
    "\n",
    "        # Save weights\n",
    "        now = datetime.datetime.now()\n",
    "        timestamp = \"{}.{}.{}.{}.{}.{}\".format(now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "        save_name = \"{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}\".format(\n",
    "            environment, total_episodes, \n",
    "            buffer_capacity, batch_size, \n",
    "            std_dev, critic_lr, actor_lr, \n",
    "            gamma, tau, noise_mult, \n",
    "            gamma_func.__name__, tau_func.__name__, \n",
    "            critic_lr_func.__name__, actor_lr_func.__name__, \n",
    "            noise_mult_func.__name__, std_dev_func.__name__, \n",
    "            mean_number, use_guide, \n",
    "            solved, continuous, \n",
    "            start_steps, gravity, \n",
    "            enable_wind, wind_power,\n",
    "            turbulence_power, \n",
    "            epsilon, epsilon_func.__name__,\n",
    "            adam_critic_eps, adam_actor_eps,\n",
    "            actor_amsgrad, critic_amsgrad,\n",
    "            actor_layer_1, actor_layer_2,\n",
    "            critic_layer_1, critic_layer_2,\n",
    "            timestamp,\n",
    "        )\n",
    "        if save_weights:\n",
    "            try:\n",
    "                agent.actor_model.save_weights(directory + actor_name + '-trial' + str(trial) + '_' + save_name + '.h5')\n",
    "            except:\n",
    "                print('actor save fail')\n",
    "            try:\n",
    "                agent.critic_model.save_weights(directory + critic_name + '-trial' + str(trial) + '_' + save_name + '.h5')\n",
    "            except:\n",
    "                print('critic save fail')\n",
    "    \n",
    "    # Plotting graph\n",
    "    for idx, p in enumerate(true_avg_reward_list):\n",
    "        plt.plot(p, label=str(idx))\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True Avg. Epsiodic Reward (\" + str(mean_number) + \")\")\n",
    "    plt.legend()\n",
    "    plt.savefig('Graphs/' + save_name + '.png')\n",
    "    plt.show()\n",
    "    \n",
    "    print('total time:', time.time() - tot_time, 's')\n",
    "    \n",
    "    # Return to be able to make graphs etc. later, or use the data for other stuff\n",
    "    if return_rewards:\n",
    "        return true_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a57bcf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(total_episodes=10, actor_weights='Weights/actor-trial0.h5', render=False,\n",
    "        environment=\"LunarLander-v2\", continuous=True, gravity=-10.0, enable_wind=False,\n",
    "        wind_power=15.0, turbulence_power=1.5, seed=1453):\n",
    "    rewards = []\n",
    "    \n",
    "    env = gym.make(\n",
    "        environment,\n",
    "        continuous=continuous,\n",
    "        gravity=gravity,\n",
    "        enable_wind=enable_wind,\n",
    "        wind_power=wind_power,\n",
    "        turbulence_power=turbulence_power\n",
    "    )\n",
    "    \n",
    "    # This is needed to get the input size for the NN\n",
    "    num_states = env.observation_space.low.shape[0]\n",
    "    if continuous:\n",
    "        num_actions = env.action_space.shape[0]\n",
    "    else:\n",
    "        num_actions = 1\n",
    "\n",
    "    # Normalize action space according to https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
    "    action_space = spaces.Box(low=-1, high=1, shape=(num_actions,), dtype='float32')\n",
    "\n",
    "    # This is needed to clip the actions within the legal boundaries\n",
    "    upper_bound = action_space.high[0]\n",
    "    lower_bound = action_space.low[0]\n",
    "    \n",
    "    # Apply the seed\n",
    "    _ = env.reset(seed=seed)\n",
    "    \n",
    "    for ep in range(total_episodes):\n",
    "        ep_reward = 0\n",
    "        \n",
    "        # Used for time benchmarking\n",
    "        before = time.time()\n",
    "        \n",
    "        prev_state = env.reset()\n",
    "        agent = Agent(num_states=num_states, num_actions=num_actions, lower_bound=lower_bound, \n",
    "                upper_bound=upper_bound, continuous=continuous, buffer_capacity=0, batch_size=0, \n",
    "                std_dev=0, critic_lr=0, actor_lr=0, gamma=0, tau=0, epsilon=0)\n",
    "        agent.actor_model.load_weights(actor_weights)\n",
    "        \n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "            action = agent.policy(state=tf_prev_state, use_noise=False)\n",
    "\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            ep_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(str(time.time() - before) + 's')\n",
    "                rewards.append(ep_reward)\n",
    "                break\n",
    "\n",
    "            prev_state = state\n",
    "            \n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a90fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random(total_episodes=10, render=False, environment=\"LunarLander-v2\", continuous=True,\n",
    "        gravity=-10.0, enable_wind=False, wind_power=15.0, turbulence_power=1.5, seed=1453):\n",
    "    \n",
    "    rewards = []\n",
    "    \n",
    "    env = gym.make(\n",
    "        environment,\n",
    "        continuous=continuous,\n",
    "        gravity=gravity,\n",
    "        enable_wind=enable_wind,\n",
    "        wind_power=wind_power,\n",
    "        turbulence_power=turbulence_power,\n",
    "    )\n",
    "    \n",
    "    # Apply the seed\n",
    "    _ = env.reset(seed=seed)\n",
    "    \n",
    "    for ep in range(total_episodes):\n",
    "        ep_reward = 0\n",
    "        \n",
    "        before = time.time()\n",
    "        \n",
    "        prev_state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = env.action_space.sample()\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(str(time.time() - before) + 's')\n",
    "                rewards.append(ep_reward)\n",
    "                break\n",
    "\n",
    "            prev_state = state\n",
    "            \n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83b8ba",
   "metadata": {},
   "source": [
    "---\n",
    "# Runs and tests\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d14ddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xax = [x for x in range(-600,250)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cf6c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decreasing_std(x, episode):\n",
    "    return max(0, min(0.2, 0.2 - (x+500)*(0.2/700)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "058e4b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decreasing_alr(x, episode):\n",
    "    return max(0, min(0.001, 0.001 - (x+500)*(0.001/700)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "890d4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decreasing_clr(x, episode):\n",
    "    return max(0, min(0.002, 0.002 - (x+500)*(0.002/700)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a30c827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decreasing_eps(x, episode):\n",
    "    return max(0, min(0.3, 0.3 - (x+500)*(0.3/700)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab85e9ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fdc74c2340>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xax,[decreasing_std(x,1) for x in range(-600,250)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1adcbbb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#run(total_trials=2, total_episodes=2000, buffer_capacity=300000, tau=0.002, critic_lr=0.0003, \n",
    "#    actor_lr=0.0002, start_steps=10000, continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32e74ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run(total_trials=2, total_episodes=3000, buffer_capacity=25000, tau=0.001, critic_lr=0.0002, \n",
    "#     actor_lr=0.0001, start_steps=40000, continuous=False, std_dev_func=decreasing_std, actor_lr_func=decreasing_alr,\n",
    "#     critic_lr_func=decreasing_clr, epsilon_func=decreasing_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc100117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test(render=True, continuous=False, actor_weights='Weights/actor-trial0_LunarLander-v2_500_20000_64_0.3_0.0002_0.0001_0.99_0.001_1_fixed_fixed_fixed_fixed_fixed_fixed_20_False_200_False_5000_-10.0_False_15.0_1.5_0.2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "63a7cb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 0 * AvgReward -136.86 * true AvgReward -136.86 * Reward -136.86 * True Reward -136.86 * time 5.67 * step 81\n",
      "Ep 1 * AvgReward -144.18 * true AvgReward -144.18 * Reward -151.49 * True Reward -151.49 * time 4.58 * step 157\n",
      "Ep 2 * AvgReward -144.49 * true AvgReward -144.49 * Reward -145.11 * True Reward -145.11 * time 4.06 * step 229\n",
      "Ep 3 * AvgReward -119.67 * true AvgReward -119.67 * Reward -45.21 * True Reward -45.21 * time 4.10 * step 299\n",
      "Ep 4 * AvgReward -92.41 * true AvgReward -92.41 * Reward 16.64 * True Reward 16.64 * time 4.47 * step 377\n",
      "Ep 5 * AvgReward -92.87 * true AvgReward -92.87 * Reward -95.20 * True Reward -95.20 * time 3.25 * step 433\n",
      "Ep 6 * AvgReward -100.00 * true AvgReward -100.00 * Reward -142.75 * True Reward -142.75 * time 4.96 * step 516\n",
      "Ep 7 * AvgReward -102.90 * true AvgReward -102.90 * Reward -123.19 * True Reward -123.19 * time 4.16 * step 585\n",
      "Ep 8 * AvgReward -103.94 * true AvgReward -103.94 * Reward -112.32 * True Reward -112.32 * time 3.41 * step 643\n",
      "Ep 9 * AvgReward -107.43 * true AvgReward -107.43 * Reward -138.77 * True Reward -138.77 * time 4.17 * step 713\n",
      "Ep 10 * AvgReward -108.50 * true AvgReward -108.50 * Reward -119.20 * True Reward -119.20 * time 4.90 * step 795\n",
      "Ep 11 * AvgReward -96.55 * true AvgReward -96.55 * Reward 34.85 * True Reward 34.85 * time 5.42 * step 887\n",
      "Ep 12 * AvgReward -98.46 * true AvgReward -98.46 * Reward -121.39 * True Reward -121.39 * time 3.19 * step 941\n",
      "Ep 13 * AvgReward -100.88 * true AvgReward -100.88 * Reward -132.32 * True Reward -132.32 * time 2.98 * step 992\n",
      "Ep 14 * AvgReward -139.49 * true AvgReward -139.49 * Reward -679.97 * True Reward -679.97 * time 5.83 * step 1085\n",
      "Ep 15 * AvgReward -153.92 * true AvgReward -153.92 * Reward -370.38 * True Reward -370.38 * time 3.16 * step 1135\n",
      "Ep 16 * AvgReward -165.65 * true AvgReward -165.65 * Reward -353.41 * True Reward -353.41 * time 5.47 * step 1221\n",
      "Ep 17 * AvgReward -177.70 * true AvgReward -177.70 * Reward -382.59 * True Reward -382.59 * time 3.61 * step 1278\n",
      "Ep 18 * AvgReward -196.15 * true AvgReward -196.15 * Reward -528.17 * True Reward -528.17 * time 4.11 * step 1344\n",
      "Ep 19 * AvgReward -206.76 * true AvgReward -206.76 * Reward -408.45 * True Reward -408.45 * time 5.52 * step 1432\n",
      "Ep 20 * AvgReward -207.54 * true AvgReward -207.54 * Reward -152.36 * True Reward -152.36 * time 6.59 * step 1536\n",
      "Ep 21 * AvgReward -202.04 * true AvgReward -202.04 * Reward -41.49 * True Reward -41.49 * time 4.79 * step 1613\n",
      "Ep 22 * AvgReward -203.00 * true AvgReward -203.00 * Reward -164.39 * True Reward -164.39 * time 5.32 * step 1700\n",
      "Ep 23 * AvgReward -207.57 * true AvgReward -207.57 * Reward -136.58 * True Reward -136.58 * time 3.41 * step 1756\n",
      "Ep 24 * AvgReward -222.04 * true AvgReward -222.04 * Reward -272.77 * True Reward -272.77 * time 5.05 * step 1837\n",
      "Ep 25 * AvgReward -229.83 * true AvgReward -229.83 * Reward -250.89 * True Reward -250.89 * time 4.67 * step 1912\n",
      "Ep 26 * AvgReward -238.57 * true AvgReward -238.57 * Reward -317.53 * True Reward -317.53 * time 5.70 * step 1993\n",
      "Ep 27 * AvgReward -247.45 * true AvgReward -247.45 * Reward -300.88 * True Reward -300.88 * time 3.64 * step 2048\n",
      "Ep 28 * AvgReward -264.67 * true AvgReward -264.67 * Reward -456.69 * True Reward -456.69 * time 4.50 * step 2116\n",
      "Ep 29 * AvgReward -277.80 * true AvgReward -277.80 * Reward -401.33 * True Reward -401.33 * time 4.12 * step 2176\n",
      "Ep 30 * AvgReward -305.98 * true AvgReward -305.98 * Reward -682.90 * True Reward -682.90 * time 5.38 * step 2257\n",
      "Ep 31 * AvgReward -326.58 * true AvgReward -326.58 * Reward -377.06 * True Reward -377.06 * time 3.74 * step 2313\n",
      "Ep 32 * AvgReward -334.02 * true AvgReward -334.02 * Reward -270.28 * True Reward -270.28 * time 3.54 * step 2367\n",
      "Ep 33 * AvgReward -355.27 * true AvgReward -355.27 * Reward -557.32 * True Reward -557.32 * time 5.02 * step 2441\n",
      "Ep 34 * AvgReward -360.14 * true AvgReward -360.14 * Reward -777.32 * True Reward -777.32 * time 5.55 * step 2525\n",
      "Ep 35 * AvgReward -359.37 * true AvgReward -359.37 * Reward -355.05 * True Reward -355.05 * time 3.37 * step 2577\n",
      "Ep 36 * AvgReward -359.71 * true AvgReward -359.71 * Reward -360.15 * True Reward -360.15 * time 3.38 * step 2629\n",
      "Ep 37 * AvgReward -368.00 * true AvgReward -368.00 * Reward -548.49 * True Reward -548.49 * time 4.15 * step 2693\n",
      "Ep 38 * AvgReward -359.32 * true AvgReward -359.32 * Reward -354.57 * True Reward -354.57 * time 3.87 * step 2752\n",
      "Ep 39 * AvgReward -347.06 * true AvgReward -347.06 * Reward -163.10 * True Reward -163.10 * time 4.21 * step 2816\n",
      "Ep 40 * AvgReward -361.06 * true AvgReward -361.06 * Reward -432.48 * True Reward -432.48 * time 4.22 * step 2880\n",
      "Ep 41 * AvgReward -391.41 * true AvgReward -391.41 * Reward -648.41 * True Reward -648.41 * time 5.42 * step 2964\n",
      "Ep 42 * AvgReward -406.97 * true AvgReward -406.97 * Reward -475.55 * True Reward -475.55 * time 3.86 * step 3023\n",
      "Ep 43 * AvgReward -418.68 * true AvgReward -418.68 * Reward -370.86 * True Reward -370.86 * time 3.94 * step 3084\n",
      "Ep 44 * AvgReward -418.34 * true AvgReward -418.34 * Reward -265.92 * True Reward -265.92 * time 3.82 * step 3143\n",
      "Ep 45 * AvgReward -427.94 * true AvgReward -427.94 * Reward -442.95 * True Reward -442.95 * time 3.66 * step 3200\n",
      "Ep 46 * AvgReward -434.40 * true AvgReward -434.40 * Reward -446.61 * True Reward -446.61 * time 4.28 * step 3266\n",
      "Ep 47 * AvgReward -438.63 * true AvgReward -438.63 * Reward -385.47 * True Reward -385.47 * time 4.12 * step 3330\n",
      "Ep 48 * AvgReward -442.76 * true AvgReward -442.76 * Reward -539.34 * True Reward -539.34 * time 4.57 * step 3402\n",
      "Ep 49 * AvgReward -441.89 * true AvgReward -441.89 * Reward -383.98 * True Reward -383.98 * time 3.74 * step 3459\n",
      "Ep 50 * AvgReward -425.14 * true AvgReward -425.14 * Reward -348.00 * True Reward -348.00 * time 3.83 * step 3517\n",
      "Ep 51 * AvgReward -440.72 * true AvgReward -440.72 * Reward -688.50 * True Reward -688.50 * time 5.76 * step 3606\n",
      "Ep 52 * AvgReward -467.04 * true AvgReward -467.04 * Reward -796.81 * True Reward -796.81 * time 5.64 * step 3692\n",
      "Ep 53 * AvgReward -457.74 * true AvgReward -457.74 * Reward -371.33 * True Reward -371.33 * time 3.99 * step 3753\n",
      "Ep 54 * AvgReward -434.99 * true AvgReward -434.99 * Reward -322.26 * True Reward -322.26 * time 3.93 * step 3812\n",
      "Ep 55 * AvgReward -436.28 * true AvgReward -436.28 * Reward -380.83 * True Reward -380.83 * time 3.74 * step 3869\n",
      "Ep 56 * AvgReward -443.90 * true AvgReward -443.90 * Reward -512.48 * True Reward -512.48 * time 4.34 * step 3932\n",
      "Ep 57 * AvgReward -443.34 * true AvgReward -443.34 * Reward -537.39 * True Reward -537.39 * time 4.27 * step 3999\n",
      "Ep 58 * AvgReward -449.56 * true AvgReward -449.56 * Reward -478.97 * True Reward -478.97 * time 3.97 * step 4061\n",
      "Ep 59 * AvgReward -469.08 * true AvgReward -469.08 * Reward -553.48 * True Reward -553.48 * time 4.14 * step 4126\n",
      "Ep 60 * AvgReward -455.14 * true AvgReward -455.14 * Reward -153.71 * True Reward -153.71 * time 4.04 * step 4187\n",
      "Ep 61 * AvgReward -444.62 * true AvgReward -444.62 * Reward -437.95 * True Reward -437.95 * time 4.29 * step 4249\n",
      "Ep 62 * AvgReward -448.02 * true AvgReward -448.02 * Reward -543.55 * True Reward -543.55 * time 5.00 * step 4324\n",
      "Ep 63 * AvgReward -453.19 * true AvgReward -453.19 * Reward -474.38 * True Reward -474.38 * time 5.11 * step 4402\n",
      "Ep 64 * AvgReward -460.58 * true AvgReward -460.58 * Reward -413.62 * True Reward -413.62 * time 5.51 * step 4485\n",
      "Ep 65 * AvgReward -448.84 * true AvgReward -448.84 * Reward -208.20 * True Reward -208.20 * time 6.40 * step 4581\n",
      "Ep 66 * AvgReward -443.87 * true AvgReward -443.87 * Reward -347.20 * True Reward -347.20 * time 4.37 * step 4649\n",
      "Ep 67 * AvgReward -438.93 * true AvgReward -438.93 * Reward -286.69 * True Reward -286.69 * time 3.79 * step 4708\n",
      "Ep 68 * AvgReward -422.91 * true AvgReward -422.91 * Reward -218.87 * True Reward -218.87 * time 3.47 * step 4763\n",
      "Ep 69 * AvgReward -414.39 * true AvgReward -414.39 * Reward -213.65 * True Reward -213.65 * time 4.23 * step 4831\n",
      "Ep 70 * AvgReward -422.38 * true AvgReward -422.38 * Reward -507.79 * True Reward -507.79 * time 5.10 * step 4910\n",
      "Ep 71 * AvgReward -403.73 * true AvgReward -403.73 * Reward -315.53 * True Reward -315.53 * time 3.76 * step 4969\n",
      "Ep 72 * AvgReward -379.02 * true AvgReward -379.02 * Reward -302.45 * True Reward -302.45 * time 5.51 * step 5056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 73 * AvgReward -372.52 * true AvgReward -372.52 * Reward -241.50 * True Reward -241.50 * time 4.20 * step 5122\n",
      "Ep 74 * AvgReward -370.18 * true AvgReward -370.18 * Reward -275.32 * True Reward -275.32 * time 3.51 * step 5177\n",
      "Ep 75 * AvgReward -363.29 * true AvgReward -363.29 * Reward -243.08 * True Reward -243.08 * time 4.47 * step 5247\n",
      "Ep 76 * AvgReward -347.54 * true AvgReward -347.54 * Reward -197.43 * True Reward -197.43 * time 3.35 * step 5300\n",
      "Ep 77 * AvgReward -347.95 * true AvgReward -347.95 * Reward -545.65 * True Reward -545.65 * time 5.82 * step 5392\n",
      "Ep 78 * AvgReward -340.56 * true AvgReward -340.56 * Reward -331.12 * True Reward -331.12 * time 4.43 * step 5462\n",
      "Ep 79 * AvgReward -332.19 * true AvgReward -332.19 * Reward -386.19 * True Reward -386.19 * time 7.67 * step 5581\n",
      "Ep 80 * AvgReward -341.27 * true AvgReward -341.27 * Reward -335.27 * True Reward -335.27 * time 5.07 * step 5654\n",
      "Ep 81 * AvgReward -343.33 * true AvgReward -343.33 * Reward -479.17 * True Reward -479.17 * time 4.40 * step 5720\n",
      "Ep 82 * AvgReward -332.90 * true AvgReward -332.90 * Reward -334.93 * True Reward -334.93 * time 4.46 * step 5789\n",
      "Ep 83 * AvgReward -319.40 * true AvgReward -319.40 * Reward -204.29 * True Reward -204.29 * time 5.41 * step 5874\n",
      "Ep 84 * AvgReward -306.13 * true AvgReward -306.13 * Reward -148.24 * True Reward -148.24 * time 4.26 * step 5938\n",
      "Ep 85 * AvgReward -302.78 * true AvgReward -302.78 * Reward -141.21 * True Reward -141.21 * time 5.83 * step 6027\n",
      "Ep 86 * AvgReward -292.92 * true AvgReward -292.92 * Reward -150.03 * True Reward -150.03 * time 4.73 * step 6099\n",
      "Ep 87 * AvgReward -286.03 * true AvgReward -286.03 * Reward -148.91 * True Reward -148.91 * time 4.78 * step 6173\n",
      "Ep 88 * AvgReward -285.44 * true AvgReward -285.44 * Reward -207.11 * True Reward -207.11 * time 6.57 * step 6275\n",
      "Ep 89 * AvgReward -281.16 * true AvgReward -281.16 * Reward -127.91 * True Reward -127.91 * time 4.12 * step 6338\n",
      "Ep 90 * AvgReward -261.76 * true AvgReward -261.76 * Reward -119.92 * True Reward -119.92 * time 5.85 * step 6430\n",
      "Ep 91 * AvgReward -254.67 * true AvgReward -254.67 * Reward -173.58 * True Reward -173.58 * time 6.00 * step 6526\n",
      "Ep 92 * AvgReward -246.43 * true AvgReward -246.43 * Reward -137.66 * True Reward -137.66 * time 5.21 * step 6609\n",
      "Ep 93 * AvgReward -238.19 * true AvgReward -238.19 * Reward -76.84 * True Reward -76.84 * time 3.46 * step 6664\n",
      "Ep 94 * AvgReward -231.66 * true AvgReward -231.66 * Reward -144.67 * True Reward -144.67 * time 5.95 * step 6756\n",
      "Ep 95 * AvgReward -226.01 * true AvgReward -226.01 * Reward -129.98 * True Reward -129.98 * time 4.32 * step 6823\n",
      "Ep 96 * AvgReward -221.22 * true AvgReward -221.22 * Reward -101.68 * True Reward -101.68 * time 3.69 * step 6881\n",
      "Ep 97 * AvgReward -201.37 * true AvgReward -201.37 * Reward -148.60 * True Reward -148.60 * time 4.89 * step 6954\n",
      "Ep 98 * AvgReward -192.33 * true AvgReward -192.33 * Reward -150.30 * True Reward -150.30 * time 5.37 * step 7035\n",
      "Ep 99 * AvgReward -177.53 * true AvgReward -177.53 * Reward -90.25 * True Reward -90.25 * time 3.99 * step 7097\n",
      "Ep 100 * AvgReward -166.86 * true AvgReward -166.86 * Reward -121.92 * True Reward -121.92 * time 3.86 * step 7157\n",
      "Ep 101 * AvgReward -146.94 * true AvgReward -146.94 * Reward -80.78 * True Reward -80.78 * time 3.74 * step 7215\n",
      "Ep 102 * AvgReward -133.78 * true AvgReward -133.78 * Reward -71.74 * True Reward -71.74 * time 4.67 * step 7288\n",
      "Ep 103 * AvgReward -130.52 * true AvgReward -130.52 * Reward -139.16 * True Reward -139.16 * time 6.14 * step 7382\n",
      "Ep 104 * AvgReward -128.28 * true AvgReward -128.28 * Reward -103.34 * True Reward -103.34 * time 3.92 * step 7442\n",
      "Ep 105 * AvgReward -124.64 * true AvgReward -124.64 * Reward -68.50 * True Reward -68.50 * time 5.92 * step 7531\n",
      "Ep 106 * AvgReward -121.07 * true AvgReward -121.07 * Reward -78.50 * True Reward -78.50 * time 4.47 * step 7600\n",
      "Ep 107 * AvgReward -117.13 * true AvgReward -117.13 * Reward -70.15 * True Reward -70.15 * time 5.69 * step 7683\n",
      "Ep 108 * AvgReward -111.67 * true AvgReward -111.67 * Reward -97.93 * True Reward -97.93 * time 4.46 * step 7751\n",
      "Ep 109 * AvgReward -102.51 * true AvgReward -102.51 * Reward 55.25 * True Reward 55.25 * time 8.28 * step 7874\n",
      "Ep 110 * AvgReward -113.49 * true AvgReward -113.49 * Reward -339.49 * True Reward -339.49 * time 10.95 * step 8036\n",
      "Ep 111 * AvgReward -118.27 * true AvgReward -118.27 * Reward -269.15 * True Reward -269.15 * time 7.55 * step 8149\n",
      "Ep 112 * AvgReward -127.42 * true AvgReward -127.42 * Reward -320.66 * True Reward -320.66 * time 17.88 * step 8420\n",
      "Ep 113 * AvgReward -135.45 * true AvgReward -135.45 * Reward -237.48 * True Reward -237.48 * time 43.32 * step 9072\n",
      "Ep 114 * AvgReward -152.59 * true AvgReward -152.59 * Reward -487.51 * True Reward -487.51 * time 24.12 * step 9435\n",
      "Ep 115 * AvgReward -152.25 * true AvgReward -152.25 * Reward -123.06 * True Reward -123.06 * time 23.76 * step 9790\n",
      "Ep 116 * AvgReward -154.88 * true AvgReward -154.88 * Reward -154.32 * True Reward -154.32 * time 19.85 * step 10080\n",
      "Ep 117 * AvgReward -152.83 * true AvgReward -152.83 * Reward -107.69 * True Reward -107.69 * time 8.95 * step 10214\n",
      "Ep 118 * AvgReward -160.52 * true AvgReward -160.52 * Reward -304.11 * True Reward -304.11 * time 7.24 * step 10323\n",
      "Ep 119 * AvgReward -166.31 * true AvgReward -166.31 * Reward -205.97 * True Reward -205.97 * time 28.23 * step 10743\n",
      "Ep 120 * AvgReward -169.68 * true AvgReward -169.68 * Reward -189.26 * True Reward -189.26 * time 22.17 * step 11085\n",
      "Ep 121 * AvgReward -182.72 * true AvgReward -182.72 * Reward -341.71 * True Reward -341.71 * time 16.01 * step 11322\n",
      "Ep 122 * AvgReward -189.37 * true AvgReward -189.37 * Reward -204.61 * True Reward -204.61 * time 12.27 * step 11507\n",
      "Ep 123 * AvgReward -192.03 * true AvgReward -192.03 * Reward -192.43 * True Reward -192.43 * time 8.80 * step 11638\n",
      "Ep 124 * AvgReward -198.66 * true AvgReward -198.66 * Reward -235.98 * True Reward -235.98 * time 12.28 * step 11824\n",
      "Ep 125 * AvgReward -203.79 * true AvgReward -203.79 * Reward -171.08 * True Reward -171.08 * time 9.01 * step 11960\n",
      "Ep 126 * AvgReward -207.85 * true AvgReward -207.85 * Reward -159.59 * True Reward -159.59 * time 9.95 * step 12111\n",
      "Ep 127 * AvgReward -216.33 * true AvgReward -216.33 * Reward -239.89 * True Reward -239.89 * time 16.13 * step 12356\n",
      "Ep 128 * AvgReward -225.59 * true AvgReward -225.59 * Reward -283.02 * True Reward -283.02 * time 20.47 * step 12671\n",
      "Ep 129 * AvgReward -237.84 * true AvgReward -237.84 * Reward -189.85 * True Reward -189.85 * time 6.08 * step 12765\n",
      "Ep 130 * AvgReward -234.02 * true AvgReward -234.02 * Reward -263.11 * True Reward -263.11 * time 17.84 * step 13043\n",
      "Ep 131 * AvgReward -233.42 * true AvgReward -233.42 * Reward -257.00 * True Reward -257.00 * time 11.31 * step 13220\n",
      "Ep 132 * AvgReward -227.93 * true AvgReward -227.93 * Reward -210.90 * True Reward -210.90 * time 8.73 * step 13357\n",
      "Ep 133 * AvgReward -231.16 * true AvgReward -231.16 * Reward -302.13 * True Reward -302.13 * time 11.90 * step 13545\n",
      "Ep 134 * AvgReward -220.41 * true AvgReward -220.41 * Reward -272.56 * True Reward -272.56 * time 10.62 * step 13707\n",
      "Ep 135 * AvgReward -225.97 * true AvgReward -225.97 * Reward -234.24 * True Reward -234.24 * time 9.39 * step 13853\n",
      "Ep 136 * AvgReward -233.76 * true AvgReward -233.76 * Reward -310.15 * True Reward -310.15 * time 12.28 * step 14040\n",
      "Ep 137 * AvgReward -242.85 * true AvgReward -242.85 * Reward -289.45 * True Reward -289.45 * time 9.18 * step 14180\n",
      "Ep 138 * AvgReward -239.69 * true AvgReward -239.69 * Reward -240.78 * True Reward -240.78 * time 7.37 * step 14292\n",
      "Ep 139 * AvgReward -239.54 * true AvgReward -239.54 * Reward -203.01 * True Reward -203.01 * time 19.47 * step 14590\n",
      "Ep 140 * AvgReward -239.08 * true AvgReward -239.08 * Reward -180.10 * True Reward -180.10 * time 10.78 * step 14749\n",
      "Ep 141 * AvgReward -233.57 * true AvgReward -233.57 * Reward -231.56 * True Reward -231.56 * time 14.93 * step 14975\n",
      "Ep 142 * AvgReward -233.55 * true AvgReward -233.55 * Reward -204.19 * True Reward -204.19 * time 11.84 * step 15158\n",
      "Ep 143 * AvgReward -235.60 * true AvgReward -235.60 * Reward -233.50 * True Reward -233.50 * time 13.06 * step 15356\n",
      "Ep 144 * AvgReward -234.47 * true AvgReward -234.47 * Reward -213.37 * True Reward -213.37 * time 11.41 * step 15528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 145 * AvgReward -236.85 * true AvgReward -236.85 * Reward -218.55 * True Reward -218.55 * time 10.50 * step 15688\n",
      "Ep 146 * AvgReward -238.62 * true AvgReward -238.62 * Reward -195.10 * True Reward -195.10 * time 10.22 * step 15845\n",
      "Ep 147 * AvgReward -236.50 * true AvgReward -236.50 * Reward -197.37 * True Reward -197.37 * time 12.09 * step 16029\n",
      "Ep 148 * AvgReward -230.87 * true AvgReward -230.87 * Reward -170.56 * True Reward -170.56 * time 13.05 * step 16227\n",
      "Ep 149 * AvgReward -229.79 * true AvgReward -229.79 * Reward -168.09 * True Reward -168.09 * time 10.66 * step 16391\n",
      "Ep 150 * AvgReward -223.43 * true AvgReward -223.43 * Reward -135.93 * True Reward -135.93 * time 13.82 * step 16597\n",
      "Ep 151 * AvgReward -222.45 * true AvgReward -222.45 * Reward -237.39 * True Reward -237.39 * time 30.63 * step 17068\n",
      "Ep 152 * AvgReward -222.96 * true AvgReward -222.96 * Reward -221.19 * True Reward -221.19 * time 20.29 * step 17377\n",
      "Ep 153 * AvgReward -217.85 * true AvgReward -217.85 * Reward -199.95 * True Reward -199.95 * time 49.08 * step 18089\n",
      "Ep 154 * AvgReward -212.67 * true AvgReward -212.67 * Reward -168.87 * True Reward -168.87 * time 27.91 * step 18498\n",
      "Ep 155 * AvgReward -207.39 * true AvgReward -207.39 * Reward -128.63 * True Reward -128.63 * time 16.73 * step 18745\n",
      "Ep 156 * AvgReward -197.14 * true AvgReward -197.14 * Reward -105.22 * True Reward -105.22 * time 69.74 * step 19745\n",
      "Ep 157 * AvgReward -191.10 * true AvgReward -191.10 * Reward -168.59 * True Reward -168.59 * time 35.13 * step 20256\n",
      "Ep 158 * AvgReward -181.31 * true AvgReward -181.31 * Reward -45.11 * True Reward -45.11 * time 41.83 * step 20877\n",
      "Ep 159 * AvgReward -179.95 * true AvgReward -179.95 * Reward -175.65 * True Reward -175.65 * time 68.37 * step 21877\n",
      "Ep 160 * AvgReward -179.48 * true AvgReward -179.48 * Reward -170.83 * True Reward -170.83 * time 65.91 * step 22877\n",
      "Ep 161 * AvgReward -177.91 * true AvgReward -177.91 * Reward -200.09 * True Reward -200.09 * time 66.27 * step 23877\n",
      "Ep 162 * AvgReward -182.45 * true AvgReward -182.45 * Reward -295.06 * True Reward -295.06 * time 47.66 * step 24576\n",
      "Ep 163 * AvgReward -185.45 * true AvgReward -185.45 * Reward -293.53 * True Reward -293.53 * time 56.98 * step 25428\n",
      "Ep 164 * AvgReward -186.81 * true AvgReward -186.81 * Reward -240.42 * True Reward -240.42 * time 17.87 * step 25706\n",
      "Ep 165 * AvgReward -189.82 * true AvgReward -189.82 * Reward -278.82 * True Reward -278.82 * time 31.11 * step 26190\n",
      "Ep 166 * AvgReward -184.97 * true AvgReward -184.97 * Reward -98.06 * True Reward -98.06 * time 10.23 * step 26347\n",
      "Ep 167 * AvgReward -190.77 * true AvgReward -190.77 * Reward -313.28 * True Reward -313.28 * time 36.85 * step 26917\n",
      "Ep 168 * AvgReward -186.27 * true AvgReward -186.27 * Reward -80.69 * True Reward -80.69 * time 24.60 * step 27304\n",
      "Ep 169 * AvgReward -198.64 * true AvgReward -198.64 * Reward -415.40 * True Reward -415.40 * time 25.47 * step 27702\n",
      "Ep 170 * AvgReward -203.99 * true AvgReward -203.99 * Reward -242.93 * True Reward -242.93 * time 64.34 * step 28646\n",
      "Ep 171 * AvgReward -207.12 * true AvgReward -207.12 * Reward -300.04 * True Reward -300.04 * time 23.22 * step 28984\n",
      "Ep 172 * AvgReward -209.26 * true AvgReward -209.26 * Reward -264.10 * True Reward -264.10 * time 22.64 * step 29314\n",
      "Ep 173 * AvgReward -213.29 * true AvgReward -213.29 * Reward -280.50 * True Reward -280.50 * time 12.58 * step 29504\n",
      "Ep 174 * AvgReward -215.69 * true AvgReward -215.69 * Reward -216.85 * True Reward -216.85 * time 53.18 * step 30285\n",
      "Ep 175 * AvgReward -209.18 * true AvgReward -209.18 * Reward 1.62 * True Reward 1.62 * time 21.61 * step 30608\n",
      "Ep 176 * AvgReward -206.62 * true AvgReward -206.62 * Reward -54.08 * True Reward -54.08 * time 9.40 * step 30747\n",
      "Ep 177 * AvgReward -202.84 * true AvgReward -202.84 * Reward -92.97 * True Reward -92.97 * time 5.31 * step 30827\n",
      "Ep 178 * AvgReward -209.95 * true AvgReward -209.95 * Reward -187.29 * True Reward -187.29 * time 8.74 * step 30961\n",
      "Ep 179 * AvgReward -200.75 * true AvgReward -200.75 * Reward 8.29 * True Reward 8.29 * time 4.82 * step 31035\n",
      "Ep 180 * AvgReward -197.25 * true AvgReward -197.25 * Reward -100.84 * True Reward -100.84 * time 14.22 * step 31252\n",
      "Ep 181 * AvgReward -191.68 * true AvgReward -191.68 * Reward -88.74 * True Reward -88.74 * time 13.77 * step 31464\n",
      "Ep 182 * AvgReward -183.72 * true AvgReward -183.72 * Reward -135.80 * True Reward -135.80 * time 7.79 * step 31583\n",
      "Ep 183 * AvgReward -179.15 * true AvgReward -179.15 * Reward -202.16 * True Reward -202.16 * time 18.01 * step 31862\n",
      "Ep 184 * AvgReward -171.87 * true AvgReward -171.87 * Reward -94.86 * True Reward -94.86 * time 7.59 * step 31979\n",
      "Ep 185 * AvgReward -165.09 * true AvgReward -165.09 * Reward -143.06 * True Reward -143.06 * time 6.20 * step 32076\n",
      "Ep 186 * AvgReward -165.86 * true AvgReward -165.86 * Reward -113.50 * True Reward -113.50 * time 28.54 * step 32520\n",
      "Ep 187 * AvgReward -156.83 * true AvgReward -156.83 * Reward -132.79 * True Reward -132.79 * time 32.80 * step 33029\n",
      "Ep 188 * AvgReward -165.93 * true AvgReward -165.93 * Reward -262.59 * True Reward -262.59 * time 18.11 * step 33312\n",
      "Ep 189 * AvgReward -149.76 * true AvgReward -149.76 * Reward -92.03 * True Reward -92.03 * time 10.10 * step 33467\n",
      "Ep 190 * AvgReward -145.15 * true AvgReward -145.15 * Reward -150.68 * True Reward -150.68 * time 12.20 * step 33657\n",
      "Ep 191 * AvgReward -141.18 * true AvgReward -141.18 * Reward -220.68 * True Reward -220.68 * time 15.63 * step 33892\n",
      "Ep 192 * AvgReward -134.35 * true AvgReward -134.35 * Reward -127.42 * True Reward -127.42 * time 9.37 * step 34037\n",
      "Ep 193 * AvgReward -125.97 * true AvgReward -125.97 * Reward -112.94 * True Reward -112.94 * time 7.57 * step 34152\n",
      "Ep 194 * AvgReward -120.33 * true AvgReward -120.33 * Reward -104.14 * True Reward -104.14 * time 8.46 * step 34282\n",
      "Ep 195 * AvgReward -129.09 * true AvgReward -129.09 * Reward -173.47 * True Reward -173.47 * time 13.50 * step 34491\n",
      "Ep 196 * AvgReward -136.36 * true AvgReward -136.36 * Reward -199.46 * True Reward -199.46 * time 14.42 * step 34717\n",
      "Ep 197 * AvgReward -139.80 * true AvgReward -139.80 * Reward -161.80 * True Reward -161.80 * time 20.59 * step 35031\n",
      "Ep 198 * AvgReward -140.41 * true AvgReward -140.41 * Reward -199.62 * True Reward -199.62 * time 13.09 * step 35230\n",
      "Ep 199 * AvgReward -154.44 * true AvgReward -154.44 * Reward -272.15 * True Reward -272.15 * time 12.42 * step 35417\n",
      "Ep 200 * AvgReward -159.35 * true AvgReward -159.35 * Reward -199.02 * True Reward -199.02 * time 14.70 * step 35638\n",
      "Ep 201 * AvgReward -161.00 * true AvgReward -161.00 * Reward -121.75 * True Reward -121.75 * time 7.04 * step 35744\n",
      "Ep 202 * AvgReward -161.38 * true AvgReward -161.38 * Reward -143.42 * True Reward -143.42 * time 8.60 * step 35875\n",
      "Ep 203 * AvgReward -159.65 * true AvgReward -159.65 * Reward -167.56 * True Reward -167.56 * time 13.71 * step 36078\n",
      "Ep 204 * AvgReward -161.71 * true AvgReward -161.71 * Reward -136.18 * True Reward -136.18 * time 10.58 * step 36236\n",
      "Ep 205 * AvgReward -164.14 * true AvgReward -164.14 * Reward -191.55 * True Reward -191.55 * time 12.67 * step 36424\n",
      "Ep 206 * AvgReward -167.13 * true AvgReward -167.13 * Reward -173.25 * True Reward -173.25 * time 12.37 * step 36606\n",
      "Ep 207 * AvgReward -170.26 * true AvgReward -170.26 * Reward -195.41 * True Reward -195.41 * time 13.37 * step 36804\n",
      "Ep 208 * AvgReward -165.88 * true AvgReward -165.88 * Reward -175.08 * True Reward -175.08 * time 10.87 * step 36967\n",
      "Ep 209 * AvgReward -168.20 * true AvgReward -168.20 * Reward -138.42 * True Reward -138.42 * time 12.98 * step 37164\n",
      "Ep 210 * AvgReward -168.91 * true AvgReward -168.91 * Reward -164.81 * True Reward -164.81 * time 10.02 * step 37311\n",
      "Ep 211 * AvgReward -165.76 * true AvgReward -165.76 * Reward -157.81 * True Reward -157.81 * time 9.61 * step 37460\n",
      "Ep 212 * AvgReward -166.65 * true AvgReward -166.65 * Reward -145.23 * True Reward -145.23 * time 8.50 * step 37590\n",
      "Ep 213 * AvgReward -168.43 * true AvgReward -168.43 * Reward -148.52 * True Reward -148.52 * time 7.43 * step 37705\n",
      "Ep 214 * AvgReward -171.80 * true AvgReward -171.80 * Reward -171.49 * True Reward -171.49 * time 9.82 * step 37855\n",
      "Ep 215 * AvgReward -172.50 * true AvgReward -172.50 * Reward -187.54 * True Reward -187.54 * time 14.38 * step 38067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 216 * AvgReward -172.73 * true AvgReward -172.73 * Reward -203.94 * True Reward -203.94 * time 12.43 * step 38250\n",
      "Ep 217 * AvgReward -175.92 * true AvgReward -175.92 * Reward -225.67 * True Reward -225.67 * time 6.73 * step 38350\n",
      "Ep 218 * AvgReward -176.02 * true AvgReward -176.02 * Reward -201.59 * True Reward -201.59 * time 12.89 * step 38539\n",
      "Ep 219 * AvgReward -170.48 * true AvgReward -170.48 * Reward -161.34 * True Reward -161.34 * time 6.32 * step 38632\n",
      "Ep 220 * AvgReward -169.62 * true AvgReward -169.62 * Reward -181.85 * True Reward -181.85 * time 14.81 * step 38850\n",
      "Ep 221 * AvgReward -170.13 * true AvgReward -170.13 * Reward -131.93 * True Reward -131.93 * time 9.08 * step 38984\n",
      "Ep 222 * AvgReward -166.25 * true AvgReward -166.25 * Reward -65.90 * True Reward -65.90 * time 8.77 * step 39115\n",
      "Ep 223 * AvgReward -167.65 * true AvgReward -167.65 * Reward -195.54 * True Reward -195.54 * time 27.44 * step 39521\n",
      "Ep 224 * AvgReward -170.57 * true AvgReward -170.57 * Reward -194.58 * True Reward -194.58 * time 6.08 * step 39610\n",
      "Ep 225 * AvgReward -165.00 * true AvgReward -165.00 * Reward -80.17 * True Reward -80.17 * time 22.15 * step 39936\n",
      "Ep 226 * AvgReward -164.34 * true AvgReward -164.34 * Reward -160.06 * True Reward -160.06 * time 16.63 * step 40183\n",
      "Ep 227 * AvgReward -161.73 * true AvgReward -161.73 * Reward -143.09 * True Reward -143.09 * time 7.02 * step 40286\n",
      "Ep 228 * AvgReward -156.13 * true AvgReward -156.13 * Reward -63.05 * True Reward -63.05 * time 22.07 * step 40614\n",
      "Ep 229 * AvgReward -159.62 * true AvgReward -159.62 * Reward -208.21 * True Reward -208.21 * time 9.64 * step 40761\n",
      "Ep 230 * AvgReward -173.44 * true AvgReward -173.44 * Reward -441.25 * True Reward -441.25 * time 18.99 * step 41051\n",
      "Ep 231 * AvgReward -170.54 * true AvgReward -170.54 * Reward -99.86 * True Reward -99.86 * time 25.85 * step 41432\n",
      "Ep 232 * AvgReward -171.62 * true AvgReward -171.62 * Reward -166.83 * True Reward -166.83 * time 13.88 * step 41642\n",
      "Ep 233 * AvgReward -173.15 * true AvgReward -173.15 * Reward -179.08 * True Reward -179.08 * time 24.67 * step 41999\n",
      "Ep 234 * AvgReward -168.66 * true AvgReward -168.66 * Reward -81.75 * True Reward -81.75 * time 12.41 * step 42182\n",
      "Ep 235 * AvgReward -163.12 * true AvgReward -163.12 * Reward -76.61 * True Reward -76.61 * time 26.55 * step 42574\n",
      "Ep 236 * AvgReward -157.39 * true AvgReward -157.39 * Reward -89.43 * True Reward -89.43 * time 9.46 * step 42718\n",
      "Ep 237 * AvgReward -153.47 * true AvgReward -153.47 * Reward -147.20 * True Reward -147.20 * time 7.08 * step 42829\n",
      "Ep 238 * AvgReward -148.22 * true AvgReward -148.22 * Reward -96.56 * True Reward -96.56 * time 13.85 * step 43048\n",
      "Ep 239 * AvgReward -139.96 * true AvgReward -139.96 * Reward 3.81 * True Reward 3.81 * time 65.15 * step 44048\n",
      "Ep 240 * AvgReward -126.69 * true AvgReward -126.69 * Reward 83.49 * True Reward 83.49 * time 67.03 * step 45048\n",
      "Ep 241 * AvgReward -126.89 * true AvgReward -126.89 * Reward -136.01 * True Reward -136.01 * time 10.71 * step 45210\n",
      "Ep 242 * AvgReward -132.62 * true AvgReward -132.62 * Reward -180.49 * True Reward -180.49 * time 13.39 * step 45415\n",
      "Ep 243 * AvgReward -131.40 * true AvgReward -131.40 * Reward -171.15 * True Reward -171.15 * time 67.63 * step 46415\n",
      "Ep 244 * AvgReward -129.75 * true AvgReward -129.75 * Reward -161.42 * True Reward -161.42 * time 32.14 * step 46901\n",
      "Ep 245 * AvgReward -135.32 * true AvgReward -135.32 * Reward -191.63 * True Reward -191.63 * time 24.20 * step 47273\n",
      "Ep 246 * AvgReward -137.85 * true AvgReward -137.85 * Reward -210.62 * True Reward -210.62 * time 15.08 * step 47498\n",
      "Ep 247 * AvgReward -140.87 * true AvgReward -140.87 * Reward -203.48 * True Reward -203.48 * time 15.31 * step 47726\n",
      "Ep 248 * AvgReward -142.97 * true AvgReward -142.97 * Reward -105.12 * True Reward -105.12 * time 7.82 * step 47844\n",
      "Ep 249 * AvgReward -141.56 * true AvgReward -141.56 * Reward -180.02 * True Reward -180.02 * time 34.42 * step 48352\n",
      "Ep 250 * AvgReward -129.98 * true AvgReward -129.98 * Reward -209.64 * True Reward -209.64 * time 32.86 * step 48830\n",
      "Ep 251 * AvgReward -127.93 * true AvgReward -127.93 * Reward -58.93 * True Reward -58.93 * time 41.95 * step 49442\n",
      "Ep 252 * AvgReward -130.88 * true AvgReward -130.88 * Reward -225.74 * True Reward -225.74 * time 30.59 * step 49883\n",
      "Ep 253 * AvgReward -128.96 * true AvgReward -128.96 * Reward -140.80 * True Reward -140.80 * time 20.38 * step 50184\n",
      "Ep 254 * AvgReward -121.18 * true AvgReward -121.18 * Reward 73.90 * True Reward 73.90 * time 45.07 * step 50853\n",
      "Ep 255 * AvgReward -116.38 * true AvgReward -116.38 * Reward 19.48 * True Reward 19.48 * time 19.19 * step 51137\n",
      "Ep 256 * AvgReward -116.04 * true AvgReward -116.04 * Reward -82.72 * True Reward -82.72 * time 23.32 * step 51482\n",
      "Ep 257 * AvgReward -114.16 * true AvgReward -114.16 * Reward -109.62 * True Reward -109.62 * time 34.97 * step 52000\n",
      "Ep 258 * AvgReward -111.59 * true AvgReward -111.59 * Reward -45.08 * True Reward -45.08 * time 18.09 * step 52283\n",
      "Ep 259 * AvgReward -128.86 * true AvgReward -128.86 * Reward -341.60 * True Reward -341.60 * time 29.22 * step 52734\n",
      "Ep 260 * AvgReward -135.90 * true AvgReward -135.90 * Reward -57.25 * True Reward -57.25 * time 36.63 * step 53303\n",
      "Ep 261 * AvgReward -138.21 * true AvgReward -138.21 * Reward -182.30 * True Reward -182.30 * time 26.43 * step 53706\n",
      "Ep 262 * AvgReward -131.34 * true AvgReward -131.34 * Reward -42.97 * True Reward -42.97 * time 27.44 * step 54122\n",
      "Ep 263 * AvgReward -133.49 * true AvgReward -133.49 * Reward -214.32 * True Reward -214.32 * time 43.43 * step 54774\n",
      "Ep 264 * AvgReward -137.98 * true AvgReward -137.98 * Reward -251.22 * True Reward -251.22 * time 39.79 * step 55368\n",
      "Ep 265 * AvgReward -139.60 * true AvgReward -139.60 * Reward -223.96 * True Reward -223.96 * time 68.90 * step 56368\n",
      "Ep 266 * AvgReward -130.98 * true AvgReward -130.98 * Reward -38.23 * True Reward -38.23 * time 68.04 * step 57368\n",
      "Ep 267 * AvgReward -129.10 * true AvgReward -129.10 * Reward -165.84 * True Reward -165.84 * time 59.67 * step 58222\n",
      "Ep 268 * AvgReward -127.12 * true AvgReward -127.12 * Reward -65.44 * True Reward -65.44 * time 36.86 * step 58753\n",
      "Ep 269 * AvgReward -126.99 * true AvgReward -126.99 * Reward -177.43 * True Reward -177.43 * time 48.96 * step 59469\n",
      "Ep 270 * AvgReward -110.95 * true AvgReward -110.95 * Reward 111.02 * True Reward 111.02 * time 42.97 * step 60110\n",
      "Ep 271 * AvgReward -109.81 * true AvgReward -109.81 * Reward -35.98 * True Reward -35.98 * time 67.74 * step 61110\n",
      "Ep 272 * AvgReward -110.29 * true AvgReward -110.29 * Reward -235.43 * True Reward -235.43 * time 68.21 * step 62110\n",
      "Ep 273 * AvgReward -110.00 * true AvgReward -110.00 * Reward -135.00 * True Reward -135.00 * time 29.05 * step 62538\n",
      "Ep 274 * AvgReward -119.84 * true AvgReward -119.84 * Reward -122.91 * True Reward -122.91 * time 11.49 * step 62707\n",
      "Ep 275 * AvgReward -129.96 * true AvgReward -129.96 * Reward -183.00 * True Reward -183.00 * time 19.70 * step 62994\n",
      "Ep 276 * AvgReward -133.10 * true AvgReward -133.10 * Reward -145.48 * True Reward -145.48 * time 16.37 * step 63238\n",
      "Ep 277 * AvgReward -135.72 * true AvgReward -135.72 * Reward -161.97 * True Reward -161.97 * time 16.24 * step 63484\n",
      "Ep 278 * AvgReward -140.96 * true AvgReward -140.96 * Reward -149.93 * True Reward -149.93 * time 12.66 * step 63675\n",
      "Ep 279 * AvgReward -129.83 * true AvgReward -129.83 * Reward -118.88 * True Reward -118.88 * time 10.70 * step 63836\n",
      "Ep 280 * AvgReward -131.81 * true AvgReward -131.81 * Reward -96.98 * True Reward -96.98 * time 10.64 * step 63998\n",
      "Ep 281 * AvgReward -125.25 * true AvgReward -125.25 * Reward -51.06 * True Reward -51.06 * time 25.96 * step 64386\n",
      "Ep 282 * AvgReward -132.19 * true AvgReward -132.19 * Reward -181.85 * True Reward -181.85 * time 13.43 * step 64586\n",
      "Ep 283 * AvgReward -131.54 * true AvgReward -131.54 * Reward -201.18 * True Reward -201.18 * time 13.77 * step 64796\n",
      "Ep 284 * AvgReward -128.19 * true AvgReward -128.19 * Reward -184.32 * True Reward -184.32 * time 13.27 * step 64998\n",
      "Ep 285 * AvgReward -126.67 * true AvgReward -126.67 * Reward -193.51 * True Reward -193.51 * time 13.05 * step 65196\n",
      "Ep 286 * AvgReward -134.33 * true AvgReward -134.33 * Reward -191.41 * True Reward -191.41 * time 9.60 * step 65340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 287 * AvgReward -133.57 * true AvgReward -133.57 * Reward -150.70 * True Reward -150.70 * time 9.66 * step 65478\n",
      "Ep 288 * AvgReward -140.72 * true AvgReward -140.72 * Reward -208.38 * True Reward -208.38 * time 15.26 * step 65701\n",
      "Ep 289 * AvgReward -143.01 * true AvgReward -143.01 * Reward -223.25 * True Reward -223.25 * time 14.56 * step 65918\n",
      "Ep 290 * AvgReward -158.58 * true AvgReward -158.58 * Reward -200.38 * True Reward -200.38 * time 15.41 * step 66145\n",
      "Ep 291 * AvgReward -165.80 * true AvgReward -165.80 * Reward -180.40 * True Reward -180.40 * time 14.06 * step 66352\n",
      "Ep 292 * AvgReward -163.29 * true AvgReward -163.29 * Reward -185.27 * True Reward -185.27 * time 19.52 * step 66646\n",
      "Ep 293 * AvgReward -160.67 * true AvgReward -160.67 * Reward -82.50 * True Reward -82.50 * time 32.19 * step 67129\n",
      "Ep 294 * AvgReward -161.29 * true AvgReward -161.29 * Reward -135.32 * True Reward -135.32 * time 69.77 * step 68129\n",
      "Ep 295 * AvgReward -160.41 * true AvgReward -160.41 * Reward -165.50 * True Reward -165.50 * time 69.93 * step 69129\n",
      "Ep 296 * AvgReward -163.67 * true AvgReward -163.67 * Reward -210.62 * True Reward -210.62 * time 32.42 * step 69609\n",
      "Ep 297 * AvgReward -164.97 * true AvgReward -164.97 * Reward -187.91 * True Reward -187.91 * time 22.26 * step 69944\n",
      "Ep 298 * AvgReward -166.59 * true AvgReward -166.59 * Reward -182.47 * True Reward -182.47 * time 20.60 * step 70256\n",
      "Ep 299 * AvgReward -170.72 * true AvgReward -170.72 * Reward -201.44 * True Reward -201.44 * time 22.00 * step 70586\n",
      "Ep 300 * AvgReward -175.36 * true AvgReward -175.36 * Reward -189.71 * True Reward -189.71 * time 16.05 * step 70829\n",
      "Ep 301 * AvgReward -184.15 * true AvgReward -184.15 * Reward -226.81 * True Reward -226.81 * time 35.76 * step 71359\n",
      "Ep 302 * AvgReward -185.25 * true AvgReward -185.25 * Reward -203.95 * True Reward -203.95 * time 15.42 * step 71586\n",
      "Ep 303 * AvgReward -185.00 * true AvgReward -185.00 * Reward -196.12 * True Reward -196.12 * time 20.27 * step 71893\n",
      "Ep 304 * AvgReward -183.07 * true AvgReward -183.07 * Reward -145.77 * True Reward -145.77 * time 10.80 * step 72056\n",
      "Ep 305 * AvgReward -182.22 * true AvgReward -182.22 * Reward -176.52 * True Reward -176.52 * time 16.03 * step 72299\n",
      "Ep 306 * AvgReward -185.23 * true AvgReward -185.23 * Reward -251.63 * True Reward -251.63 * time 67.06 * step 73299\n",
      "Ep 307 * AvgReward -183.83 * true AvgReward -183.83 * Reward -122.71 * True Reward -122.71 * time 17.46 * step 73548\n",
      "Ep 308 * AvgReward -183.24 * true AvgReward -183.24 * Reward -196.45 * True Reward -196.45 * time 68.09 * step 74548\n",
      "Ep 309 * AvgReward -186.67 * true AvgReward -186.67 * Reward -291.95 * True Reward -291.95 * time 33.10 * step 75032\n",
      "Ep 310 * AvgReward -186.18 * true AvgReward -186.18 * Reward -190.49 * True Reward -190.49 * time 14.63 * step 75242\n",
      "Ep 311 * AvgReward -183.69 * true AvgReward -183.69 * Reward -130.72 * True Reward -130.72 * time 16.71 * step 75493\n",
      "Ep 312 * AvgReward -180.52 * true AvgReward -180.52 * Reward -121.84 * True Reward -121.84 * time 20.08 * step 75800\n",
      "Ep 313 * AvgReward -181.80 * true AvgReward -181.80 * Reward -108.15 * True Reward -108.15 * time 12.03 * step 75984\n",
      "Ep 314 * AvgReward -181.08 * true AvgReward -181.08 * Reward -120.82 * True Reward -120.82 * time 12.58 * step 76172\n",
      "Ep 315 * AvgReward -178.53 * true AvgReward -178.53 * Reward -114.43 * True Reward -114.43 * time 8.62 * step 76297\n",
      "Ep 316 * AvgReward -173.95 * true AvgReward -173.95 * Reward -119.13 * True Reward -119.13 * time 25.84 * step 76681\n",
      "Ep 317 * AvgReward -171.51 * true AvgReward -171.51 * Reward -139.14 * True Reward -139.14 * time 8.11 * step 76804\n",
      "Ep 318 * AvgReward -170.13 * true AvgReward -170.13 * Reward -154.73 * True Reward -154.73 * time 7.87 * step 76922\n",
      "Ep 319 * AvgReward -166.63 * true AvgReward -166.63 * Reward -131.58 * True Reward -131.58 * time 7.19 * step 77031\n",
      "Ep 320 * AvgReward -152.85 * true AvgReward -152.85 * Reward 85.92 * True Reward 85.92 * time 70.64 * step 78031\n",
      "Ep 321 * AvgReward -151.01 * true AvgReward -151.01 * Reward -189.97 * True Reward -189.97 * time 10.99 * step 78184\n",
      "Ep 322 * AvgReward -146.02 * true AvgReward -146.02 * Reward -104.16 * True Reward -104.16 * time 22.54 * step 78514\n",
      "Ep 323 * AvgReward -125.05 * true AvgReward -125.05 * Reward 223.27 * True Reward 223.27 * time 36.52 * step 79048\n",
      "Ep 324 * AvgReward -107.45 * true AvgReward -107.45 * Reward 206.25 * True Reward 206.25 * time 38.82 * step 79619\n",
      "Ep 325 * AvgReward -105.58 * true AvgReward -105.58 * Reward -139.06 * True Reward -139.06 * time 19.07 * step 79901\n",
      "Ep 326 * AvgReward -91.23 * true AvgReward -91.23 * Reward 35.31 * True Reward 35.31 * time 14.80 * step 80119\n",
      "Ep 327 * AvgReward -92.00 * true AvgReward -92.00 * Reward -138.07 * True Reward -138.07 * time 7.78 * step 80235\n",
      "Ep 328 * AvgReward -93.78 * true AvgReward -93.78 * Reward -232.05 * True Reward -232.05 * time 15.43 * step 80458\n",
      "Ep 329 * AvgReward -83.02 * true AvgReward -83.02 * Reward -76.79 * True Reward -76.79 * time 27.91 * step 80858\n",
      "Ep 330 * AvgReward -79.05 * true AvgReward -79.05 * Reward -111.20 * True Reward -111.20 * time 18.06 * step 81102\n",
      "Ep 331 * AvgReward -79.19 * true AvgReward -79.19 * Reward -133.43 * True Reward -133.43 * time 11.80 * step 81271\n",
      "Ep 332 * AvgReward -76.04 * true AvgReward -76.04 * Reward -58.78 * True Reward -58.78 * time 13.09 * step 81467\n",
      "Ep 333 * AvgReward -79.12 * true AvgReward -79.12 * Reward -169.88 * True Reward -169.88 * time 14.33 * step 81677\n",
      "Ep 334 * AvgReward -78.58 * true AvgReward -78.58 * Reward -109.94 * True Reward -109.94 * time 15.62 * step 81907\n",
      "Ep 335 * AvgReward -62.60 * true AvgReward -62.60 * Reward 205.17 * True Reward 205.17 * time 45.84 * step 82555\n",
      "Ep 336 * AvgReward -61.39 * true AvgReward -61.39 * Reward -94.92 * True Reward -94.92 * time 18.20 * step 82815\n",
      "Ep 337 * AvgReward -42.07 * true AvgReward -42.07 * Reward 247.26 * True Reward 247.26 * time 54.10 * step 83581\n",
      "Ep 338 * AvgReward -38.46 * true AvgReward -38.46 * Reward -82.53 * True Reward -82.53 * time 41.05 * step 84167\n",
      "Ep 339 * AvgReward -20.80 * true AvgReward -20.80 * Reward 221.55 * True Reward 221.55 * time 68.60 * step 85139\n",
      "Ep 340 * AvgReward -32.66 * true AvgReward -32.66 * Reward -151.33 * True Reward -151.33 * time 30.42 * step 85585\n",
      "Ep 341 * AvgReward -30.81 * true AvgReward -30.81 * Reward -152.83 * True Reward -152.83 * time 49.52 * step 86297\n",
      "Ep 342 * AvgReward -33.34 * true AvgReward -33.34 * Reward -154.83 * True Reward -154.83 * time 51.92 * step 87035\n",
      "Ep 343 * AvgReward -52.83 * true AvgReward -52.83 * Reward -166.49 * True Reward -166.49 * time 8.69 * step 87164\n",
      "Ep 344 * AvgReward -68.86 * true AvgReward -68.86 * Reward -114.46 * True Reward -114.46 * time 20.79 * step 87451\n",
      "Ep 345 * AvgReward -67.97 * true AvgReward -67.97 * Reward -121.11 * True Reward -121.11 * time 8.74 * step 87574\n",
      "Ep 346 * AvgReward -76.69 * true AvgReward -76.69 * Reward -139.17 * True Reward -139.17 * time 13.81 * step 87776\n",
      "Ep 347 * AvgReward -58.68 * true AvgReward -58.68 * Reward 222.23 * True Reward 222.23 * time 61.07 * step 88635\n",
      "Ep 348 * AvgReward -50.25 * true AvgReward -50.25 * Reward -63.60 * True Reward -63.60 * time 14.00 * step 88837\n",
      "Ep 349 * AvgReward -53.89 * true AvgReward -53.89 * Reward -149.53 * True Reward -149.53 * time 11.81 * step 89012\n",
      "Ep 350 * AvgReward -50.24 * true AvgReward -50.24 * Reward -38.19 * True Reward -38.19 * time 19.55 * step 89301\n",
      "Ep 351 * AvgReward -48.81 * true AvgReward -48.81 * Reward -104.88 * True Reward -104.88 * time 16.83 * step 89548\n",
      "Ep 352 * AvgReward -51.36 * true AvgReward -51.36 * Reward -109.72 * True Reward -109.72 * time 20.17 * step 89844\n",
      "Ep 353 * AvgReward -51.68 * true AvgReward -51.68 * Reward -176.25 * True Reward -176.25 * time 35.49 * step 90355\n",
      "Ep 354 * AvgReward -48.85 * true AvgReward -48.85 * Reward -53.45 * True Reward -53.45 * time 45.54 * step 91016\n",
      "Ep 355 * AvgReward -45.75 * true AvgReward -45.75 * Reward 267.16 * True Reward 267.16 * time 28.88 * step 91430\n",
      "Ep 356 * AvgReward -48.51 * true AvgReward -48.51 * Reward -150.10 * True Reward -150.10 * time 18.69 * step 91704\n",
      "Ep 357 * AvgReward -66.71 * true AvgReward -66.71 * Reward -116.74 * True Reward -116.74 * time 10.87 * step 91868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 358 * AvgReward -69.18 * true AvgReward -69.18 * Reward -131.76 * True Reward -131.76 * time 20.14 * step 92163\n",
      "Ep 359 * AvgReward -86.73 * true AvgReward -86.73 * Reward -129.54 * True Reward -129.54 * time 21.56 * step 92477\n",
      "Ep 360 * AvgReward -67.18 * true AvgReward -67.18 * Reward 239.75 * True Reward 239.75 * time 41.90 * step 93092\n",
      "Ep 361 * AvgReward -62.70 * true AvgReward -62.70 * Reward -63.37 * True Reward -63.37 * time 25.37 * step 93453\n",
      "Ep 362 * AvgReward -52.84 * true AvgReward -52.84 * Reward 42.49 * True Reward 42.49 * time 69.82 * step 94453\n",
      "Ep 363 * AvgReward -47.28 * true AvgReward -47.28 * Reward -55.29 * True Reward -55.29 * time 26.26 * step 94829\n",
      "Ep 364 * AvgReward -51.46 * true AvgReward -51.46 * Reward -198.13 * True Reward -198.13 * time 27.13 * step 95155\n",
      "Ep 365 * AvgReward -49.05 * true AvgReward -49.05 * Reward -73.00 * True Reward -73.00 * time 38.55 * step 95694\n",
      "Ep 366 * AvgReward -47.67 * true AvgReward -47.67 * Reward -111.56 * True Reward -111.56 * time 71.99 * step 96694\n",
      "Ep 367 * AvgReward -54.65 * true AvgReward -54.65 * Reward 82.61 * True Reward 82.61 * time 69.34 * step 97694\n",
      "Ep 368 * AvgReward -54.40 * true AvgReward -54.40 * Reward -58.44 * True Reward -58.44 * time 52.89 * step 98432\n",
      "Ep 369 * AvgReward -53.90 * true AvgReward -53.90 * Reward -139.68 * True Reward -139.68 * time 13.33 * step 98618\n",
      "Ep 370 * AvgReward -57.14 * true AvgReward -57.14 * Reward -102.86 * True Reward -102.86 * time 19.75 * step 98897\n",
      "Ep 371 * AvgReward -59.71 * true AvgReward -59.71 * Reward -156.42 * True Reward -156.42 * time 21.01 * step 99197\n",
      "Ep 372 * AvgReward -60.31 * true AvgReward -60.31 * Reward -121.59 * True Reward -121.59 * time 17.13 * step 99436\n",
      "Ep 373 * AvgReward -58.32 * true AvgReward -58.32 * Reward -136.60 * True Reward -136.60 * time 38.09 * step 99959\n",
      "Ep 374 * AvgReward -72.65 * true AvgReward -72.65 * Reward -339.99 * True Reward -339.99 * time 21.17 * step 100256\n",
      "Ep 375 * AvgReward -108.80 * true AvgReward -108.80 * Reward -455.79 * True Reward -455.79 * time 21.19 * step 100559\n",
      "Ep 376 * AvgReward -101.84 * true AvgReward -101.84 * Reward -10.98 * True Reward -10.98 * time 68.53 * step 101559\n",
      "Ep 377 * AvgReward -109.90 * true AvgReward -109.90 * Reward -277.89 * True Reward -277.89 * time 64.43 * step 102489\n",
      "Ep 378 * AvgReward -117.97 * true AvgReward -117.97 * Reward -293.13 * True Reward -293.13 * time 14.85 * step 102708\n",
      "Ep 379 * AvgReward -122.58 * true AvgReward -122.58 * Reward -221.84 * True Reward -221.84 * time 64.04 * step 103635\n",
      "Ep 380 * AvgReward -147.78 * true AvgReward -147.78 * Reward -264.21 * True Reward -264.21 * time 43.90 * step 104269\n",
      "Ep 381 * AvgReward -151.67 * true AvgReward -151.67 * Reward -141.02 * True Reward -141.02 * time 19.65 * step 104551\n",
      "Ep 382 * AvgReward -149.38 * true AvgReward -149.38 * Reward 88.28 * True Reward 88.28 * time 70.00 * step 105551\n",
      "Ep 383 * AvgReward -140.90 * true AvgReward -140.90 * Reward 114.20 * True Reward 114.20 * time 69.94 * step 106551\n",
      "Ep 384 * AvgReward -140.81 * true AvgReward -140.81 * Reward -196.26 * True Reward -196.26 * time 17.19 * step 106806\n",
      "Ep 385 * AvgReward -134.18 * true AvgReward -134.18 * Reward 59.64 * True Reward 59.64 * time 68.25 * step 107806\n",
      "Ep 386 * AvgReward -124.30 * true AvgReward -124.30 * Reward 85.92 * True Reward 85.92 * time 73.31 * step 108806\n",
      "Ep 387 * AvgReward -117.14 * true AvgReward -117.14 * Reward 225.76 * True Reward 225.76 * time 54.14 * step 109558\n",
      "Ep 388 * AvgReward -118.22 * true AvgReward -118.22 * Reward -79.96 * True Reward -79.96 * time 27.55 * step 109954\n",
      "Ep 389 * AvgReward -110.25 * true AvgReward -110.25 * Reward 19.63 * True Reward 19.63 * time 70.57 * step 110954\n",
      "Ep 390 * AvgReward -106.71 * true AvgReward -106.71 * Reward -32.00 * True Reward -32.00 * time 23.42 * step 111289\n",
      "Ep 391 * AvgReward -90.10 * true AvgReward -90.10 * Reward 175.75 * True Reward 175.75 * time 40.62 * step 111867\n",
      "Ep 392 * AvgReward -89.69 * true AvgReward -89.69 * Reward -113.40 * True Reward -113.40 * time 22.22 * step 112187\n",
      "Ep 393 * AvgReward -91.12 * true AvgReward -91.12 * Reward -165.06 * True Reward -165.06 * time 23.70 * step 112521\n",
      "Ep 394 * AvgReward -73.57 * true AvgReward -73.57 * Reward 10.96 * True Reward 10.96 * time 12.48 * step 112699\n",
      "Ep 395 * AvgReward -55.78 * true AvgReward -55.78 * Reward -99.95 * True Reward -99.95 * time 13.17 * step 112887\n",
      "Ep 396 * AvgReward -57.37 * true AvgReward -57.37 * Reward -42.78 * True Reward -42.78 * time 27.20 * step 113284\n",
      "Ep 397 * AvgReward -37.26 * true AvgReward -37.26 * Reward 124.32 * True Reward 124.32 * time 67.75 * step 114284\n",
      "Ep 398 * AvgReward -28.19 * true AvgReward -28.19 * Reward -111.84 * True Reward -111.84 * time 18.26 * step 114549\n",
      "Ep 399 * AvgReward -23.38 * true AvgReward -23.38 * Reward -125.60 * True Reward -125.60 * time 17.18 * step 114805\n",
      "Ep 400 * AvgReward -13.28 * true AvgReward -13.28 * Reward -62.10 * True Reward -62.10 * time 17.68 * step 115066\n",
      "Ep 401 * AvgReward -9.40 * true AvgReward -9.40 * Reward -63.46 * True Reward -63.46 * time 20.14 * step 115360\n",
      "Ep 402 * AvgReward -21.40 * true AvgReward -21.40 * Reward -151.85 * True Reward -151.85 * time 21.08 * step 115658\n",
      "Ep 403 * AvgReward -23.23 * true AvgReward -23.23 * Reward 77.73 * True Reward 77.73 * time 69.50 * step 116658\n",
      "Ep 404 * AvgReward -3.26 * true AvgReward -3.26 * Reward 203.00 * True Reward 203.00 * time 64.96 * step 117601\n",
      "Ep 405 * AvgReward -28.07 * true AvgReward -28.07 * Reward -436.53 * True Reward -436.53 * time 12.52 * step 117776\n",
      "Ep 406 * AvgReward -24.47 * true AvgReward -24.47 * Reward 158.06 * True Reward 158.06 * time 70.88 * step 118776\n",
      "Ep 407 * AvgReward -46.25 * true AvgReward -46.25 * Reward -209.89 * True Reward -209.89 * time 26.91 * step 119155\n",
      "Ep 408 * AvgReward -34.74 * true AvgReward -34.74 * Reward 150.21 * True Reward 150.21 * time 69.44 * step 120155\n",
      "Ep 409 * AvgReward -25.84 * true AvgReward -25.84 * Reward 197.70 * True Reward 197.70 * time 60.07 * step 121019\n",
      "Ep 410 * AvgReward -14.16 * true AvgReward -14.16 * Reward 201.50 * True Reward 201.50 * time 40.78 * step 121606\n",
      "Ep 411 * AvgReward -31.84 * true AvgReward -31.84 * Reward -177.88 * True Reward -177.88 * time 14.11 * step 121812\n",
      "Ep 412 * AvgReward -35.06 * true AvgReward -35.06 * Reward -177.77 * True Reward -177.77 * time 24.88 * step 122164\n",
      "Ep 413 * AvgReward -39.33 * true AvgReward -39.33 * Reward -250.36 * True Reward -250.36 * time 25.13 * step 122513\n",
      "Ep 414 * AvgReward -49.12 * true AvgReward -49.12 * Reward -184.93 * True Reward -184.93 * time 21.54 * step 122817\n",
      "Ep 415 * AvgReward -52.67 * true AvgReward -52.67 * Reward -170.86 * True Reward -170.86 * time 11.03 * step 122977\n",
      "Ep 416 * AvgReward -54.95 * true AvgReward -54.95 * Reward -88.40 * True Reward -88.40 * time 10.54 * step 123129\n",
      "Ep 417 * AvgReward -69.78 * true AvgReward -69.78 * Reward -172.33 * True Reward -172.33 * time 12.46 * step 123308\n",
      "Ep 418 * AvgReward -75.48 * true AvgReward -75.48 * Reward -225.77 * True Reward -225.77 * time 11.81 * step 123478\n",
      "Ep 419 * AvgReward -81.29 * true AvgReward -81.29 * Reward -241.95 * True Reward -241.95 * time 9.81 * step 123620\n",
      "Ep 420 * AvgReward -81.62 * true AvgReward -81.62 * Reward -68.65 * True Reward -68.65 * time 24.96 * step 123975\n",
      "Ep 421 * AvgReward -90.96 * true AvgReward -90.96 * Reward -250.34 * True Reward -250.34 * time 39.47 * step 124530\n",
      "Ep 422 * AvgReward -84.39 * true AvgReward -84.39 * Reward -20.34 * True Reward -20.34 * time 69.80 * step 125530\n",
      "Ep 423 * AvgReward -94.44 * true AvgReward -94.44 * Reward -123.25 * True Reward -123.25 * time 17.35 * step 125783\n",
      "Ep 424 * AvgReward -116.19 * true AvgReward -116.19 * Reward -232.00 * True Reward -232.00 * time 31.71 * step 126251\n",
      "Ep 425 * AvgReward -93.83 * true AvgReward -93.83 * Reward 10.67 * True Reward 10.67 * time 8.72 * step 126382\n",
      "Ep 426 * AvgReward -117.90 * true AvgReward -117.90 * Reward -323.31 * True Reward -323.31 * time 26.12 * step 126763\n",
      "Ep 427 * AvgReward -120.20 * true AvgReward -120.20 * Reward -255.87 * True Reward -255.87 * time 21.93 * step 127090\n",
      "Ep 428 * AvgReward -139.12 * true AvgReward -139.12 * Reward -228.34 * True Reward -228.34 * time 23.73 * step 127442\n",
      "Ep 429 * AvgReward -144.37 * true AvgReward -144.37 * Reward 92.73 * True Reward 92.73 * time 72.51 * step 128442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 430 * AvgReward -156.81 * true AvgReward -156.81 * Reward -47.16 * True Reward -47.16 * time 20.28 * step 128731\n",
      "Ep 431 * AvgReward -155.06 * true AvgReward -155.06 * Reward -142.98 * True Reward -142.98 * time 11.93 * step 128904\n",
      "Ep 432 * AvgReward -134.99 * true AvgReward -134.99 * Reward 223.62 * True Reward 223.62 * time 53.14 * step 129629\n",
      "Ep 433 * AvgReward -132.07 * true AvgReward -132.07 * Reward -191.87 * True Reward -191.87 * time 41.63 * step 130217\n",
      "Ep 434 * AvgReward -109.69 * true AvgReward -109.69 * Reward 262.55 * True Reward 262.55 * time 27.47 * step 130605\n",
      "Ep 435 * AvgReward -102.84 * true AvgReward -102.84 * Reward -33.79 * True Reward -33.79 * time 13.88 * step 130805\n",
      "Ep 436 * AvgReward -90.55 * true AvgReward -90.55 * Reward 157.34 * True Reward 157.34 * time 32.51 * step 131269\n",
      "Ep 437 * AvgReward -83.41 * true AvgReward -83.41 * Reward -29.54 * True Reward -29.54 * time 69.84 * step 132269\n",
      "Ep 438 * AvgReward -62.99 * true AvgReward -62.99 * Reward 182.69 * True Reward 182.69 * time 30.65 * step 132708\n",
      "Ep 439 * AvgReward -42.57 * true AvgReward -42.57 * Reward 166.52 * True Reward 166.52 * time 44.23 * step 133342\n",
      "Ep 440 * AvgReward -48.16 * true AvgReward -48.16 * Reward -180.51 * True Reward -180.51 * time 20.14 * step 133635\n",
      "Ep 441 * AvgReward -46.53 * true AvgReward -46.53 * Reward -217.76 * True Reward -217.76 * time 53.22 * step 134385\n",
      "Ep 442 * AvgReward -45.95 * true AvgReward -45.95 * Reward -8.70 * True Reward -8.70 * time 21.13 * step 134680\n",
      "Ep 443 * AvgReward -30.01 * true AvgReward -30.01 * Reward 195.61 * True Reward 195.61 * time 65.85 * step 135600\n",
      "Ep 444 * AvgReward -18.78 * true AvgReward -18.78 * Reward -7.44 * True Reward -7.44 * time 12.26 * step 135774\n",
      "Ep 445 * AvgReward -32.26 * true AvgReward -32.26 * Reward -258.93 * True Reward -258.93 * time 50.05 * step 136481\n",
      "Ep 446 * AvgReward -23.97 * true AvgReward -23.97 * Reward -157.50 * True Reward -157.50 * time 24.21 * step 136820\n",
      "Ep 447 * AvgReward -11.28 * true AvgReward -11.28 * Reward -2.18 * True Reward -2.18 * time 26.30 * step 137197\n",
      "Ep 448 * AvgReward -9.64 * true AvgReward -9.64 * Reward -195.59 * True Reward -195.59 * time 11.34 * step 137361\n",
      "Ep 449 * AvgReward -12.36 * true AvgReward -12.36 * Reward 38.45 * True Reward 38.45 * time 14.03 * step 137563\n",
      "Ep 450 * AvgReward -15.00 * true AvgReward -15.00 * Reward -99.94 * True Reward -99.94 * time 70.58 * step 138563\n",
      "Ep 451 * AvgReward -17.47 * true AvgReward -17.47 * Reward -192.51 * True Reward -192.51 * time 21.96 * step 138869\n",
      "Ep 452 * AvgReward -34.07 * true AvgReward -34.07 * Reward -108.39 * True Reward -108.39 * time 10.81 * step 139018\n",
      "Ep 453 * AvgReward -19.05 * true AvgReward -19.05 * Reward 108.65 * True Reward 108.65 * time 72.31 * step 140018\n",
      "Ep 454 * AvgReward -47.79 * true AvgReward -47.79 * Reward -312.25 * True Reward -312.25 * time 20.61 * step 140310\n",
      "Ep 455 * AvgReward -40.03 * true AvgReward -40.03 * Reward 121.30 * True Reward 121.30 * time 70.94 * step 141310\n",
      "Ep 456 * AvgReward -38.18 * true AvgReward -38.18 * Reward 194.47 * True Reward 194.47 * time 36.35 * step 141828\n",
      "Ep 457 * AvgReward -38.20 * true AvgReward -38.20 * Reward -29.99 * True Reward -29.99 * time 25.03 * step 142187\n",
      "Ep 458 * AvgReward -41.73 * true AvgReward -41.73 * Reward 112.03 * True Reward 112.03 * time 70.25 * step 143187\n",
      "Ep 459 * AvgReward -61.22 * true AvgReward -61.22 * Reward -223.25 * True Reward -223.25 * time 45.55 * step 143844\n",
      "Ep 460 * AvgReward -56.87 * true AvgReward -56.87 * Reward -93.57 * True Reward -93.57 * time 13.25 * step 144034\n",
      "Ep 461 * AvgReward -43.61 * true AvgReward -43.61 * Reward 47.59 * True Reward 47.59 * time 70.77 * step 145034\n",
      "Ep 462 * AvgReward -44.34 * true AvgReward -44.34 * Reward -23.31 * True Reward -23.31 * time 16.96 * step 145274\n",
      "Ep 463 * AvgReward -50.91 * true AvgReward -50.91 * Reward 64.11 * True Reward 64.11 * time 72.16 * step 146274\n",
      "Ep 464 * AvgReward -49.92 * true AvgReward -49.92 * Reward 12.32 * True Reward 12.32 * time 14.78 * step 146484\n",
      "Ep 465 * AvgReward -40.16 * true AvgReward -40.16 * Reward -63.56 * True Reward -63.56 * time 10.12 * step 146628\n",
      "Ep 466 * AvgReward -25.95 * true AvgReward -25.95 * Reward 126.53 * True Reward 126.53 * time 71.53 * step 147628\n",
      "Ep 467 * AvgReward -32.66 * true AvgReward -32.66 * Reward -136.23 * True Reward -136.23 * time 9.85 * step 147769\n",
      "Ep 468 * AvgReward -33.95 * true AvgReward -33.95 * Reward -221.53 * True Reward -221.53 * time 22.46 * step 148097\n",
      "Ep 469 * AvgReward -40.02 * true AvgReward -40.02 * Reward -82.81 * True Reward -82.81 * time 15.29 * step 148323\n",
      "Ep 470 * AvgReward -35.67 * true AvgReward -35.67 * Reward -13.09 * True Reward -13.09 * time 8.93 * step 148457\n",
      "Ep 471 * AvgReward -16.18 * true AvgReward -16.18 * Reward 197.28 * True Reward 197.28 * time 56.86 * step 149250\n",
      "Ep 472 * AvgReward -11.25 * true AvgReward -11.25 * Reward -9.66 * True Reward -9.66 * time 17.11 * step 149486\n",
      "Ep 473 * AvgReward -19.63 * true AvgReward -19.63 * Reward -58.96 * True Reward -58.96 * time 22.22 * step 149787\n",
      "Ep 474 * AvgReward -12.25 * true AvgReward -12.25 * Reward -164.71 * True Reward -164.71 * time 14.13 * step 149986\n",
      "Ep 475 * AvgReward -28.46 * true AvgReward -28.46 * Reward -202.86 * True Reward -202.86 * time 40.36 * step 150541\n",
      "Ep 476 * AvgReward -44.74 * true AvgReward -44.74 * Reward -131.07 * True Reward -131.07 * time 9.28 * step 150673\n",
      "Ep 477 * AvgReward -48.10 * true AvgReward -48.10 * Reward -97.18 * True Reward -97.18 * time 33.80 * step 151156\n",
      "Ep 478 * AvgReward -61.05 * true AvgReward -61.05 * Reward -147.03 * True Reward -147.03 * time 32.47 * step 151602\n",
      "Ep 479 * AvgReward -55.45 * true AvgReward -55.45 * Reward -111.22 * True Reward -111.22 * time 13.40 * step 151787\n",
      "Ep 480 * AvgReward -61.72 * true AvgReward -61.72 * Reward -219.02 * True Reward -219.02 * time 8.05 * step 151899\n",
      "Ep 481 * AvgReward -71.77 * true AvgReward -71.77 * Reward -153.48 * True Reward -153.48 * time 13.03 * step 152078\n",
      "Ep 482 * AvgReward -76.75 * true AvgReward -76.75 * Reward -122.83 * True Reward -122.83 * time 11.07 * step 152229\n",
      "Ep 483 * AvgReward -90.70 * true AvgReward -90.70 * Reward -214.86 * True Reward -214.86 * time 19.49 * step 152501\n",
      "Ep 484 * AvgReward -95.54 * true AvgReward -95.54 * Reward -84.46 * True Reward -84.46 * time 9.86 * step 152639\n",
      "Ep 485 * AvgReward -100.39 * true AvgReward -100.39 * Reward -160.55 * True Reward -160.55 * time 15.73 * step 152862\n",
      "Ep 486 * AvgReward -115.32 * true AvgReward -115.32 * Reward -172.22 * True Reward -172.22 * time 23.66 * step 153198\n",
      "Ep 487 * AvgReward -109.49 * true AvgReward -109.49 * Reward -19.53 * True Reward -19.53 * time 30.54 * step 153631\n",
      "Ep 488 * AvgReward -106.39 * true AvgReward -106.39 * Reward -159.57 * True Reward -159.57 * time 12.62 * step 153810\n",
      "Ep 489 * AvgReward -107.28 * true AvgReward -107.28 * Reward -100.51 * True Reward -100.51 * time 11.94 * step 153983\n",
      "Ep 490 * AvgReward -111.70 * true AvgReward -111.70 * Reward -101.60 * True Reward -101.60 * time 21.07 * step 154281\n",
      "Ep 491 * AvgReward -128.83 * true AvgReward -128.83 * Reward -145.34 * True Reward -145.34 * time 10.27 * step 154424\n",
      "Ep 492 * AvgReward -136.75 * true AvgReward -136.75 * Reward -168.00 * True Reward -168.00 * time 64.83 * step 155311\n",
      "Ep 493 * AvgReward -137.46 * true AvgReward -137.46 * Reward -73.19 * True Reward -73.19 * time 16.01 * step 155540\n",
      "Ep 494 * AvgReward -140.48 * true AvgReward -140.48 * Reward -225.15 * True Reward -225.15 * time 24.15 * step 155885\n",
      "Ep 495 * AvgReward -121.96 * true AvgReward -121.96 * Reward 167.57 * True Reward 167.57 * time 44.04 * step 156509\n",
      "Ep 496 * AvgReward -117.16 * true AvgReward -117.16 * Reward -34.94 * True Reward -34.94 * time 14.99 * step 156723\n",
      "Ep 497 * AvgReward -99.29 * true AvgReward -99.29 * Reward 260.08 * True Reward 260.08 * time 55.16 * step 157498\n",
      "Ep 498 * AvgReward -89.78 * true AvgReward -89.78 * Reward 43.13 * True Reward 43.13 * time 12.46 * step 157670\n",
      "Ep 499 * AvgReward -79.01 * true AvgReward -79.01 * Reward 104.32 * True Reward 104.32 * time 74.48 * step 158670\n",
      "Ep 500 * AvgReward -59.42 * true AvgReward -59.42 * Reward 172.72 * True Reward 172.72 * time 38.83 * step 159205\n",
      "Ep 501 * AvgReward -60.38 * true AvgReward -60.38 * Reward -172.73 * True Reward -172.73 * time 30.77 * step 159620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 502 * AvgReward -61.00 * true AvgReward -61.00 * Reward -135.10 * True Reward -135.10 * time 19.45 * step 159887\n",
      "Ep 503 * AvgReward -41.42 * true AvgReward -41.42 * Reward 176.57 * True Reward 176.57 * time 58.45 * step 160688\n",
      "Ep 504 * AvgReward -45.56 * true AvgReward -45.56 * Reward -167.22 * True Reward -167.22 * time 42.21 * step 161287\n",
      "Ep 505 * AvgReward -33.31 * true AvgReward -33.31 * Reward 84.46 * True Reward 84.46 * time 70.36 * step 162287\n",
      "Ep 506 * AvgReward -20.49 * true AvgReward -20.49 * Reward 84.26 * True Reward 84.26 * time 70.14 * step 163287\n",
      "Ep 507 * AvgReward -34.85 * true AvgReward -34.85 * Reward -306.86 * True Reward -306.86 * time 14.55 * step 163482\n",
      "Ep 508 * AvgReward -22.43 * true AvgReward -22.43 * Reward 88.89 * True Reward 88.89 * time 71.06 * step 164482\n",
      "Ep 509 * AvgReward -18.93 * true AvgReward -18.93 * Reward -30.52 * True Reward -30.52 * time 10.66 * step 164632\n",
      "Ep 510 * AvgReward -51.13 * true AvgReward -51.13 * Reward -745.50 * True Reward -745.50 * time 31.94 * step 165087\n",
      "Ep 511 * AvgReward -40.06 * true AvgReward -40.06 * Reward 75.97 * True Reward 75.97 * time 70.08 * step 166087\n",
      "Ep 512 * AvgReward -38.59 * true AvgReward -38.59 * Reward -138.50 * True Reward -138.50 * time 20.76 * step 166375\n",
      "Ep 513 * AvgReward -31.47 * true AvgReward -31.47 * Reward 69.05 * True Reward 69.05 * time 72.08 * step 167375\n",
      "Ep 514 * AvgReward -7.42 * true AvgReward -7.42 * Reward 255.87 * True Reward 255.87 * time 34.14 * step 167848\n",
      "Ep 515 * AvgReward -8.31 * true AvgReward -8.31 * Reward 149.75 * True Reward 149.75 * time 55.09 * step 168633\n",
      "Ep 516 * AvgReward -14.06 * true AvgReward -14.06 * Reward -149.94 * True Reward -149.94 * time 24.54 * step 168978\n",
      "Ep 517 * AvgReward -13.51 * true AvgReward -13.51 * Reward 271.15 * True Reward 271.15 * time 34.40 * step 169454\n",
      "Ep 518 * AvgReward -26.39 * true AvgReward -26.39 * Reward -214.52 * True Reward -214.52 * time 23.36 * step 169773\n",
      "Ep 519 * AvgReward -26.39 * true AvgReward -26.39 * Reward 104.37 * True Reward 104.37 * time 70.69 * step 170773\n",
      "Ep 520 * AvgReward -44.09 * true AvgReward -44.09 * Reward -181.26 * True Reward -181.26 * time 31.66 * step 171206\n",
      "Ep 521 * AvgReward -29.24 * true AvgReward -29.24 * Reward 124.20 * True Reward 124.20 * time 73.28 * step 172206\n",
      "Ep 522 * AvgReward -15.29 * true AvgReward -15.29 * Reward 143.96 * True Reward 143.96 * time 70.78 * step 173206\n",
      "Ep 523 * AvgReward -12.62 * true AvgReward -12.62 * Reward 229.96 * True Reward 229.96 * time 27.21 * step 173605\n",
      "Ep 524 * AvgReward 7.91 * true AvgReward 7.91 * Reward 243.48 * True Reward 243.48 * time 31.48 * step 174050\n",
      "Ep 525 * AvgReward 15.73 * true AvgReward 15.73 * Reward 240.74 * True Reward 240.74 * time 35.36 * step 174539\n",
      "Ep 526 * AvgReward 3.04 * true AvgReward 3.04 * Reward -169.43 * True Reward -169.43 * time 24.14 * step 174880\n",
      "Ep 527 * AvgReward 13.22 * true AvgReward 13.22 * Reward -103.39 * True Reward -103.39 * time 22.68 * step 175198\n",
      "Ep 528 * AvgReward 9.26 * true AvgReward 9.26 * Reward 9.82 * True Reward 9.82 * time 10.82 * step 175351\n",
      "Ep 529 * AvgReward 24.79 * true AvgReward 24.79 * Reward 279.93 * True Reward 279.93 * time 50.42 * step 176063\n",
      "Ep 530 * AvgReward 56.61 * true AvgReward 56.61 * Reward -109.01 * True Reward -109.01 * time 16.85 * step 176305\n",
      "Ep 531 * AvgReward 66.10 * true AvgReward 66.10 * Reward 265.79 * True Reward 265.79 * time 35.71 * step 176806\n",
      "Ep 532 * AvgReward 85.11 * true AvgReward 85.11 * Reward 241.62 * True Reward 241.62 * time 60.50 * step 177641\n",
      "Ep 533 * AvgReward 92.84 * true AvgReward 92.84 * Reward 223.72 * True Reward 223.72 * time 24.31 * step 177975\n",
      "Ep 534 * AvgReward 83.54 * true AvgReward 83.54 * Reward 69.76 * True Reward 69.76 * time 74.08 * step 178975\n",
      "Ep 535 * AvgReward 72.12 * true AvgReward 72.12 * Reward -78.66 * True Reward -78.66 * time 17.29 * step 179216\n",
      "Ep 536 * AvgReward 79.46 * true AvgReward 79.46 * Reward -3.04 * True Reward -3.04 * time 71.60 * step 180216\n",
      "Ep 537 * AvgReward 79.25 * true AvgReward 79.25 * Reward 266.91 * True Reward 266.91 * time 24.90 * step 180567\n",
      "Ep 538 * AvgReward 87.72 * true AvgReward 87.72 * Reward -45.09 * True Reward -45.09 * time 72.25 * step 181567\n",
      "Ep 539 * AvgReward 88.91 * true AvgReward 88.91 * Reward 128.21 * True Reward 128.21 * time 70.66 * step 182567\n",
      "Ep 540 * AvgReward 109.50 * true AvgReward 109.50 * Reward 230.44 * True Reward 230.44 * time 34.77 * step 183059\n",
      "Ep 541 * AvgReward 98.04 * true AvgReward 98.04 * Reward -104.86 * True Reward -104.86 * time 12.61 * step 183235\n",
      "Ep 542 * AvgReward 80.22 * true AvgReward 80.22 * Reward -212.46 * True Reward -212.46 * time 29.13 * step 183641\n",
      "Ep 543 * AvgReward 64.35 * true AvgReward 64.35 * Reward -87.48 * True Reward -87.48 * time 40.87 * step 184188\n",
      "Ep 544 * AvgReward 51.71 * true AvgReward 51.71 * Reward -9.28 * True Reward -9.28 * time 9.73 * step 184323\n",
      "Ep 545 * AvgReward 35.65 * true AvgReward 35.65 * Reward -80.45 * True Reward -80.45 * time 74.09 * step 185323\n",
      "Ep 546 * AvgReward 38.91 * true AvgReward 38.91 * Reward -104.23 * True Reward -104.23 * time 25.96 * step 185625\n",
      "Ep 547 * AvgReward 46.05 * true AvgReward 46.05 * Reward 39.43 * True Reward 39.43 * time 9.47 * step 185752\n",
      "Ep 548 * AvgReward 43.63 * true AvgReward 43.63 * Reward -38.75 * True Reward -38.75 * time 73.10 * step 186752\n",
      "Ep 549 * AvgReward 40.63 * true AvgReward 40.63 * Reward 219.95 * True Reward 219.95 * time 50.65 * step 187462\n",
      "Ep 550 * AvgReward 43.39 * true AvgReward 43.39 * Reward -53.66 * True Reward -53.66 * time 70.98 * step 188462\n",
      "Ep 551 * AvgReward 31.13 * true AvgReward 31.13 * Reward 20.47 * True Reward 20.47 * time 7.33 * step 188564\n",
      "Ep 552 * AvgReward 15.20 * true AvgReward 15.20 * Reward -77.00 * True Reward -77.00 * time 38.54 * step 189098\n",
      "Ep 553 * AvgReward 11.19 * true AvgReward 11.19 * Reward 143.55 * True Reward 143.55 * time 71.04 * step 190098\n",
      "Ep 554 * AvgReward 8.59 * true AvgReward 8.59 * Reward 17.79 * True Reward 17.79 * time 68.97 * step 191098\n",
      "Ep 555 * AvgReward 5.27 * true AvgReward 5.27 * Reward -145.12 * True Reward -145.12 * time 31.39 * step 191546\n",
      "Ep 556 * AvgReward 1.60 * true AvgReward 1.60 * Reward -76.31 * True Reward -76.31 * time 11.97 * step 191723\n",
      "Ep 557 * AvgReward 2.17 * true AvgReward 2.17 * Reward 278.24 * True Reward 278.24 * time 39.77 * step 192294\n",
      "Ep 558 * AvgReward 10.50 * true AvgReward 10.50 * Reward 121.54 * True Reward 121.54 * time 72.18 * step 193294\n",
      "Ep 559 * AvgReward 14.51 * true AvgReward 14.51 * Reward 208.45 * True Reward 208.45 * time 26.90 * step 193666\n",
      "Ep 560 * AvgReward 5.52 * true AvgReward 5.52 * Reward 50.52 * True Reward 50.52 * time 10.97 * step 193815\n",
      "Ep 561 * AvgReward 15.93 * true AvgReward 15.93 * Reward 103.44 * True Reward 103.44 * time 70.92 * step 194815\n",
      "Ep 562 * AvgReward 39.44 * true AvgReward 39.44 * Reward 257.74 * True Reward 257.74 * time 43.57 * step 195416\n",
      "Ep 563 * AvgReward 44.52 * true AvgReward 44.52 * Reward 14.05 * True Reward 14.05 * time 23.89 * step 195758\n",
      "Ep 564 * AvgReward 54.45 * true AvgReward 54.45 * Reward 189.40 * True Reward 189.40 * time 57.69 * step 196565\n",
      "Ep 565 * AvgReward 59.58 * true AvgReward 59.58 * Reward 22.15 * True Reward 22.15 * time 23.88 * step 196900\n",
      "Ep 566 * AvgReward 60.51 * true AvgReward 60.51 * Reward -85.70 * True Reward -85.70 * time 33.90 * step 197384\n",
      "Ep 567 * AvgReward 62.24 * true AvgReward 62.24 * Reward 74.03 * True Reward 74.03 * time 69.98 * step 198384\n",
      "Ep 568 * AvgReward 60.94 * true AvgReward 60.94 * Reward -64.72 * True Reward -64.72 * time 24.33 * step 198716\n",
      "Ep 569 * AvgReward 62.48 * true AvgReward 62.48 * Reward 250.66 * True Reward 250.66 * time 25.49 * step 199074\n",
      "Ep 570 * AvgReward 70.46 * true AvgReward 70.46 * Reward 106.08 * True Reward 106.08 * time 68.00 * step 200043\n",
      "Ep 571 * AvgReward 76.40 * true AvgReward 76.40 * Reward 139.13 * True Reward 139.13 * time 70.07 * step 201043\n",
      "Ep 572 * AvgReward 86.70 * true AvgReward 86.70 * Reward 129.16 * True Reward 129.16 * time 71.02 * step 202043\n",
      "Ep 573 * AvgReward 92.31 * true AvgReward 92.31 * Reward 255.60 * True Reward 255.60 * time 38.42 * step 202576\n",
      "Ep 574 * AvgReward 95.09 * true AvgReward 95.09 * Reward 73.46 * True Reward 73.46 * time 73.56 * step 203576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 575 * AvgReward 102.08 * true AvgReward 102.08 * Reward -5.23 * True Reward -5.23 * time 19.02 * step 203849\n",
      "Ep 576 * AvgReward 119.42 * true AvgReward 119.42 * Reward 270.31 * True Reward 270.31 * time 42.28 * step 204457\n",
      "Ep 577 * AvgReward 96.80 * true AvgReward 96.80 * Reward -173.99 * True Reward -173.99 * time 28.89 * step 204878\n",
      "Ep 578 * AvgReward 92.31 * true AvgReward 92.31 * Reward 31.75 * True Reward 31.75 * time 16.06 * step 205101\n",
      "Ep 579 * AvgReward 85.32 * true AvgReward 85.32 * Reward 68.53 * True Reward 68.53 * time 71.74 * step 206101\n",
      "Ep 580 * AvgReward 88.90 * true AvgReward 88.90 * Reward 122.10 * True Reward 122.10 * time 70.61 * step 207101\n",
      "Ep 581 * AvgReward 86.31 * true AvgReward 86.31 * Reward 51.77 * True Reward 51.77 * time 72.41 * step 208101\n",
      "Ep 582 * AvgReward 79.30 * true AvgReward 79.30 * Reward 117.51 * True Reward 117.51 * time 73.04 * step 209101\n",
      "Ep 583 * AvgReward 75.86 * true AvgReward 75.86 * Reward -54.89 * True Reward -54.89 * time 29.80 * step 209508\n",
      "Ep 584 * AvgReward 53.15 * true AvgReward 53.15 * Reward -264.68 * True Reward -264.68 * time 51.15 * step 210191\n",
      "Ep 585 * AvgReward 57.73 * true AvgReward 57.73 * Reward 113.62 * True Reward 113.62 * time 91.57 * step 211191\n",
      "Ep 586 * AvgReward 62.37 * true AvgReward 62.37 * Reward 7.12 * True Reward 7.12 * time 10.35 * step 211325\n",
      "Ep 587 * AvgReward 54.40 * true AvgReward 54.40 * Reward -85.41 * True Reward -85.41 * time 51.11 * step 212039\n",
      "Ep 588 * AvgReward 46.30 * true AvgReward 46.30 * Reward -226.62 * True Reward -226.62 * time 25.49 * step 212412\n",
      "Ep 589 * AvgReward 32.05 * true AvgReward 32.05 * Reward -34.25 * True Reward -34.25 * time 22.38 * step 212738\n",
      "Ep 590 * AvgReward 23.95 * true AvgReward 23.95 * Reward -55.96 * True Reward -55.96 * time 17.25 * step 212986\n",
      "Ep 591 * AvgReward 16.50 * true AvgReward 16.50 * Reward -9.93 * True Reward -9.93 * time 18.18 * step 213240\n",
      "Ep 592 * AvgReward 8.33 * true AvgReward 8.33 * Reward -34.24 * True Reward -34.24 * time 13.70 * step 213402\n",
      "Ep 593 * AvgReward -6.09 * true AvgReward -6.09 * Reward -32.85 * True Reward -32.85 * time 16.30 * step 213634\n",
      "Ep 594 * AvgReward -3.81 * true AvgReward -3.81 * Reward 119.08 * True Reward 119.08 * time 69.73 * step 214634\n",
      "Ep 595 * AvgReward 9.57 * true AvgReward 9.57 * Reward 262.46 * True Reward 262.46 * time 39.12 * step 215188\n",
      "Ep 596 * AvgReward -4.03 * true AvgReward -4.03 * Reward -1.80 * True Reward -1.80 * time 7.52 * step 215298\n",
      "Ep 597 * AvgReward -3.39 * true AvgReward -3.39 * Reward -161.08 * True Reward -161.08 * time 68.48 * step 216298\n",
      "Ep 598 * AvgReward -4.82 * true AvgReward -4.82 * Reward 3.19 * True Reward 3.19 * time 11.60 * step 216466\n",
      "Ep 599 * AvgReward -9.07 * true AvgReward -9.07 * Reward -16.56 * True Reward -16.56 * time 10.43 * step 216615\n",
      "Ep 0 * AvgReward -180.72 * true AvgReward -180.72 * Reward -180.72 * True Reward -180.72 * time 6.80 * step 82\n",
      "Ep 1 * AvgReward -156.86 * true AvgReward -156.86 * Reward -132.99 * True Reward -132.99 * time 4.30 * step 142\n",
      "Ep 2 * AvgReward -164.63 * true AvgReward -164.63 * Reward -180.17 * True Reward -180.17 * time 5.51 * step 220\n",
      "Ep 3 * AvgReward -164.33 * true AvgReward -164.33 * Reward -163.43 * True Reward -163.43 * time 4.64 * step 283\n",
      "Ep 4 * AvgReward -160.08 * true AvgReward -160.08 * Reward -143.10 * True Reward -143.10 * time 4.36 * step 340\n",
      "Ep 5 * AvgReward -151.19 * true AvgReward -151.19 * Reward -106.71 * True Reward -106.71 * time 3.90 * step 396\n",
      "Ep 6 * AvgReward -144.40 * true AvgReward -144.40 * Reward -103.67 * True Reward -103.67 * time 5.58 * step 477\n",
      "Ep 7 * AvgReward -145.10 * true AvgReward -145.10 * Reward -150.01 * True Reward -150.01 * time 4.04 * step 535\n",
      "Ep 8 * AvgReward -138.06 * true AvgReward -138.06 * Reward -81.78 * True Reward -81.78 * time 3.75 * step 589\n",
      "Ep 9 * AvgReward -140.60 * true AvgReward -140.60 * Reward -163.45 * True Reward -163.45 * time 4.54 * step 654\n",
      "Ep 10 * AvgReward -140.42 * true AvgReward -140.42 * Reward -138.59 * True Reward -138.59 * time 4.01 * step 711\n",
      "Ep 11 * AvgReward -136.14 * true AvgReward -136.14 * Reward -89.07 * True Reward -89.07 * time 3.77 * step 766\n",
      "Ep 12 * AvgReward -136.55 * true AvgReward -136.55 * Reward -141.39 * True Reward -141.39 * time 4.59 * step 834\n",
      "Ep 13 * AvgReward -135.56 * true AvgReward -135.56 * Reward -122.71 * True Reward -122.71 * time 3.77 * step 888\n",
      "Ep 14 * AvgReward -135.13 * true AvgReward -135.13 * Reward -129.12 * True Reward -129.12 * time 4.69 * step 953\n",
      "Ep 15 * AvgReward -145.39 * true AvgReward -145.39 * Reward -299.32 * True Reward -299.32 * time 5.71 * step 1036\n",
      "Ep 16 * AvgReward -166.58 * true AvgReward -166.58 * Reward -505.62 * True Reward -505.62 * time 5.41 * step 1108\n",
      "Ep 17 * AvgReward -184.28 * true AvgReward -184.28 * Reward -485.15 * True Reward -485.15 * time 4.86 * step 1173\n",
      "Ep 18 * AvgReward -215.95 * true AvgReward -215.95 * Reward -786.04 * True Reward -786.04 * time 6.18 * step 1257\n",
      "Ep 19 * AvgReward -233.33 * true AvgReward -233.33 * Reward -563.52 * True Reward -563.52 * time 4.81 * step 1323\n",
      "Ep 20 * AvgReward -248.63 * true AvgReward -248.63 * Reward -486.70 * True Reward -486.70 * time 4.43 * step 1385\n",
      "Ep 21 * AvgReward -263.69 * true AvgReward -263.69 * Reward -434.16 * True Reward -434.16 * time 4.41 * step 1446\n",
      "Ep 22 * AvgReward -278.46 * true AvgReward -278.46 * Reward -475.55 * True Reward -475.55 * time 4.27 * step 1505\n",
      "Ep 23 * AvgReward -296.43 * true AvgReward -296.43 * Reward -522.85 * True Reward -522.85 * time 4.29 * step 1565\n",
      "Ep 24 * AvgReward -309.54 * true AvgReward -309.54 * Reward -405.35 * True Reward -405.35 * time 4.66 * step 1630\n",
      "Ep 25 * AvgReward -330.77 * true AvgReward -330.77 * Reward -531.35 * True Reward -531.35 * time 5.45 * step 1705\n",
      "Ep 26 * AvgReward -354.42 * true AvgReward -354.42 * Reward -576.65 * True Reward -576.65 * time 5.80 * step 1781\n",
      "Ep 27 * AvgReward -369.67 * true AvgReward -369.67 * Reward -454.98 * True Reward -454.98 * time 4.25 * step 1841\n",
      "Ep 28 * AvgReward -379.06 * true AvgReward -379.06 * Reward -269.55 * True Reward -269.55 * time 3.90 * step 1897\n",
      "Ep 29 * AvgReward -395.59 * true AvgReward -395.59 * Reward -494.04 * True Reward -494.04 * time 3.97 * step 1954\n",
      "Ep 30 * AvgReward -409.48 * true AvgReward -409.48 * Reward -416.45 * True Reward -416.45 * time 4.69 * step 2021\n",
      "Ep 31 * AvgReward -420.79 * true AvgReward -420.79 * Reward -315.38 * True Reward -315.38 * time 3.64 * step 2072\n",
      "Ep 32 * AvgReward -429.74 * true AvgReward -429.74 * Reward -320.34 * True Reward -320.34 * time 3.98 * step 2129\n",
      "Ep 33 * AvgReward -438.77 * true AvgReward -438.77 * Reward -303.28 * True Reward -303.28 * time 4.08 * step 2187\n",
      "Ep 34 * AvgReward -458.05 * true AvgReward -458.05 * Reward -514.67 * True Reward -514.67 * time 5.37 * step 2265\n",
      "Ep 35 * AvgReward -468.18 * true AvgReward -468.18 * Reward -502.01 * True Reward -502.01 * time 5.15 * step 2338\n",
      "Ep 36 * AvgReward -482.61 * true AvgReward -482.61 * Reward -794.20 * True Reward -794.20 * time 6.72 * step 2430\n",
      "Ep 37 * AvgReward -482.30 * true AvgReward -482.30 * Reward -478.93 * True Reward -478.93 * time 6.56 * step 2512\n",
      "Ep 38 * AvgReward -467.57 * true AvgReward -467.57 * Reward -491.35 * True Reward -491.35 * time 5.96 * step 2598\n",
      "Ep 39 * AvgReward -454.94 * true AvgReward -454.94 * Reward -311.02 * True Reward -311.02 * time 6.27 * step 2686\n",
      "Ep 40 * AvgReward -440.58 * true AvgReward -440.58 * Reward -199.54 * True Reward -199.54 * time 5.88 * step 2769\n",
      "Ep 41 * AvgReward -425.55 * true AvgReward -425.55 * Reward -133.59 * True Reward -133.59 * time 5.14 * step 2843\n",
      "Ep 42 * AvgReward -409.04 * true AvgReward -409.04 * Reward -145.20 * True Reward -145.20 * time 5.21 * step 2914\n",
      "Ep 43 * AvgReward -389.37 * true AvgReward -389.37 * Reward -129.46 * True Reward -129.46 * time 4.32 * step 2977\n",
      "Ep 44 * AvgReward -376.12 * true AvgReward -376.12 * Reward -140.45 * True Reward -140.45 * time 4.73 * step 3047\n",
      "Ep 45 * AvgReward -357.56 * true AvgReward -357.56 * Reward -160.04 * True Reward -160.04 * time 5.95 * step 3134\n",
      "Ep 46 * AvgReward -335.28 * true AvgReward -335.28 * Reward -131.11 * True Reward -131.11 * time 3.97 * step 3190\n",
      "Ep 47 * AvgReward -317.59 * true AvgReward -317.59 * Reward -101.09 * True Reward -101.09 * time 3.80 * step 3243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 48 * AvgReward -311.92 * true AvgReward -311.92 * Reward -156.34 * True Reward -156.34 * time 5.33 * step 3320\n",
      "Ep 49 * AvgReward -296.84 * true AvgReward -296.84 * Reward -192.37 * True Reward -192.37 * time 6.02 * step 3405\n",
      "Ep 50 * AvgReward -282.44 * true AvgReward -282.44 * Reward -128.40 * True Reward -128.40 * time 4.28 * step 3466\n",
      "Ep 51 * AvgReward -273.46 * true AvgReward -273.46 * Reward -135.75 * True Reward -135.75 * time 6.44 * step 3561\n",
      "Ep 52 * AvgReward -265.54 * true AvgReward -265.54 * Reward -161.98 * True Reward -161.98 * time 5.06 * step 3635\n",
      "Ep 53 * AvgReward -259.00 * true AvgReward -259.00 * Reward -172.53 * True Reward -172.53 * time 5.58 * step 3717\n",
      "Ep 54 * AvgReward -236.82 * true AvgReward -236.82 * Reward -71.02 * True Reward -71.02 * time 4.92 * step 3789\n",
      "Ep 55 * AvgReward -218.88 * true AvgReward -218.88 * Reward -143.20 * True Reward -143.20 * time 4.36 * step 3854\n",
      "Ep 56 * AvgReward -186.37 * true AvgReward -186.37 * Reward -143.96 * True Reward -143.96 * time 4.91 * step 3926\n",
      "Ep 57 * AvgReward -167.83 * true AvgReward -167.83 * Reward -108.26 * True Reward -108.26 * time 5.34 * step 4004\n",
      "Ep 58 * AvgReward -153.07 * true AvgReward -153.07 * Reward -196.08 * True Reward -196.08 * time 5.15 * step 4077\n",
      "Ep 59 * AvgReward -146.54 * true AvgReward -146.54 * Reward -180.39 * True Reward -180.39 * time 6.40 * step 4166\n",
      "Ep 60 * AvgReward -143.23 * true AvgReward -143.23 * Reward -133.42 * True Reward -133.42 * time 4.15 * step 4226\n",
      "Ep 61 * AvgReward -143.30 * true AvgReward -143.30 * Reward -134.96 * True Reward -134.96 * time 6.94 * step 4322\n",
      "Ep 62 * AvgReward -140.88 * true AvgReward -140.88 * Reward -96.76 * True Reward -96.76 * time 5.34 * step 4397\n",
      "Ep 63 * AvgReward -160.37 * true AvgReward -160.37 * Reward -519.32 * True Reward -519.32 * time 5.89 * step 4483\n",
      "Ep 64 * AvgReward -202.74 * true AvgReward -202.74 * Reward -987.76 * True Reward -987.76 * time 6.40 * step 4573\n",
      "Ep 65 * AvgReward -219.25 * true AvgReward -219.25 * Reward -490.20 * True Reward -490.20 * time 5.02 * step 4644\n",
      "Ep 66 * AvgReward -229.57 * true AvgReward -229.57 * Reward -337.59 * True Reward -337.59 * time 3.75 * step 4698\n",
      "Ep 67 * AvgReward -243.29 * true AvgReward -243.29 * Reward -375.43 * True Reward -375.43 * time 4.86 * step 4769\n",
      "Ep 68 * AvgReward -256.02 * true AvgReward -256.02 * Reward -410.99 * True Reward -410.99 * time 4.14 * step 4830\n",
      "Ep 69 * AvgReward -266.37 * true AvgReward -266.37 * Reward -399.44 * True Reward -399.44 * time 3.56 * step 4881\n",
      "Ep 70 * AvgReward -288.23 * true AvgReward -288.23 * Reward -565.52 * True Reward -565.52 * time 4.36 * step 4944\n",
      "Ep 71 * AvgReward -312.50 * true AvgReward -312.50 * Reward -621.29 * True Reward -621.29 * time 5.61 * step 5024\n",
      "Ep 72 * AvgReward -328.58 * true AvgReward -328.58 * Reward -483.49 * True Reward -483.49 * time 4.50 * step 5089\n",
      "Ep 73 * AvgReward -356.83 * true AvgReward -356.83 * Reward -737.46 * True Reward -737.46 * time 5.82 * step 5172\n",
      "Ep 74 * AvgReward -372.00 * true AvgReward -372.00 * Reward -374.39 * True Reward -374.39 * time 3.78 * step 5227\n",
      "Ep 75 * AvgReward -391.00 * true AvgReward -391.00 * Reward -523.37 * True Reward -523.37 * time 4.88 * step 5297\n",
      "Ep 76 * AvgReward -389.71 * true AvgReward -389.71 * Reward -118.01 * True Reward -118.01 * time 5.55 * step 5378\n",
      "Ep 77 * AvgReward -388.99 * true AvgReward -388.99 * Reward -93.83 * True Reward -93.83 * time 6.04 * step 5467\n",
      "Ep 78 * AvgReward -385.13 * true AvgReward -385.13 * Reward -119.08 * True Reward -119.08 * time 5.13 * step 5543\n",
      "Ep 79 * AvgReward -386.06 * true AvgReward -386.06 * Reward -198.89 * True Reward -198.89 * time 4.80 * step 5615\n",
      "Ep 80 * AvgReward -385.48 * true AvgReward -385.48 * Reward -121.75 * True Reward -121.75 * time 5.14 * step 5690\n",
      "Ep 81 * AvgReward -386.24 * true AvgReward -386.24 * Reward -150.16 * True Reward -150.16 * time 5.61 * step 5769\n",
      "Ep 82 * AvgReward -396.73 * true AvgReward -396.73 * Reward -306.72 * True Reward -306.72 * time 6.14 * step 5859\n",
      "Ep 83 * AvgReward -376.33 * true AvgReward -376.33 * Reward -111.19 * True Reward -111.19 * time 3.77 * step 5914\n",
      "Ep 84 * AvgReward -332.60 * true AvgReward -332.60 * Reward -113.26 * True Reward -113.26 * time 6.36 * step 6005\n",
      "Ep 85 * AvgReward -315.11 * true AvgReward -315.11 * Reward -140.32 * True Reward -140.32 * time 3.50 * step 6055\n",
      "Ep 86 * AvgReward -304.09 * true AvgReward -304.09 * Reward -117.12 * True Reward -117.12 * time 4.30 * step 6118\n",
      "Ep 87 * AvgReward -291.60 * true AvgReward -291.60 * Reward -125.64 * True Reward -125.64 * time 6.11 * step 6207\n",
      "Ep 88 * AvgReward -278.56 * true AvgReward -278.56 * Reward -150.35 * True Reward -150.35 * time 5.31 * step 6286\n",
      "Ep 89 * AvgReward -266.06 * true AvgReward -266.06 * Reward -149.37 * True Reward -149.37 * time 6.15 * step 6376\n",
      "Ep 90 * AvgReward -244.59 * true AvgReward -244.59 * Reward -136.10 * True Reward -136.10 * time 6.10 * step 6464\n",
      "Ep 91 * AvgReward -220.50 * true AvgReward -220.50 * Reward -139.51 * True Reward -139.51 * time 6.24 * step 6555\n",
      "Ep 92 * AvgReward -203.63 * true AvgReward -203.63 * Reward -146.16 * True Reward -146.16 * time 5.24 * step 6632\n",
      "Ep 93 * AvgReward -174.75 * true AvgReward -174.75 * Reward -159.69 * True Reward -159.69 * time 4.48 * step 6698\n",
      "Ep 94 * AvgReward -161.18 * true AvgReward -161.18 * Reward -103.15 * True Reward -103.15 * time 4.04 * step 6757\n",
      "Ep 95 * AvgReward -140.90 * true AvgReward -140.90 * Reward -117.66 * True Reward -117.66 * time 4.77 * step 6825\n",
      "Ep 96 * AvgReward -144.65 * true AvgReward -144.65 * Reward -193.04 * True Reward -193.04 * time 6.99 * step 6922\n",
      "Ep 97 * AvgReward -146.77 * true AvgReward -146.77 * Reward -136.22 * True Reward -136.22 * time 4.92 * step 6994\n",
      "Ep 98 * AvgReward -146.70 * true AvgReward -146.70 * Reward -117.73 * True Reward -117.73 * time 3.60 * step 7047\n",
      "Ep 99 * AvgReward -143.32 * true AvgReward -143.32 * Reward -131.26 * True Reward -131.26 * time 3.80 * step 7103\n",
      "Ep 100 * AvgReward -143.52 * true AvgReward -143.52 * Reward -125.75 * True Reward -125.75 * time 5.19 * step 7179\n",
      "Ep 101 * AvgReward -143.84 * true AvgReward -143.84 * Reward -156.47 * True Reward -156.47 * time 4.18 * step 7238\n",
      "Ep 102 * AvgReward -134.72 * true AvgReward -134.72 * Reward -124.46 * True Reward -124.46 * time 3.60 * step 7291\n",
      "Ep 103 * AvgReward -136.40 * true AvgReward -136.40 * Reward -144.77 * True Reward -144.77 * time 4.93 * step 7363\n",
      "Ep 104 * AvgReward -137.93 * true AvgReward -137.93 * Reward -143.91 * True Reward -143.91 * time 5.46 * step 7440\n",
      "Ep 105 * AvgReward -137.05 * true AvgReward -137.05 * Reward -122.69 * True Reward -122.69 * time 4.47 * step 7505\n",
      "Ep 106 * AvgReward -136.72 * true AvgReward -136.72 * Reward -110.56 * True Reward -110.56 * time 4.42 * step 7568\n",
      "Ep 107 * AvgReward -134.90 * true AvgReward -134.90 * Reward -89.20 * True Reward -89.20 * time 3.89 * step 7624\n",
      "Ep 108 * AvgReward -132.91 * true AvgReward -132.91 * Reward -110.60 * True Reward -110.60 * time 4.54 * step 7688\n",
      "Ep 109 * AvgReward -131.29 * true AvgReward -131.29 * Reward -116.90 * True Reward -116.90 * time 3.99 * step 7746\n",
      "Ep 110 * AvgReward -127.91 * true AvgReward -127.91 * Reward -68.53 * True Reward -68.53 * time 3.47 * step 7799\n",
      "Ep 111 * AvgReward -131.45 * true AvgReward -131.45 * Reward -210.19 * True Reward -210.19 * time 5.44 * step 7881\n",
      "Ep 112 * AvgReward -129.97 * true AvgReward -129.97 * Reward -116.69 * True Reward -116.69 * time 4.05 * step 7942\n",
      "Ep 113 * AvgReward -129.17 * true AvgReward -129.17 * Reward -143.71 * True Reward -143.71 * time 3.83 * step 8000\n",
      "Ep 114 * AvgReward -131.45 * true AvgReward -131.45 * Reward -148.71 * True Reward -148.71 * time 3.84 * step 8058\n",
      "Ep 115 * AvgReward -130.97 * true AvgReward -130.97 * Reward -108.06 * True Reward -108.06 * time 6.52 * step 8156\n",
      "Ep 116 * AvgReward -128.61 * true AvgReward -128.61 * Reward -145.86 * True Reward -145.86 * time 6.80 * step 8254\n",
      "Ep 117 * AvgReward -128.91 * true AvgReward -128.91 * Reward -142.20 * True Reward -142.20 * time 4.98 * step 8328\n",
      "Ep 118 * AvgReward -122.87 * true AvgReward -122.87 * Reward 3.08 * True Reward 3.08 * time 6.69 * step 8421\n",
      "Ep 119 * AvgReward -133.42 * true AvgReward -133.42 * Reward -342.28 * True Reward -342.28 * time 6.77 * step 8520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 120 * AvgReward -134.19 * true AvgReward -134.19 * Reward -141.20 * True Reward -141.20 * time 5.86 * step 8607\n",
      "Ep 121 * AvgReward -131.56 * true AvgReward -131.56 * Reward -103.80 * True Reward -103.80 * time 4.33 * step 8671\n",
      "Ep 122 * AvgReward -131.36 * true AvgReward -131.36 * Reward -120.38 * True Reward -120.38 * time 5.15 * step 8748\n",
      "Ep 123 * AvgReward -130.35 * true AvgReward -130.35 * Reward -124.53 * True Reward -124.53 * time 4.13 * step 8809\n",
      "Ep 124 * AvgReward -130.71 * true AvgReward -130.71 * Reward -151.25 * True Reward -151.25 * time 4.16 * step 8870\n",
      "Ep 125 * AvgReward -130.72 * true AvgReward -130.72 * Reward -122.86 * True Reward -122.86 * time 4.40 * step 8936\n",
      "Ep 126 * AvgReward -130.10 * true AvgReward -130.10 * Reward -98.05 * True Reward -98.05 * time 4.66 * step 9003\n",
      "Ep 127 * AvgReward -131.13 * true AvgReward -131.13 * Reward -109.96 * True Reward -109.96 * time 4.30 * step 9066\n",
      "Ep 128 * AvgReward -136.42 * true AvgReward -136.42 * Reward -216.37 * True Reward -216.37 * time 6.60 * step 9156\n",
      "Ep 129 * AvgReward -135.31 * true AvgReward -135.31 * Reward -94.70 * True Reward -94.70 * time 4.03 * step 9215\n",
      "Ep 130 * AvgReward -139.26 * true AvgReward -139.26 * Reward -147.53 * True Reward -147.53 * time 4.99 * step 9288\n",
      "Ep 131 * AvgReward -135.16 * true AvgReward -135.16 * Reward -128.22 * True Reward -128.22 * time 5.64 * step 9369\n",
      "Ep 132 * AvgReward -135.17 * true AvgReward -135.17 * Reward -116.80 * True Reward -116.80 * time 5.64 * step 9449\n",
      "Ep 133 * AvgReward -133.51 * true AvgReward -133.51 * Reward -110.54 * True Reward -110.54 * time 5.68 * step 9531\n",
      "Ep 134 * AvgReward -131.18 * true AvgReward -131.18 * Reward -102.12 * True Reward -102.12 * time 5.05 * step 9601\n",
      "Ep 135 * AvgReward -134.03 * true AvgReward -134.03 * Reward -165.02 * True Reward -165.02 * time 3.66 * step 9656\n",
      "Ep 136 * AvgReward -133.66 * true AvgReward -133.66 * Reward -138.54 * True Reward -138.54 * time 3.89 * step 9713\n",
      "Ep 137 * AvgReward -132.44 * true AvgReward -132.44 * Reward -117.68 * True Reward -117.68 * time 4.85 * step 9785\n",
      "Ep 138 * AvgReward -142.00 * true AvgReward -142.00 * Reward -188.25 * True Reward -188.25 * time 6.02 * step 9872\n",
      "Ep 139 * AvgReward -132.33 * true AvgReward -132.33 * Reward -148.79 * True Reward -148.79 * time 4.12 * step 9931\n",
      "Ep 140 * AvgReward -130.58 * true AvgReward -130.58 * Reward -106.11 * True Reward -106.11 * time 4.03 * step 9989\n",
      "Ep 141 * AvgReward -131.19 * true AvgReward -131.19 * Reward -116.16 * True Reward -116.16 * time 6.11 * step 10074\n",
      "Ep 142 * AvgReward -134.66 * true AvgReward -134.66 * Reward -189.78 * True Reward -189.78 * time 5.69 * step 10156\n",
      "Ep 143 * AvgReward -135.34 * true AvgReward -135.34 * Reward -137.97 * True Reward -137.97 * time 5.81 * step 10238\n",
      "Ep 144 * AvgReward -133.29 * true AvgReward -133.29 * Reward -110.39 * True Reward -110.39 * time 3.50 * step 10289\n",
      "Ep 145 * AvgReward -142.24 * true AvgReward -142.24 * Reward -301.73 * True Reward -301.73 * time 6.36 * step 10382\n",
      "Ep 146 * AvgReward -145.50 * true AvgReward -145.50 * Reward -163.42 * True Reward -163.42 * time 4.68 * step 10452\n",
      "Ep 147 * AvgReward -150.75 * true AvgReward -150.75 * Reward -214.84 * True Reward -214.84 * time 6.20 * step 10542\n",
      "Ep 148 * AvgReward -146.90 * true AvgReward -146.90 * Reward -139.37 * True Reward -139.37 * time 5.48 * step 10621\n",
      "Ep 149 * AvgReward -150.68 * true AvgReward -150.68 * Reward -170.35 * True Reward -170.35 * time 5.45 * step 10700\n",
      "Ep 150 * AvgReward -154.81 * true AvgReward -154.81 * Reward -230.12 * True Reward -230.12 * time 5.86 * step 10783\n",
      "Ep 151 * AvgReward -156.70 * true AvgReward -156.70 * Reward -165.96 * True Reward -165.96 * time 5.09 * step 10857\n",
      "Ep 152 * AvgReward -157.84 * true AvgReward -157.84 * Reward -139.65 * True Reward -139.65 * time 4.89 * step 10928\n",
      "Ep 153 * AvgReward -159.64 * true AvgReward -159.64 * Reward -146.48 * True Reward -146.48 * time 5.67 * step 11006\n",
      "Ep 154 * AvgReward -164.81 * true AvgReward -164.81 * Reward -205.56 * True Reward -205.56 * time 5.87 * step 11089\n",
      "Ep 155 * AvgReward -162.79 * true AvgReward -162.79 * Reward -124.59 * True Reward -124.59 * time 5.53 * step 11167\n",
      "Ep 156 * AvgReward -164.93 * true AvgReward -164.93 * Reward -181.36 * True Reward -181.36 * time 5.58 * step 11246\n",
      "Ep 157 * AvgReward -163.87 * true AvgReward -163.87 * Reward -96.54 * True Reward -96.54 * time 5.74 * step 11327\n",
      "Ep 158 * AvgReward -161.20 * true AvgReward -161.20 * Reward -134.78 * True Reward -134.78 * time 4.60 * step 11392\n",
      "Ep 159 * AvgReward -160.68 * true AvgReward -160.68 * Reward -138.42 * True Reward -138.42 * time 5.10 * step 11464\n",
      "Ep 160 * AvgReward -164.44 * true AvgReward -164.44 * Reward -181.39 * True Reward -181.39 * time 3.74 * step 11517\n",
      "Ep 161 * AvgReward -165.63 * true AvgReward -165.63 * Reward -139.91 * True Reward -139.91 * time 3.76 * step 11570\n",
      "Ep 162 * AvgReward -161.26 * true AvgReward -161.26 * Reward -102.28 * True Reward -102.28 * time 4.29 * step 11630\n",
      "Ep 163 * AvgReward -164.10 * true AvgReward -164.10 * Reward -194.90 * True Reward -194.90 * time 5.52 * step 11708\n",
      "Ep 164 * AvgReward -163.40 * true AvgReward -163.40 * Reward -96.37 * True Reward -96.37 * time 6.74 * step 11804\n",
      "Ep 165 * AvgReward -154.25 * true AvgReward -154.25 * Reward -118.73 * True Reward -118.73 * time 4.21 * step 11864\n",
      "Ep 166 * AvgReward -150.66 * true AvgReward -150.66 * Reward -91.60 * True Reward -91.60 * time 5.61 * step 11941\n",
      "Ep 167 * AvgReward -146.51 * true AvgReward -146.51 * Reward -131.83 * True Reward -131.83 * time 6.31 * step 12031\n",
      "Ep 168 * AvgReward -144.42 * true AvgReward -144.42 * Reward -97.60 * True Reward -97.60 * time 4.33 * step 12092\n",
      "Ep 169 * AvgReward -142.47 * true AvgReward -142.47 * Reward -131.22 * True Reward -131.22 * time 4.28 * step 12153\n",
      "Ep 170 * AvgReward -135.75 * true AvgReward -135.75 * Reward -95.85 * True Reward -95.85 * time 4.53 * step 12218\n",
      "Ep 171 * AvgReward -126.67 * true AvgReward -126.67 * Reward 15.67 * True Reward 15.67 * time 6.64 * step 12313\n",
      "Ep 172 * AvgReward -124.82 * true AvgReward -124.82 * Reward -102.73 * True Reward -102.73 * time 4.03 * step 12371\n",
      "Ep 173 * AvgReward -124.61 * true AvgReward -124.61 * Reward -142.17 * True Reward -142.17 * time 5.21 * step 12444\n",
      "Ep 174 * AvgReward -120.39 * true AvgReward -120.39 * Reward -121.15 * True Reward -121.15 * time 4.39 * step 12507\n",
      "Ep 175 * AvgReward -120.54 * true AvgReward -120.54 * Reward -127.60 * True Reward -127.60 * time 4.55 * step 12572\n",
      "Ep 176 * AvgReward -116.78 * true AvgReward -116.78 * Reward -106.18 * True Reward -106.18 * time 4.33 * step 12632\n",
      "Ep 177 * AvgReward -119.89 * true AvgReward -119.89 * Reward -158.82 * True Reward -158.82 * time 4.48 * step 12694\n",
      "Ep 178 * AvgReward -119.90 * true AvgReward -119.90 * Reward -134.89 * True Reward -134.89 * time 5.74 * step 12775\n",
      "Ep 179 * AvgReward -118.91 * true AvgReward -118.91 * Reward -118.58 * True Reward -118.58 * time 5.79 * step 12855\n",
      "Ep 180 * AvgReward -111.62 * true AvgReward -111.62 * Reward -35.67 * True Reward -35.67 * time 5.87 * step 12939\n",
      "Ep 181 * AvgReward -110.49 * true AvgReward -110.49 * Reward -117.38 * True Reward -117.38 * time 4.35 * step 13001\n",
      "Ep 182 * AvgReward -112.96 * true AvgReward -112.96 * Reward -151.53 * True Reward -151.53 * time 4.92 * step 13071\n",
      "Ep 183 * AvgReward -112.58 * true AvgReward -112.58 * Reward -187.35 * True Reward -187.35 * time 6.10 * step 13156\n",
      "Ep 184 * AvgReward -115.80 * true AvgReward -115.80 * Reward -160.88 * True Reward -160.88 * time 6.26 * step 13243\n",
      "Ep 185 * AvgReward -118.77 * true AvgReward -118.77 * Reward -178.12 * True Reward -178.12 * time 4.91 * step 13311\n",
      "Ep 186 * AvgReward -121.91 * true AvgReward -121.91 * Reward -154.25 * True Reward -154.25 * time 4.66 * step 13380\n",
      "Ep 187 * AvgReward -127.57 * true AvgReward -127.57 * Reward -245.07 * True Reward -245.07 * time 6.84 * step 13481\n",
      "Ep 188 * AvgReward -144.85 * true AvgReward -144.85 * Reward -443.27 * True Reward -443.27 * time 6.45 * step 13575\n",
      "Ep 189 * AvgReward -169.37 * true AvgReward -169.37 * Reward -621.57 * True Reward -621.57 * time 6.97 * step 13678\n",
      "Ep 190 * AvgReward -216.39 * true AvgReward -216.39 * Reward -1036.28 * True Reward -1036.28 * time 14.93 * step 13882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 191 * AvgReward -242.20 * true AvgReward -242.20 * Reward -500.60 * True Reward -500.60 * time 7.87 * step 13998\n",
      "Ep 192 * AvgReward -261.60 * true AvgReward -261.60 * Reward -490.59 * True Reward -490.59 * time 6.27 * step 14087\n",
      "Ep 193 * AvgReward -283.85 * true AvgReward -283.85 * Reward -587.13 * True Reward -587.13 * time 8.67 * step 14213\n",
      "Ep 194 * AvgReward -298.22 * true AvgReward -298.22 * Reward -408.69 * True Reward -408.69 * time 5.62 * step 14292\n",
      "Ep 195 * AvgReward -338.81 * true AvgReward -338.81 * Reward -939.37 * True Reward -939.37 * time 14.35 * step 14499\n",
      "Ep 196 * AvgReward -358.72 * true AvgReward -358.72 * Reward -504.31 * True Reward -504.31 * time 7.62 * step 14611\n",
      "Ep 197 * AvgReward -381.04 * true AvgReward -381.04 * Reward -605.28 * True Reward -605.28 * time 5.63 * step 14692\n",
      "Ep 198 * AvgReward -394.74 * true AvgReward -394.74 * Reward -408.89 * True Reward -408.89 * time 5.51 * step 14772\n",
      "Ep 199 * AvgReward -422.52 * true AvgReward -422.52 * Reward -674.13 * True Reward -674.13 * time 6.86 * step 14871\n",
      "Ep 200 * AvgReward -448.49 * true AvgReward -448.49 * Reward -555.18 * True Reward -555.18 * time 6.10 * step 14959\n",
      "Ep 201 * AvgReward -461.93 * true AvgReward -461.93 * Reward -386.11 * True Reward -386.11 * time 11.06 * step 15122\n",
      "Ep 202 * AvgReward -471.63 * true AvgReward -471.63 * Reward -345.50 * True Reward -345.50 * time 7.63 * step 15236\n",
      "Ep 203 * AvgReward -479.52 * true AvgReward -479.52 * Reward -345.24 * True Reward -345.24 * time 7.99 * step 15356\n",
      "Ep 204 * AvgReward -494.96 * true AvgReward -494.96 * Reward -469.60 * True Reward -469.60 * time 23.11 * step 15699\n",
      "Ep 205 * AvgReward -499.85 * true AvgReward -499.85 * Reward -275.88 * True Reward -275.88 * time 6.83 * step 15795\n",
      "Ep 206 * AvgReward -507.14 * true AvgReward -507.14 * Reward -300.13 * True Reward -300.13 * time 13.14 * step 15985\n",
      "Ep 207 * AvgReward -511.54 * true AvgReward -511.54 * Reward -332.98 * True Reward -332.98 * time 9.08 * step 16118\n",
      "Ep 208 * AvgReward -506.22 * true AvgReward -506.22 * Reward -336.98 * True Reward -336.98 * time 16.02 * step 16353\n",
      "Ep 209 * AvgReward -516.07 * true AvgReward -516.07 * Reward -818.60 * True Reward -818.60 * time 66.40 * step 17297\n",
      "Ep 210 * AvgReward -487.07 * true AvgReward -487.07 * Reward -456.20 * True Reward -456.20 * time 8.68 * step 17415\n",
      "Ep 211 * AvgReward -478.55 * true AvgReward -478.55 * Reward -330.30 * True Reward -330.30 * time 7.29 * step 17511\n",
      "Ep 212 * AvgReward -479.94 * true AvgReward -479.94 * Reward -518.27 * True Reward -518.27 * time 9.59 * step 17639\n",
      "Ep 213 * AvgReward -477.89 * true AvgReward -477.89 * Reward -546.18 * True Reward -546.18 * time 6.69 * step 17731\n",
      "Ep 214 * AvgReward -484.41 * true AvgReward -484.41 * Reward -539.16 * True Reward -539.16 * time 7.97 * step 17838\n",
      "Ep 215 * AvgReward -465.71 * true AvgReward -465.71 * Reward -565.36 * True Reward -565.36 * time 5.59 * step 17915\n",
      "Ep 216 * AvgReward -466.71 * true AvgReward -466.71 * Reward -524.32 * True Reward -524.32 * time 5.01 * step 17984\n",
      "Ep 217 * AvgReward -459.81 * true AvgReward -459.81 * Reward -467.25 * True Reward -467.25 * time 6.82 * step 18078\n",
      "Ep 218 * AvgReward -463.53 * true AvgReward -463.53 * Reward -483.13 * True Reward -483.13 * time 7.19 * step 18178\n",
      "Ep 219 * AvgReward -461.01 * true AvgReward -461.01 * Reward -623.79 * True Reward -623.79 * time 6.20 * step 18263\n",
      "Ep 220 * AvgReward -457.92 * true AvgReward -457.92 * Reward -493.33 * True Reward -493.33 * time 8.88 * step 18385\n",
      "Ep 221 * AvgReward -456.53 * true AvgReward -456.53 * Reward -358.30 * True Reward -358.30 * time 6.15 * step 18468\n",
      "Ep 222 * AvgReward -464.47 * true AvgReward -464.47 * Reward -504.30 * True Reward -504.30 * time 5.20 * step 18541\n",
      "Ep 223 * AvgReward -465.26 * true AvgReward -465.26 * Reward -361.22 * True Reward -361.22 * time 5.94 * step 18623\n",
      "Ep 224 * AvgReward -457.96 * true AvgReward -457.96 * Reward -323.54 * True Reward -323.54 * time 6.44 * step 18712\n",
      "Ep 225 * AvgReward -460.89 * true AvgReward -460.89 * Reward -334.37 * True Reward -334.37 * time 7.13 * step 18812\n",
      "Ep 226 * AvgReward -458.14 * true AvgReward -458.14 * Reward -245.26 * True Reward -245.26 * time 7.11 * step 18911\n",
      "Ep 227 * AvgReward -455.11 * true AvgReward -455.11 * Reward -272.30 * True Reward -272.30 * time 10.31 * step 19056\n",
      "Ep 228 * AvgReward -446.82 * true AvgReward -446.82 * Reward -171.27 * True Reward -171.27 * time 8.48 * step 19174\n",
      "Ep 229 * AvgReward -423.20 * true AvgReward -423.20 * Reward -346.11 * True Reward -346.11 * time 16.86 * step 19412\n",
      "Ep 230 * AvgReward -407.91 * true AvgReward -407.91 * Reward -150.37 * True Reward -150.37 * time 14.62 * step 19622\n",
      "Ep 231 * AvgReward -403.26 * true AvgReward -403.26 * Reward -237.30 * True Reward -237.30 * time 23.22 * step 19958\n",
      "Ep 232 * AvgReward -399.63 * true AvgReward -399.63 * Reward -445.72 * True Reward -445.72 * time 29.83 * step 20394\n",
      "Ep 233 * AvgReward -381.80 * true AvgReward -381.80 * Reward -189.54 * True Reward -189.54 * time 26.60 * step 20778\n",
      "Ep 234 * AvgReward -366.70 * true AvgReward -366.70 * Reward -237.20 * True Reward -237.20 * time 21.34 * step 21083\n",
      "Ep 235 * AvgReward -343.91 * true AvgReward -343.91 * Reward -109.65 * True Reward -109.65 * time 73.84 * step 22083\n",
      "Ep 236 * AvgReward -322.14 * true AvgReward -322.14 * Reward -88.78 * True Reward -88.78 * time 72.83 * step 23083\n",
      "Ep 237 * AvgReward -307.03 * true AvgReward -307.03 * Reward -165.17 * True Reward -165.17 * time 71.31 * step 24083\n",
      "Ep 238 * AvgReward -289.42 * true AvgReward -289.42 * Reward -130.92 * True Reward -130.92 * time 71.81 * step 25083\n",
      "Ep 239 * AvgReward -267.84 * true AvgReward -267.84 * Reward -192.20 * True Reward -192.20 * time 60.95 * step 25933\n",
      "Ep 240 * AvgReward -250.10 * true AvgReward -250.10 * Reward -138.48 * True Reward -138.48 * time 71.88 * step 26933\n",
      "Ep 241 * AvgReward -240.67 * true AvgReward -240.67 * Reward -169.72 * True Reward -169.72 * time 42.88 * step 27526\n",
      "Ep 242 * AvgReward -229.35 * true AvgReward -229.35 * Reward -277.81 * True Reward -277.81 * time 13.36 * step 27711\n",
      "Ep 243 * AvgReward -217.50 * true AvgReward -217.50 * Reward -124.34 * True Reward -124.34 * time 12.02 * step 27876\n",
      "Ep 244 * AvgReward -208.76 * true AvgReward -208.76 * Reward -148.79 * True Reward -148.79 * time 17.52 * step 28116\n",
      "Ep 245 * AvgReward -198.89 * true AvgReward -198.89 * Reward -136.98 * True Reward -136.98 * time 10.41 * step 28261\n",
      "Ep 246 * AvgReward -192.33 * true AvgReward -192.33 * Reward -114.04 * True Reward -114.04 * time 13.89 * step 28454\n",
      "Ep 247 * AvgReward -187.11 * true AvgReward -187.11 * Reward -167.83 * True Reward -167.83 * time 15.71 * step 28676\n",
      "Ep 248 * AvgReward -184.98 * true AvgReward -184.98 * Reward -128.77 * True Reward -128.77 * time 8.45 * step 28795\n",
      "Ep 249 * AvgReward -172.54 * true AvgReward -172.54 * Reward -97.29 * True Reward -97.29 * time 4.48 * step 28859\n",
      "Ep 250 * AvgReward -175.96 * true AvgReward -175.96 * Reward -218.68 * True Reward -218.68 * time 27.38 * step 29245\n",
      "Ep 251 * AvgReward -169.30 * true AvgReward -169.30 * Reward -104.15 * True Reward -104.15 * time 14.67 * step 29454\n",
      "Ep 252 * AvgReward -152.11 * true AvgReward -152.11 * Reward -101.91 * True Reward -101.91 * time 5.66 * step 29537\n",
      "Ep 253 * AvgReward -145.29 * true AvgReward -145.29 * Reward -53.11 * True Reward -53.11 * time 5.31 * step 29616\n",
      "Ep 254 * AvgReward -135.28 * true AvgReward -135.28 * Reward -36.95 * True Reward -36.95 * time 6.11 * step 29707\n",
      "Ep 255 * AvgReward -131.17 * true AvgReward -131.17 * Reward -27.47 * True Reward -27.47 * time 5.59 * step 29790\n",
      "Ep 256 * AvgReward -126.79 * true AvgReward -126.79 * Reward -1.19 * True Reward -1.19 * time 7.50 * step 29902\n",
      "Ep 257 * AvgReward -121.65 * true AvgReward -121.65 * Reward -62.32 * True Reward -62.32 * time 6.15 * step 29991\n",
      "Ep 258 * AvgReward -123.60 * true AvgReward -123.60 * Reward -169.92 * True Reward -169.92 * time 10.13 * step 30139\n",
      "Ep 259 * AvgReward -121.53 * true AvgReward -121.53 * Reward -150.91 * True Reward -150.91 * time 15.38 * step 30358\n",
      "Ep 260 * AvgReward -118.26 * true AvgReward -118.26 * Reward -73.10 * True Reward -73.10 * time 8.22 * step 30477\n",
      "Ep 261 * AvgReward -117.28 * true AvgReward -117.28 * Reward -150.06 * True Reward -150.06 * time 8.79 * step 30604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 262 * AvgReward -105.84 * true AvgReward -105.84 * Reward -49.06 * True Reward -49.06 * time 5.85 * step 30689\n",
      "Ep 263 * AvgReward -107.49 * true AvgReward -107.49 * Reward -157.32 * True Reward -157.32 * time 13.05 * step 30880\n",
      "Ep 264 * AvgReward -99.82 * true AvgReward -99.82 * Reward 4.65 * True Reward 4.65 * time 8.81 * step 31007\n",
      "Ep 265 * AvgReward -104.06 * true AvgReward -104.06 * Reward -221.84 * True Reward -221.84 * time 61.69 * step 31838\n",
      "Ep 266 * AvgReward -106.35 * true AvgReward -106.35 * Reward -159.75 * True Reward -159.75 * time 13.66 * step 32034\n",
      "Ep 267 * AvgReward -98.56 * true AvgReward -98.56 * Reward -12.06 * True Reward -12.06 * time 6.47 * step 32115\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_capacity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0002\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactor_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontinuous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(total_trials, total_episodes, buffer_capacity, batch_size, std_dev, critic_lr, render, actor_lr, gamma, tau, noise_mult, save_weights, directory, actor_name, critic_name, gamma_func, tau_func, critic_lr_func, actor_lr_func, noise_mult_func, std_dev_func, mean_number, output, return_rewards, total_time, use_guide, solved, continuous, environment, seed, start_steps, gravity, enable_wind, wind_power, turbulence_power, epsilon, epsilon_func, adam_critic_eps, adam_actor_eps, actor_amsgrad, critic_amsgrad)\u001b[0m\n\u001b[0;32m    122\u001b[0m agent\u001b[38;5;241m.\u001b[39mrecord((prev_state, action, reward, state, terminal_state))\n\u001b[0;32m    123\u001b[0m episodic_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m--> 125\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m update_target(agent\u001b[38;5;241m.\u001b[39mtarget_actor\u001b[38;5;241m.\u001b[39mvariables, agent\u001b[38;5;241m.\u001b[39mactor_model\u001b[38;5;241m.\u001b[39mvariables, agent\u001b[38;5;241m.\u001b[39mtau)\n\u001b[0;32m    127\u001b[0m update_target(agent\u001b[38;5;241m.\u001b[39mtarget_critic\u001b[38;5;241m.\u001b[39mvariables, agent\u001b[38;5;241m.\u001b[39mcritic_model\u001b[38;5;241m.\u001b[39mvariables, agent\u001b[38;5;241m.\u001b[39mtau)\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    112\u001b[0m next_state_batch \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_state_buffer[batch_indices])\n\u001b[0;32m    113\u001b[0m done_batch \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone_buffer[batch_indices])\n\u001b[1;32m--> 115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run(total_trials=2, total_episodes=600, buffer_capacity=200000, tau=0.001, critic_lr=0.0002, \n",
    "    actor_lr=0.0001, start_steps=1000, continuous=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
