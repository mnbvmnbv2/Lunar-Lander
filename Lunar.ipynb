{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7764110b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\flatbuffers\\compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  'hamming': pil_image.HAMMING,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  'box': pil_image.BOX,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  'lanczos': pil_image.LANCZOS,\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.math import argmax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e4ab220",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(358)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd30b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdd14b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_actions_num = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6470ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.gymlibrary.ml/environments/box2d/lunar_lander/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f235a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OU Noise\n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "    def __call__(self):\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n",
    "\n",
    "def get_actor(num_states, num_actions=1, upper_bound=1, continuous=True, layer1=400, layer2=300, \n",
    "              init_weights_min=-0.003, init_weights_max=0.003):\n",
    "    last_init = tf.random_uniform_initializer(minval=init_weights_min, maxval=init_weights_max)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(layer1, activation=\"relu\")(inputs)\n",
    "    out = layers.Dense(layer2, activation=\"relu\")(out)\n",
    "    \n",
    "    # Different output activation based on discrete or continous version\n",
    "    if continuous:\n",
    "        outputs = layers.Dense(num_actions, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "    else:\n",
    "        outputs = layers.Dense(disc_actions_num, activation=\"softmax\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Multiply to fill the whole action space which should be equal around 0\n",
    "    outputs = outputs * upper_bound\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "def get_critic(num_states, num_actions=1, continuous=True, layer1=400, layer2=300):\n",
    "    state_input = layers.Input(shape=(num_states,))\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_input)\n",
    "    #state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    if continuous:\n",
    "        action_input = layers.Input(shape=(num_actions,))\n",
    "    else:\n",
    "        action_input = layers.Input(shape=(disc_actions_num,))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(layer1, activation=\"relu\")(concat)\n",
    "    out = layers.Dense(layer2, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(num_actions)(out)\n",
    "#     if continuous:\n",
    "#         outputs = layers.Dense(num_actions)(out)\n",
    "#     else:\n",
    "#         outputs = layers.Dense(disc_actions_num)(out)\n",
    "\n",
    "    return tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "# This updates the weights in a slow manner which keeps stability\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d09a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions=1, lower_bound=-1, upper_bound=1, continuous=True,\n",
    "            buffer_capacity=50000, batch_size=64, std_dev=0.2, critic_lr=0.002,\n",
    "            actor_lr=0.001, gamma=0.99, tau=0.005, epsilon=0.2, adam_critic_eps=1e-07,\n",
    "            adam_actor_eps=1e-07, actor_amsgrad=False, critic_amsgrad=False, actor_layer_1=256, \n",
    "            actor_layer_2=256, critic_layer_1=256, critic_layer_2=256):\n",
    "        \n",
    "        self.continuous = continuous\n",
    "        \n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # For methods\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "\n",
    "        # This is used to make sure we only sample from used buffer space\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        if self.continuous:\n",
    "            self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        else:\n",
    "            self.action_buffer = np.zeros((self.buffer_capacity, disc_actions_num))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "        # Also keep track if it is in terminal state (legs on ground)\n",
    "        self.done_buffer = np.zeros((self.buffer_capacity, 1)).astype(np.float32)\n",
    "        \n",
    "        self.std_dev = std_dev\n",
    "        self.critic_lr = critic_lr\n",
    "        self.actor_lr = actor_lr\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        # Epsilon in epsilon-greedy\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.actor_model = get_actor(\n",
    "            num_states, num_actions, upper_bound, continuous=continuous, layer1=actor_layer_1, layer2=actor_layer_2\n",
    "        )\n",
    "        self.critic_model = get_critic(\n",
    "            num_states, num_actions, continuous=continuous, layer1=critic_layer_1, layer2=critic_layer_2\n",
    "        )\n",
    "        \n",
    "        self.target_actor = get_actor(\n",
    "            num_states, num_actions, upper_bound, continuous=continuous, layer1=actor_layer_1, layer2=actor_layer_2\n",
    "        )\n",
    "        self.target_critic = get_critic(\n",
    "            num_states, num_actions, continuous=continuous, layer1=critic_layer_1, layer2=critic_layer_2\n",
    "        )\n",
    "        \n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=actor_lr, beta_1=0.9, beta_2=0.999, epsilon=adam_actor_eps, amsgrad=actor_amsgrad,\n",
    "        )\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=critic_lr, beta_1=0.9, beta_2=0.999, epsilon=adam_critic_eps, amsgrad=critic_amsgrad,\n",
    "        )\n",
    "        # Making the weights equal initially\n",
    "        self.target_actor.set_weights(self.actor_model.get_weights())\n",
    "        self.target_critic.set_weights(self.critic_model.get_weights())\n",
    "        \n",
    "        self.ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "    \n",
    "    # Makes a record of the outputted (s,a,r,s') obervation tuple + terminal state\n",
    "    def record(self, obs_tuple):\n",
    "        # Reuse the same buffer replacing old entries\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        self.done_buffer[index] = obs_tuple[4]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "    \n",
    "    # Calculation of loss and gradients\n",
    "    @tf.function\n",
    "    def update(self, state_batch, action_batch, reward_batch, next_state_batch, done_batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor(next_state_batch, training=True)\n",
    "            # Add done_batch to y function for terminal state\n",
    "            y = reward_batch + done_batch * self.gamma * self.target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = self.critic_model([state_batch, action_batch], training=True)\n",
    "            l = losses.MeanSquaredError()\n",
    "            critic_loss = l(y, critic_value)\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
    "        \n",
    "        # Gradient clipping to avoid exploding and vanishing gradients\n",
    "        critic_gvd = zip(critic_grad, self.critic_model.trainable_variables)\n",
    "        critic_capped_grad = [(tf.clip_by_value(grad, clip_value_min=-1, clip_value_max=1), var) for grad, var in critic_gvd]\n",
    "        \n",
    "        self.critic_optimizer.apply_gradients(critic_capped_grad)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor_model(state_batch, training=True)\n",
    "            critic_value = self.critic_model([state_batch, actions], training=True)\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
    "        # clip actor too\n",
    "        actor_gvd = zip(actor_grad, self.actor_model.trainable_variables)\n",
    "        actor_capped_grad = [(tf.clip_by_value(grad, clip_value_min=-1, clip_value_max=1), var) for grad, var in actor_gvd]\n",
    "        \n",
    "        self.actor_optimizer.apply_gradients(actor_capped_grad)\n",
    "\n",
    "    def learn(self):\n",
    "        # Sample only valid data\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        done_batch = tf.convert_to_tensor(self.done_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
    "        \n",
    "    def policy(self, state, noise_object=0, use_noise=True, noise_mult=1):\n",
    "        # Default noise_object to 0 for when it is not needed\n",
    "        # For doing actions without added noise\n",
    "        if not use_noise:    \n",
    "            if self.continuous:\n",
    "                sampled_actions = tf.squeeze(self.actor_model(state)).numpy()\n",
    "                legal_action = np.clip(sampled_actions, self.lower_bound, self.upper_bound)\n",
    "                return [np.squeeze(legal_action)][0]\n",
    "            else:\n",
    "                return self.actor_model(state)\n",
    "        else:\n",
    "            if self.continuous:\n",
    "                sampled_actions = tf.squeeze(self.actor_model(state))\n",
    "                \n",
    "                noise = noise_object()\n",
    "                # Adding noise to action\n",
    "                sampled_actions = sampled_actions.numpy() + noise * noise_mult\n",
    "\n",
    "                # We make sure action is within bounds\n",
    "                legal_action = np.clip(sampled_actions, self.lower_bound, self.upper_bound)\n",
    "                return [np.squeeze(legal_action)][0]\n",
    "            else:\n",
    "                if (rng.random() < self.epsilon):\n",
    "                    #random move\n",
    "                    action = np.zeros(disc_actions_num)\n",
    "                    action[np.random.randint(0, disc_actions_num, 1)[0]] = 1\n",
    "                    return action\n",
    "                else:\n",
    "                    return self.actor_model(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d576d1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5441492862750589"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3f522c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed(x, episode):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42f9f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(total_trials=1, total_episodes=100, \n",
    "            buffer_capacity=50000, batch_size=64, std_dev=0.3, critic_lr=0.003, render=False,\n",
    "            actor_lr=0.002, gamma=0.99, tau=0.005, noise_mult=1, save_weights=True, \n",
    "            directory='Weights/', actor_name='actor', critic_name='critic',\n",
    "            gamma_func=fixed, tau_func=fixed, critic_lr_func=fixed, actor_lr_func=fixed,\n",
    "            noise_mult_func=fixed, std_dev_func=fixed, mean_number=20, output=True,\n",
    "            return_rewards=False, total_time=True, use_guide=False, solved=200,\n",
    "            continuous=True, environment='LunarLander-v2', seed=1453, start_steps=0,\n",
    "            gravity=-10.0, enable_wind=False, wind_power=15.0, turbulence_power=1.5,\n",
    "            epsilon=0.2, epsilon_func=fixed, adam_critic_eps=1e-07, adam_actor_eps=1e-07,\n",
    "            actor_amsgrad=False, critic_amsgrad=False, actor_layer_1=256, actor_layer_2=256,\n",
    "            critic_layer_1=256, critic_layer_2=256):\n",
    "    tot_time = time.time()\n",
    "    \n",
    "    if environment == 'LunarLander-v2':\n",
    "        env = gym.make(\n",
    "            \"LunarLander-v2\",\n",
    "            continuous=continuous,\n",
    "            gravity=gravity,\n",
    "            enable_wind=enable_wind,\n",
    "            wind_power=wind_power,\n",
    "            turbulence_power=turbulence_power\n",
    "        )\n",
    "    else:\n",
    "        env = gym.make(environment)\n",
    "        \n",
    "    # Apply the seed\n",
    "    _ = env.reset(seed=seed)\n",
    "        \n",
    "    # This is needed to get the input size for the NN\n",
    "    num_states = env.observation_space.low.shape[0]\n",
    "    if continuous:\n",
    "        num_actions = env.action_space.shape[0]\n",
    "    else:\n",
    "        num_actions = 1\n",
    "\n",
    "    # Normalize action space according to https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
    "    action_space = spaces.Box(low=-1, high=1, shape=(num_actions,), dtype='float32')\n",
    "\n",
    "    # This is needed to clip the actions within the legal boundaries\n",
    "    upper_bound = action_space.high[0]\n",
    "    lower_bound = action_space.low[0]\n",
    "    \n",
    "    ep_reward_list = []\n",
    "    # To store average reward history of last few episodes\n",
    "    avg_reward_list = []\n",
    "    # To separate assisted reward structures from the \"true\"\n",
    "    true_reward_list = []\n",
    "    true_avg_reward_list = []\n",
    "    \n",
    "    for trial in range(total_trials):\n",
    "        # Stepcount used for random start\n",
    "        step = 0\n",
    "\n",
    "        # Add sublists for each trial\n",
    "        avg_reward_list.append([])\n",
    "        ep_reward_list.append([])\n",
    "        true_reward_list.append([])\n",
    "        true_avg_reward_list.append([])\n",
    "        \n",
    "        agent = Agent(num_states=num_states, num_actions=num_actions, lower_bound=lower_bound, \n",
    "                upper_bound=upper_bound, continuous=continuous, buffer_capacity=buffer_capacity, \n",
    "                batch_size=batch_size, std_dev=std_dev, critic_lr=critic_lr, actor_lr=actor_lr, \n",
    "                gamma=gamma, tau=tau, epsilon=epsilon, adam_critic_eps=adam_critic_eps, adam_actor_eps=adam_actor_eps,\n",
    "                actor_amsgrad=actor_amsgrad, critic_amsgrad=critic_amsgrad, actor_layer_1=actor_layer_1, \n",
    "                actor_layer_2=actor_layer_2, critic_layer_1=critic_layer_1, critic_layer_2=critic_layer_2)\n",
    "\n",
    "        for ep in range(total_episodes):\n",
    "            # functions for different parameters\n",
    "            agent.gamma = gamma_func(gamma, ep)\n",
    "            agent.tau = tau_func(tau, ep)\n",
    "            agent.critic_lr = critic_lr_func(critic_lr, ep)\n",
    "            agent.actor_lr = actor_lr_func(actor_lr, ep)\n",
    "            agent.noise_mult = noise_mult_func(noise_mult, ep)\n",
    "            agent.std_dev = std_dev_func(std_dev, ep)\n",
    "            agent.epsilon = epsilon_func(epsilon, ep)\n",
    "            \n",
    "            # Used for time benchmarking\n",
    "            before = time.time()\n",
    "\n",
    "            prev_state = env.reset()\n",
    "            episodic_reward = 0\n",
    "            true_reward = 0\n",
    "\n",
    "            while True:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                \n",
    "                tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "                if step >= start_steps:\n",
    "                    action = agent.policy(state=tf_prev_state, noise_object=agent.ou_noise, noise_mult=noise_mult)\n",
    "                else:\n",
    "                    action = env.action_space.sample()\n",
    "                \n",
    "                step += 1\n",
    "                \n",
    "                # Recieve state and reward from environment.\n",
    "                if continuous:\n",
    "                    state, reward, done, info = env.step(action)\n",
    "                else:\n",
    "                    state, reward, done, info = env.step(np.argmax(action))\n",
    "                \n",
    "                # Add this before eventual reward modification\n",
    "                true_reward += reward\n",
    "                \n",
    "                # Reward modification\n",
    "                if use_guide:\n",
    "                    # giving penalty for straying far from flags and having high speed\n",
    "                    # x max\n",
    "#                     reward -= int(abs(state[0]) > 0.15) * 2 * abs(state[0])\n",
    "#                     # y top\n",
    "#                     reward -= int(state[1] > 1) * state[1] / 2\n",
    "#                     # horizontal speed\n",
    "#                     reward -= int(abs(state[2]) > 1) * abs(state[2])\n",
    "#                     # down speed\n",
    "#                     reward -= int(state[3] <  -1) * abs(state[3])\n",
    "#                     # up speed\n",
    "#                     reward -= int(state[3] > 0.1) * 3 * state[3]\n",
    "                    reward -= abs(state[2]/2) + abs(state[3]) + (abs(state[0])) + (abs(state[1])/2)\n",
    "\n",
    "                # Add terminal state for when it has landed. Just look at legs on the ground.\n",
    "                terminal_state = int(not done)#int(not (state[6] and state[7]))\n",
    "                agent.record((prev_state, action, reward, state, terminal_state))\n",
    "                episodic_reward += reward\n",
    "\n",
    "                agent.learn()\n",
    "                update_target(agent.target_actor.variables, agent.actor_model.variables, agent.tau)\n",
    "                update_target(agent.target_critic.variables, agent.critic_model.variables, agent.tau)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                prev_state = state\n",
    "\n",
    "            ep_reward_list[trial].append(episodic_reward)\n",
    "            true_reward_list[trial].append(true_reward)\n",
    "            \n",
    "            avg_reward = np.mean(ep_reward_list[trial][-mean_number:])\n",
    "            avg_reward_list[trial].append(avg_reward)\n",
    "            true_avg_reward = np.mean(true_reward_list[trial][-mean_number:])\n",
    "            true_avg_reward_list[trial].append(true_avg_reward)\n",
    "            \n",
    "            if output:\n",
    "                print(\"Ep {} * AvgReward {:.2f} * true AvgReward {:.2f} * Reward {:.2f} * True Reward {:.2f} * time {:.2f} * step {}\"\n",
    "                  .format(ep, avg_reward, true_avg_reward, episodic_reward, \n",
    "                          true_reward, (time.time() - before), step))\n",
    "            \n",
    "            # Stop if avg is above 'solved'\n",
    "            if true_avg_reward >= solved:\n",
    "                break\n",
    "\n",
    "        # Save weights\n",
    "        now = datetime.datetime.now()\n",
    "        timestamp = \"{}.{}.{}.{}.{}.{}\".format(now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "        save_name = \"{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}\".format(\n",
    "            environment, total_episodes, \n",
    "            buffer_capacity, batch_size, \n",
    "            std_dev, critic_lr, actor_lr, \n",
    "            gamma, tau, noise_mult, \n",
    "            gamma_func.__name__, tau_func.__name__, \n",
    "            critic_lr_func.__name__, actor_lr_func.__name__, \n",
    "            noise_mult_func.__name__, std_dev_func.__name__, \n",
    "            mean_number, use_guide, \n",
    "            solved, continuous, \n",
    "            start_steps, gravity, \n",
    "            enable_wind, wind_power,\n",
    "            turbulence_power, \n",
    "            epsilon, epsilon_func.__name__,\n",
    "            adam_critic_eps, adam_actor_eps,\n",
    "            actor_amsgrad, critic_amsgrad,\n",
    "            actor_layer_1, actor_layer_2,\n",
    "            critic_layer_1, critic_layer_2,\n",
    "            timestamp,\n",
    "        )\n",
    "        if save_weights:\n",
    "            try:\n",
    "                agent.actor_model.save_weights(directory + actor_name + '-trial' + str(trial) + '_' + save_name + '.h5')\n",
    "            except:\n",
    "                print('actor save fail')\n",
    "            try:\n",
    "                agent.critic_model.save_weights(directory + critic_name + '-trial' + str(trial) + '_' + save_name + '.h5')\n",
    "            except:\n",
    "                print('critic save fail')\n",
    "    \n",
    "    # Plotting graph\n",
    "    for idx, p in enumerate(true_avg_reward_list):\n",
    "        plt.plot(p, label=str(idx))\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True Avg. Epsiodic Reward (\" + str(mean_number) + \")\")\n",
    "    plt.legend()\n",
    "    try:\n",
    "        plt.savefig('Graphs/' + save_name + '.png')\n",
    "    except:\n",
    "        print('save fig fail')\n",
    "    plt.show()\n",
    "    \n",
    "    print('total time:', time.time() - tot_time, 's')\n",
    "    \n",
    "    # Return to be able to make graphs etc. later, or use the data for other stuff\n",
    "    if return_rewards:\n",
    "        return true_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a57bcf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(total_episodes=10, actor_weights='Weights/actor-trial0.h5', render=False,\n",
    "        environment=\"LunarLander-v2\", continuous=True, gravity=-10.0, enable_wind=False,\n",
    "        wind_power=15.0, turbulence_power=1.5, seed=1453):\n",
    "    rewards = []\n",
    "    \n",
    "    env = gym.make(\n",
    "        environment,\n",
    "        continuous=continuous,\n",
    "        gravity=gravity,\n",
    "        enable_wind=enable_wind,\n",
    "        wind_power=wind_power,\n",
    "        turbulence_power=turbulence_power\n",
    "    )\n",
    "    \n",
    "    # This is needed to get the input size for the NN\n",
    "    num_states = env.observation_space.low.shape[0]\n",
    "    if continuous:\n",
    "        num_actions = env.action_space.shape[0]\n",
    "    else:\n",
    "        num_actions = 1\n",
    "\n",
    "    # Normalize action space according to https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
    "    action_space = spaces.Box(low=-1, high=1, shape=(num_actions,), dtype='float32')\n",
    "\n",
    "    # This is needed to clip the actions within the legal boundaries\n",
    "    upper_bound = action_space.high[0]\n",
    "    lower_bound = action_space.low[0]\n",
    "    \n",
    "    # Apply the seed\n",
    "    _ = env.reset(seed=seed)\n",
    "    \n",
    "    for ep in range(total_episodes):\n",
    "        ep_reward = 0\n",
    "        \n",
    "        # Used for time benchmarking\n",
    "        before = time.time()\n",
    "        \n",
    "        prev_state = env.reset()\n",
    "        agent = Agent(num_states=num_states, num_actions=num_actions, lower_bound=lower_bound, \n",
    "                upper_bound=upper_bound, continuous=continuous, buffer_capacity=0, batch_size=0, \n",
    "                std_dev=0, critic_lr=0, actor_lr=0, gamma=0, tau=0, epsilon=0)\n",
    "        agent.actor_model.load_weights(actor_weights)\n",
    "        \n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "            action = agent.policy(state=tf_prev_state, use_noise=False)\n",
    "\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            ep_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(str(time.time() - before) + 's')\n",
    "                rewards.append(ep_reward)\n",
    "                break\n",
    "\n",
    "            prev_state = state\n",
    "            \n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a90fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random(total_episodes=10, render=False, environment=\"LunarLander-v2\", continuous=True,\n",
    "        gravity=-10.0, enable_wind=False, wind_power=15.0, turbulence_power=1.5, seed=1453):\n",
    "    \n",
    "    rewards = []\n",
    "    \n",
    "    env = gym.make(\n",
    "        environment,\n",
    "        continuous=continuous,\n",
    "        gravity=gravity,\n",
    "        enable_wind=enable_wind,\n",
    "        wind_power=wind_power,\n",
    "        turbulence_power=turbulence_power,\n",
    "    )\n",
    "    \n",
    "    # Apply the seed\n",
    "    _ = env.reset(seed=seed)\n",
    "    \n",
    "    for ep in range(total_episodes):\n",
    "        ep_reward = 0\n",
    "        \n",
    "        before = time.time()\n",
    "        \n",
    "        prev_state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = env.action_space.sample()\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(str(time.time() - before) + 's')\n",
    "                rewards.append(ep_reward)\n",
    "                break\n",
    "\n",
    "            prev_state = state\n",
    "            \n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83b8ba",
   "metadata": {},
   "source": [
    "---\n",
    "# Runs and tests\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d14ddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xax = [x for x in range(-600,250)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cf6c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decreasing_std(x, episode):\n",
    "    return max(0, min(0.2, 0.2 - (x+500)*(0.2/700)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "058e4b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decreasing_alr(x, episode):\n",
    "    return max(0, min(0.0002, 0.0002 - (x+500)*(0.0002/700)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "890d4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decreasing_clr(x, episode):\n",
    "    return max(0, min(0.0004, 0.0004 - (x+500)*(0.0004/700)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2a9bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decreasing_tau(x, episode):\n",
    "    return max(0, min(0.002, 0.002 - (x+500)*(0.002/700)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a30c827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decreasing_eps(x, episode):\n",
    "    return max(0, min(0.3, 0.3 - (x+500)*(0.3/700)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab85e9ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20a6c8e8ac0>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xax,[decreasing_std(x,1) for x in range(-600,250)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91662c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test(render=True, continuous=False, actor_weights='Weights/actor-trial0_LunarLander-v2_500_20000_64_0.3_0.0002_0.0001_0.99_0.001_1_fixed_fixed_fixed_fixed_fixed_fixed_20_False_200_False_5000_-10.0_False_15.0_1.5_0.2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f44e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 0 * AvgReward -136.86 * true AvgReward -136.86 * Reward -136.86 * True Reward -136.86 * time 1.25 * step 81\n",
      "Ep 1 * AvgReward -144.18 * true AvgReward -144.18 * Reward -151.49 * True Reward -151.49 * time 0.37 * step 157\n",
      "Ep 2 * AvgReward -144.49 * true AvgReward -144.49 * Reward -145.11 * True Reward -145.11 * time 0.34 * step 229\n",
      "Ep 3 * AvgReward -119.67 * true AvgReward -119.67 * Reward -45.21 * True Reward -45.21 * time 0.33 * step 299\n",
      "Ep 4 * AvgReward -92.41 * true AvgReward -92.41 * Reward 16.64 * True Reward 16.64 * time 0.39 * step 377\n",
      "Ep 5 * AvgReward -92.87 * true AvgReward -92.87 * Reward -95.20 * True Reward -95.20 * time 0.26 * step 433\n",
      "Ep 6 * AvgReward -100.00 * true AvgReward -100.00 * Reward -142.75 * True Reward -142.75 * time 0.40 * step 516\n",
      "Ep 7 * AvgReward -102.90 * true AvgReward -102.90 * Reward -123.19 * True Reward -123.19 * time 0.35 * step 585\n",
      "Ep 8 * AvgReward -103.94 * true AvgReward -103.94 * Reward -112.32 * True Reward -112.32 * time 0.27 * step 643\n",
      "Ep 9 * AvgReward -107.43 * true AvgReward -107.43 * Reward -138.77 * True Reward -138.77 * time 0.34 * step 713\n",
      "Ep 10 * AvgReward -108.50 * true AvgReward -108.50 * Reward -119.20 * True Reward -119.20 * time 0.42 * step 795\n",
      "Ep 11 * AvgReward -96.55 * true AvgReward -96.55 * Reward 34.85 * True Reward 34.85 * time 0.63 * step 887\n",
      "Ep 12 * AvgReward -98.46 * true AvgReward -98.46 * Reward -121.39 * True Reward -121.39 * time 0.34 * step 941\n",
      "Ep 13 * AvgReward -100.88 * true AvgReward -100.88 * Reward -132.32 * True Reward -132.32 * time 0.28 * step 992\n",
      "Ep 14 * AvgReward -154.54 * true AvgReward -154.54 * Reward -905.84 * True Reward -905.84 * time 1.04 * step 1146\n",
      "Ep 15 * AvgReward -175.52 * true AvgReward -175.52 * Reward -490.23 * True Reward -490.23 * time 0.44 * step 1214\n",
      "Ep 16 * AvgReward -189.44 * true AvgReward -189.44 * Reward -412.04 * True Reward -412.04 * time 0.59 * step 1310\n",
      "Ep 17 * AvgReward -196.96 * true AvgReward -196.96 * Reward -324.90 * True Reward -324.90 * time 0.42 * step 1373\n",
      "Ep 18 * AvgReward -208.63 * true AvgReward -208.63 * Reward -418.70 * True Reward -418.70 * time 0.34 * step 1424\n",
      "Ep 19 * AvgReward -220.53 * true AvgReward -220.53 * Reward -446.56 * True Reward -446.56 * time 0.39 * step 1483\n",
      "Ep 20 * AvgReward -222.13 * true AvgReward -222.13 * Reward -168.79 * True Reward -168.79 * time 0.38 * step 1541\n",
      "Ep 21 * AvgReward -235.48 * true AvgReward -235.48 * Reward -418.51 * True Reward -418.51 * time 0.75 * step 1658\n",
      "Ep 22 * AvgReward -253.76 * true AvgReward -253.76 * Reward -510.68 * True Reward -510.68 * time 0.71 * step 1762\n",
      "Ep 23 * AvgReward -277.32 * true AvgReward -277.32 * Reward -516.58 * True Reward -516.58 * time 1.05 * step 1922\n",
      "Ep 24 * AvgReward -296.72 * true AvgReward -296.72 * Reward -371.36 * True Reward -371.36 * time 0.84 * step 2034\n",
      "Ep 25 * AvgReward -302.99 * true AvgReward -302.99 * Reward -220.55 * True Reward -220.55 * time 0.75 * step 2113\n",
      "Ep 26 * AvgReward -316.28 * true AvgReward -316.28 * Reward -408.46 * True Reward -408.46 * time 0.39 * step 2168\n",
      "Ep 27 * AvgReward -335.24 * true AvgReward -335.24 * Reward -502.43 * True Reward -502.43 * time 0.42 * step 2226\n",
      "Ep 28 * AvgReward -367.95 * true AvgReward -367.95 * Reward -766.56 * True Reward -766.56 * time 0.73 * step 2322\n",
      "Ep 29 * AvgReward -386.22 * true AvgReward -386.22 * Reward -504.12 * True Reward -504.12 * time 0.57 * step 2385\n",
      "Ep 30 * AvgReward -400.41 * true AvgReward -400.41 * Reward -403.03 * True Reward -403.03 * time 0.54 * step 2466\n",
      "Ep 31 * AvgReward -407.24 * true AvgReward -407.24 * Reward -101.72 * True Reward -101.72 * time 0.41 * step 2525\n",
      "Ep 32 * AvgReward -402.44 * true AvgReward -402.44 * Reward -25.52 * True Reward -25.52 * time 0.42 * step 2589\n",
      "Ep 33 * AvgReward -405.69 * true AvgReward -405.69 * Reward -197.19 * True Reward -197.19 * time 0.59 * step 2679\n",
      "Ep 34 * AvgReward -366.58 * true AvgReward -366.58 * Reward -123.66 * True Reward -123.66 * time 0.37 * step 2737\n",
      "Ep 35 * AvgReward -345.02 * true AvgReward -345.02 * Reward -59.04 * True Reward -59.04 * time 0.40 * step 2800\n",
      "Ep 36 * AvgReward -330.75 * true AvgReward -330.75 * Reward -126.75 * True Reward -126.75 * time 0.44 * step 2865\n",
      "Ep 37 * AvgReward -321.59 * true AvgReward -321.59 * Reward -141.53 * True Reward -141.53 * time 0.42 * step 2929\n",
      "Ep 38 * AvgReward -304.63 * true AvgReward -304.63 * Reward -79.59 * True Reward -79.59 * time 0.65 * step 3026\n",
      "Ep 39 * AvgReward -297.31 * true AvgReward -297.31 * Reward -300.06 * True Reward -300.06 * time 0.53 * step 3106\n",
      "Ep 40 * AvgReward -290.65 * true AvgReward -290.65 * Reward -35.68 * True Reward -35.68 * time 0.53 * step 3182\n",
      "Ep 41 * AvgReward -284.84 * true AvgReward -284.84 * Reward -302.39 * True Reward -302.39 * time 0.68 * step 3285\n",
      "Ep 42 * AvgReward -274.61 * true AvgReward -274.61 * Reward -306.04 * True Reward -306.04 * time 1.79 * step 3544\n",
      "Ep 43 * AvgReward -270.27 * true AvgReward -270.27 * Reward -429.71 * True Reward -429.71 * time 1.04 * step 3698\n",
      "Ep 44 * AvgReward -274.52 * true AvgReward -274.52 * Reward -456.47 * True Reward -456.47 * time 0.74 * step 3805\n",
      "Ep 45 * AvgReward -271.02 * true AvgReward -271.02 * Reward -150.45 * True Reward -150.45 * time 0.43 * step 3870\n",
      "Ep 46 * AvgReward -259.57 * true AvgReward -259.57 * Reward -179.56 * True Reward -179.56 * time 0.61 * step 3962\n",
      "Ep 47 * AvgReward -232.65 * true AvgReward -232.65 * Reward 36.10 * True Reward 36.10 * time 1.01 * step 4104\n",
      "Ep 48 * AvgReward -207.28 * true AvgReward -207.28 * Reward -259.29 * True Reward -259.29 * time 0.67 * step 4205\n",
      "Ep 49 * AvgReward -182.95 * true AvgReward -182.95 * Reward -17.44 * True Reward -17.44 * time 0.43 * step 4267\n",
      "Ep 50 * AvgReward -167.06 * true AvgReward -167.06 * Reward -85.17 * True Reward -85.17 * time 0.45 * step 4332\n",
      "Ep 51 * AvgReward -174.21 * true AvgReward -174.21 * Reward -244.79 * True Reward -244.79 * time 0.67 * step 4430\n",
      "Ep 52 * AvgReward -183.04 * true AvgReward -183.04 * Reward -202.09 * True Reward -202.09 * time 0.59 * step 4520\n",
      "Ep 53 * AvgReward -189.52 * true AvgReward -189.52 * Reward -326.74 * True Reward -326.74 * time 0.62 * step 4610\n",
      "Ep 54 * AvgReward -183.82 * true AvgReward -183.82 * Reward -9.76 * True Reward -9.76 * time 0.50 * step 4675\n",
      "Ep 55 * AvgReward -192.50 * true AvgReward -192.50 * Reward -232.49 * True Reward -232.49 * time 0.79 * step 4772\n",
      "Ep 56 * AvgReward -188.96 * true AvgReward -188.96 * Reward -56.10 * True Reward -56.10 * time 0.48 * step 4827\n",
      "Ep 57 * AvgReward -183.95 * true AvgReward -183.95 * Reward -41.25 * True Reward -41.25 * time 0.81 * step 4935\n",
      "Ep 58 * AvgReward -181.34 * true AvgReward -181.34 * Reward -27.31 * True Reward -27.31 * time 0.92 * step 5067\n",
      "Ep 59 * AvgReward -173.29 * true AvgReward -173.29 * Reward -139.11 * True Reward -139.11 * time 1.10 * step 5224\n",
      "Ep 60 * AvgReward -184.83 * true AvgReward -184.83 * Reward -266.45 * True Reward -266.45 * time 0.79 * step 5340\n",
      "Ep 61 * AvgReward -177.66 * true AvgReward -177.66 * Reward -159.07 * True Reward -159.07 * time 0.47 * step 5406\n",
      "Ep 62 * AvgReward -176.96 * true AvgReward -176.96 * Reward -291.96 * True Reward -291.96 * time 0.79 * step 5522\n",
      "Ep 63 * AvgReward -167.41 * true AvgReward -167.41 * Reward -238.73 * True Reward -238.73 * time 0.54 * step 5601\n",
      "Ep 64 * AvgReward -159.04 * true AvgReward -159.04 * Reward -289.11 * True Reward -289.11 * time 0.87 * step 5726\n",
      "Ep 65 * AvgReward -155.12 * true AvgReward -155.12 * Reward -71.99 * True Reward -71.99 * time 1.00 * step 5870\n",
      "Ep 66 * AvgReward -149.46 * true AvgReward -149.46 * Reward -66.43 * True Reward -66.43 * time 0.49 * step 5942\n",
      "Ep 67 * AvgReward -159.88 * true AvgReward -159.88 * Reward -172.36 * True Reward -172.36 * time 1.34 * step 6112\n",
      "Ep 68 * AvgReward -149.18 * true AvgReward -149.18 * Reward -45.30 * True Reward -45.30 * time 0.60 * step 6197\n",
      "Ep 69 * AvgReward -164.89 * true AvgReward -164.89 * Reward -331.64 * True Reward -331.64 * time 0.95 * step 6328\n",
      "Ep 70 * AvgReward -177.61 * true AvgReward -177.61 * Reward -339.49 * True Reward -339.49 * time 2.89 * step 6706\n",
      "Ep 71 * AvgReward -171.93 * true AvgReward -171.93 * Reward -131.19 * True Reward -131.19 * time 2.77 * step 7073\n",
      "Ep 72 * AvgReward -166.91 * true AvgReward -166.91 * Reward -101.72 * True Reward -101.72 * time 2.18 * step 7362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 73 * AvgReward -160.90 * true AvgReward -160.90 * Reward -206.46 * True Reward -206.46 * time 1.93 * step 7630\n",
      "Ep 74 * AvgReward -165.56 * true AvgReward -165.56 * Reward -102.96 * True Reward -102.96 * time 1.27 * step 7808\n",
      "Ep 75 * AvgReward -160.25 * true AvgReward -160.25 * Reward -126.45 * True Reward -126.45 * time 2.38 * step 8131\n",
      "Ep 76 * AvgReward -167.69 * true AvgReward -167.69 * Reward -204.87 * True Reward -204.87 * time 1.46 * step 8331\n",
      "Ep 77 * AvgReward -176.50 * true AvgReward -176.50 * Reward -217.35 * True Reward -217.35 * time 1.62 * step 8550\n",
      "Ep 78 * AvgReward -183.67 * true AvgReward -183.67 * Reward -170.72 * True Reward -170.72 * time 1.16 * step 8711\n",
      "Ep 79 * AvgReward -185.28 * true AvgReward -185.28 * Reward -171.33 * True Reward -171.33 * time 1.52 * step 8922\n",
      "Ep 80 * AvgReward -205.73 * true AvgReward -205.73 * Reward -675.52 * True Reward -675.52 * time 3.41 * step 9352\n",
      "Ep 81 * AvgReward -209.98 * true AvgReward -209.98 * Reward -244.04 * True Reward -244.04 * time 5.05 * step 9982\n",
      "Ep 82 * AvgReward -205.15 * true AvgReward -205.15 * Reward -195.32 * True Reward -195.32 * time 3.54 * step 10437\n",
      "Ep 83 * AvgReward -201.86 * true AvgReward -201.86 * Reward -172.96 * True Reward -172.96 * time 2.79 * step 10812\n",
      "Ep 84 * AvgReward -199.66 * true AvgReward -199.66 * Reward -245.09 * True Reward -245.09 * time 5.85 * step 11515\n",
      "Ep 85 * AvgReward -200.99 * true AvgReward -200.99 * Reward -98.54 * True Reward -98.54 * time 2.73 * step 11872\n",
      "Ep 86 * AvgReward -208.14 * true AvgReward -208.14 * Reward -209.43 * True Reward -209.43 * time 5.42 * step 12567\n",
      "Ep 87 * AvgReward -207.15 * true AvgReward -207.15 * Reward -152.63 * True Reward -152.63 * time 3.12 * step 12958\n",
      "Ep 88 * AvgReward -216.30 * true AvgReward -216.30 * Reward -228.40 * True Reward -228.40 * time 4.16 * step 13489\n",
      "Ep 89 * AvgReward -209.87 * true AvgReward -209.87 * Reward -203.03 * True Reward -203.03 * time 5.80 * step 14211\n",
      "Ep 90 * AvgReward -199.92 * true AvgReward -199.92 * Reward -140.44 * True Reward -140.44 * time 3.88 * step 14714\n",
      "Ep 91 * AvgReward -199.64 * true AvgReward -199.64 * Reward -125.46 * True Reward -125.46 * time 5.23 * step 15377\n",
      "Ep 92 * AvgReward -203.94 * true AvgReward -203.94 * Reward -187.77 * True Reward -187.77 * time 7.31 * step 16304\n",
      "Ep 93 * AvgReward -200.45 * true AvgReward -200.45 * Reward -136.78 * True Reward -136.78 * time 8.50 * step 17304\n",
      "Ep 94 * AvgReward -207.09 * true AvgReward -207.09 * Reward -235.59 * True Reward -235.59 * time 7.93 * step 18228\n",
      "Ep 95 * AvgReward -206.22 * true AvgReward -206.22 * Reward -109.04 * True Reward -109.04 * time 8.56 * step 19228\n",
      "Ep 96 * AvgReward -201.25 * true AvgReward -201.25 * Reward -105.62 * True Reward -105.62 * time 2.60 * step 19571\n",
      "Ep 97 * AvgReward -195.42 * true AvgReward -195.42 * Reward -100.61 * True Reward -100.61 * time 8.97 * step 20571\n",
      "Ep 98 * AvgReward -189.17 * true AvgReward -189.17 * Reward -45.83 * True Reward -45.83 * time 7.84 * step 21571\n",
      "Ep 99 * AvgReward -184.11 * true AvgReward -184.11 * Reward -70.06 * True Reward -70.06 * time 8.09 * step 22571\n",
      "Ep 100 * AvgReward -164.25 * true AvgReward -164.25 * Reward -278.31 * True Reward -278.31 * time 7.31 * step 23481\n",
      "Ep 101 * AvgReward -156.42 * true AvgReward -156.42 * Reward -87.50 * True Reward -87.50 * time 8.12 * step 24481\n",
      "Ep 102 * AvgReward -150.28 * true AvgReward -150.28 * Reward -72.57 * True Reward -72.57 * time 9.05 * step 25481\n",
      "Ep 103 * AvgReward -145.31 * true AvgReward -145.31 * Reward -73.53 * True Reward -73.53 * time 2.78 * step 25849\n",
      "Ep 104 * AvgReward -136.15 * true AvgReward -136.15 * Reward -61.81 * True Reward -61.81 * time 8.78 * step 26849\n",
      "Ep 105 * AvgReward -132.91 * true AvgReward -132.91 * Reward -33.79 * True Reward -33.79 * time 8.66 * step 27849\n",
      "Ep 106 * AvgReward -127.29 * true AvgReward -127.29 * Reward -97.05 * True Reward -97.05 * time 9.07 * step 28849\n",
      "Ep 107 * AvgReward -123.92 * true AvgReward -123.92 * Reward -85.29 * True Reward -85.29 * time 8.85 * step 29849\n",
      "Ep 108 * AvgReward -117.36 * true AvgReward -117.36 * Reward -97.21 * True Reward -97.21 * time 8.95 * step 30849\n",
      "Ep 109 * AvgReward -104.76 * true AvgReward -104.76 * Reward 48.96 * True Reward 48.96 * time 8.28 * step 31849\n",
      "Ep 110 * AvgReward -101.62 * true AvgReward -101.62 * Reward -77.53 * True Reward -77.53 * time 1.33 * step 32026\n",
      "Ep 111 * AvgReward -100.76 * true AvgReward -100.76 * Reward -108.30 * True Reward -108.30 * time 3.57 * step 32482\n",
      "Ep 112 * AvgReward -96.86 * true AvgReward -96.86 * Reward -109.75 * True Reward -109.75 * time 1.35 * step 32668\n",
      "Ep 113 * AvgReward -95.77 * true AvgReward -95.77 * Reward -114.94 * True Reward -114.94 * time 1.24 * step 32836\n",
      "Ep 114 * AvgReward -91.80 * true AvgReward -91.80 * Reward -156.28 * True Reward -156.28 * time 3.14 * step 33188\n",
      "Ep 115 * AvgReward -95.80 * true AvgReward -95.80 * Reward -188.89 * True Reward -188.89 * time 3.58 * step 33639\n",
      "Ep 116 * AvgReward -95.63 * true AvgReward -95.63 * Reward -102.30 * True Reward -102.30 * time 2.05 * step 33884\n",
      "Ep 117 * AvgReward -97.12 * true AvgReward -97.12 * Reward -130.34 * True Reward -130.34 * time 1.90 * step 34062\n",
      "Ep 118 * AvgReward -102.88 * true AvgReward -102.88 * Reward -161.07 * True Reward -161.07 * time 2.54 * step 34352\n",
      "Ep 119 * AvgReward -106.29 * true AvgReward -106.29 * Reward -138.28 * True Reward -138.28 * time 1.79 * step 34576\n",
      "Ep 120 * AvgReward -97.45 * true AvgReward -97.45 * Reward -101.49 * True Reward -101.49 * time 1.20 * step 34714\n",
      "Ep 121 * AvgReward -98.29 * true AvgReward -98.29 * Reward -104.35 * True Reward -104.35 * time 1.82 * step 34927\n",
      "Ep 122 * AvgReward -100.62 * true AvgReward -100.62 * Reward -119.15 * True Reward -119.15 * time 1.37 * step 35101\n",
      "Ep 123 * AvgReward -104.47 * true AvgReward -104.47 * Reward -150.45 * True Reward -150.45 * time 1.87 * step 35347\n",
      "Ep 124 * AvgReward -107.02 * true AvgReward -107.02 * Reward -112.90 * True Reward -112.90 * time 1.62 * step 35554\n",
      "Ep 125 * AvgReward -112.33 * true AvgReward -112.33 * Reward -140.06 * True Reward -140.06 * time 1.46 * step 35741\n",
      "Ep 126 * AvgReward -112.65 * true AvgReward -112.65 * Reward -103.33 * True Reward -103.33 * time 1.51 * step 35925\n",
      "Ep 127 * AvgReward -115.86 * true AvgReward -115.86 * Reward -149.50 * True Reward -149.50 * time 2.88 * step 36251\n",
      "Ep 128 * AvgReward -119.18 * true AvgReward -119.18 * Reward -163.58 * True Reward -163.58 * time 3.01 * step 36625\n",
      "Ep 129 * AvgReward -125.98 * true AvgReward -125.98 * Reward -87.11 * True Reward -87.11 * time 1.62 * step 36837\n",
      "Ep 130 * AvgReward -127.43 * true AvgReward -127.43 * Reward -106.53 * True Reward -106.53 * time 1.25 * step 36957\n",
      "Ep 131 * AvgReward -129.49 * true AvgReward -129.49 * Reward -149.46 * True Reward -149.46 * time 2.05 * step 37186\n",
      "Ep 132 * AvgReward -130.44 * true AvgReward -130.44 * Reward -128.86 * True Reward -128.86 * time 2.39 * step 37477\n",
      "Ep 133 * AvgReward -129.46 * true AvgReward -129.46 * Reward -95.31 * True Reward -95.31 * time 1.37 * step 37658\n",
      "Ep 134 * AvgReward -131.37 * true AvgReward -131.37 * Reward -194.45 * True Reward -194.45 * time 1.16 * step 37809\n",
      "Ep 135 * AvgReward -130.54 * true AvgReward -130.54 * Reward -172.31 * True Reward -172.31 * time 1.71 * step 38038\n",
      "Ep 136 * AvgReward -130.64 * true AvgReward -130.64 * Reward -104.29 * True Reward -104.29 * time 1.02 * step 38177\n",
      "Ep 137 * AvgReward -130.44 * true AvgReward -130.44 * Reward -126.40 * True Reward -126.40 * time 1.03 * step 38314\n",
      "Ep 138 * AvgReward -129.68 * true AvgReward -129.68 * Reward -145.70 * True Reward -145.70 * time 3.02 * step 38696\n",
      "Ep 139 * AvgReward -130.56 * true AvgReward -130.56 * Reward -156.04 * True Reward -156.04 * time 1.37 * step 38880\n",
      "Ep 140 * AvgReward -132.81 * true AvgReward -132.81 * Reward -146.43 * True Reward -146.43 * time 1.05 * step 39023\n",
      "Ep 141 * AvgReward -135.07 * true AvgReward -135.07 * Reward -149.61 * True Reward -149.61 * time 1.29 * step 39191\n",
      "Ep 142 * AvgReward -135.75 * true AvgReward -135.75 * Reward -132.77 * True Reward -132.77 * time 1.20 * step 39348\n",
      "Ep 143 * AvgReward -137.73 * true AvgReward -137.73 * Reward -189.94 * True Reward -189.94 * time 1.61 * step 39567\n",
      "Ep 144 * AvgReward -145.12 * true AvgReward -145.12 * Reward -260.66 * True Reward -260.66 * time 8.22 * step 40491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 145 * AvgReward -149.28 * true AvgReward -149.28 * Reward -223.38 * True Reward -223.38 * time 5.69 * step 41189\n",
      "Ep 146 * AvgReward -152.00 * true AvgReward -152.00 * Reward -157.61 * True Reward -157.61 * time 1.71 * step 41417\n",
      "Ep 147 * AvgReward -153.18 * true AvgReward -153.18 * Reward -173.10 * True Reward -173.10 * time 3.92 * step 41908\n",
      "Ep 148 * AvgReward -150.32 * true AvgReward -150.32 * Reward -106.42 * True Reward -106.42 * time 2.37 * step 42224\n",
      "Ep 149 * AvgReward -155.20 * true AvgReward -155.20 * Reward -184.69 * True Reward -184.69 * time 4.08 * step 42715\n",
      "Ep 150 * AvgReward -158.67 * true AvgReward -158.67 * Reward -175.99 * True Reward -175.99 * time 4.29 * step 43203\n",
      "Ep 151 * AvgReward -158.82 * true AvgReward -158.82 * Reward -152.48 * True Reward -152.48 * time 2.48 * step 43532\n",
      "Ep 152 * AvgReward -145.30 * true AvgReward -145.30 * Reward 141.52 * True Reward 141.52 * time 4.13 * step 44065\n",
      "Ep 153 * AvgReward -144.82 * true AvgReward -144.82 * Reward -85.74 * True Reward -85.74 * time 4.04 * step 44586\n",
      "Ep 154 * AvgReward -141.67 * true AvgReward -141.67 * Reward -131.40 * True Reward -131.40 * time 2.32 * step 44891\n",
      "Ep 155 * AvgReward -133.38 * true AvgReward -133.38 * Reward -6.58 * True Reward -6.58 * time 8.75 * step 45891\n",
      "Ep 156 * AvgReward -137.42 * true AvgReward -137.42 * Reward -185.08 * True Reward -185.08 * time 2.16 * step 46166\n",
      "Ep 157 * AvgReward -138.25 * true AvgReward -138.25 * Reward -143.00 * True Reward -143.00 * time 1.52 * step 46351\n",
      "Ep 158 * AvgReward -137.45 * true AvgReward -137.45 * Reward -129.53 * True Reward -129.53 * time 1.41 * step 46530\n",
      "Ep 159 * AvgReward -136.29 * true AvgReward -136.29 * Reward -132.86 * True Reward -132.86 * time 2.11 * step 46810\n",
      "Ep 160 * AvgReward -139.72 * true AvgReward -139.72 * Reward -215.06 * True Reward -215.06 * time 2.22 * step 47107\n",
      "Ep 161 * AvgReward -140.54 * true AvgReward -140.54 * Reward -165.95 * True Reward -165.95 * time 1.59 * step 47320\n",
      "Ep 162 * AvgReward -139.48 * true AvgReward -139.48 * Reward -111.62 * True Reward -111.62 * time 1.01 * step 47458\n",
      "Ep 163 * AvgReward -133.05 * true AvgReward -133.05 * Reward -61.46 * True Reward -61.46 * time 4.57 * step 48049\n",
      "Ep 164 * AvgReward -131.61 * true AvgReward -131.61 * Reward -231.82 * True Reward -231.82 * time 0.63 * step 48135\n",
      "Ep 165 * AvgReward -127.23 * true AvgReward -127.23 * Reward -135.65 * True Reward -135.65 * time 1.55 * step 48344\n",
      "Ep 166 * AvgReward -125.32 * true AvgReward -125.32 * Reward -119.48 * True Reward -119.48 * time 1.36 * step 48530\n",
      "Ep 167 * AvgReward -125.20 * true AvgReward -125.20 * Reward -170.70 * True Reward -170.70 * time 1.69 * step 48756\n",
      "Ep 168 * AvgReward -124.64 * true AvgReward -124.64 * Reward -95.21 * True Reward -95.21 * time 1.23 * step 48920\n",
      "Ep 169 * AvgReward -125.41 * true AvgReward -125.41 * Reward -200.03 * True Reward -200.03 * time 1.93 * step 49175\n",
      "Ep 170 * AvgReward -125.12 * true AvgReward -125.12 * Reward -170.34 * True Reward -170.34 * time 1.83 * step 49410\n",
      "Ep 171 * AvgReward -133.04 * true AvgReward -133.04 * Reward -310.81 * True Reward -310.81 * time 2.55 * step 49742\n",
      "Ep 172 * AvgReward -152.94 * true AvgReward -152.94 * Reward -256.40 * True Reward -256.40 * time 0.72 * step 49842\n",
      "Ep 173 * AvgReward -158.18 * true AvgReward -158.18 * Reward -190.58 * True Reward -190.58 * time 2.95 * step 50217\n",
      "Ep 174 * AvgReward -163.40 * true AvgReward -163.40 * Reward -235.84 * True Reward -235.84 * time 0.72 * step 50318\n",
      "Ep 175 * AvgReward -174.65 * true AvgReward -174.65 * Reward -231.53 * True Reward -231.53 * time 0.96 * step 50441\n",
      "Ep 176 * AvgReward -174.88 * true AvgReward -174.88 * Reward -189.73 * True Reward -189.73 * time 1.84 * step 50675\n",
      "Ep 177 * AvgReward -173.57 * true AvgReward -173.57 * Reward -116.74 * True Reward -116.74 * time 1.05 * step 50813\n",
      "Ep 178 * AvgReward -175.03 * true AvgReward -175.03 * Reward -158.70 * True Reward -158.70 * time 2.75 * step 51176\n",
      "Ep 179 * AvgReward -178.94 * true AvgReward -178.94 * Reward -211.07 * True Reward -211.07 * time 4.21 * step 51689\n",
      "Ep 180 * AvgReward -176.34 * true AvgReward -176.34 * Reward -163.10 * True Reward -163.10 * time 1.68 * step 51907\n",
      "Ep 181 * AvgReward -176.38 * true AvgReward -176.38 * Reward -166.84 * True Reward -166.84 * time 1.70 * step 52122\n",
      "Ep 182 * AvgReward -180.23 * true AvgReward -180.23 * Reward -188.50 * True Reward -188.50 * time 4.62 * step 52675\n",
      "Ep 183 * AvgReward -182.09 * true AvgReward -182.09 * Reward -98.63 * True Reward -98.63 * time 1.27 * step 52826\n",
      "Ep 184 * AvgReward -178.28 * true AvgReward -178.28 * Reward -155.66 * True Reward -155.66 * time 3.10 * step 53184\n"
     ]
    }
   ],
   "source": [
    "run(total_trials=1, total_episodes=600, gamma=0.999, buffer_capacity=200000, tau=0.001, critic_lr=0.0002, \n",
    "    actor_lr=0.0001, start_steps=1000, continuous=False, epsilon_func=decreasing_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff4a83a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
