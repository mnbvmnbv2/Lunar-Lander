{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7764110b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\flatbuffers\\compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  'hamming': pil_image.HAMMING,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  'box': pil_image.BOX,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  'lanczos': pil_image.LANCZOS,\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "763b8710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.gymlibrary.ml/environments/box2d/lunar_lander/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a25ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OU Noise\n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f235a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(num_states, num_actions, upper_bound, continuous=True, layer1=400, layer2=300):\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(layer1, activation=\"relu\")(inputs)\n",
    "    out = layers.Dense(layer2, activation=\"relu\")(out)\n",
    "    \n",
    "    # Different output activation based on discrete or continous version\n",
    "    if continuous:\n",
    "        outputs = layers.Dense(num_actions, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "    else:\n",
    "        outputs = layers.Dense(num_actions, activation=\"softmax\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Multiply to fill the whole action space which should be equal around 0\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def get_critic(num_states, num_actions, layer1=400, layer2=300):\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(layer1, activation=\"relu\")(concat)\n",
    "    out = layers.Dense(layer2, activation=\"relu\")(out)\n",
    "\n",
    "    outputs = layers.Dense(num_actions)(out)\n",
    "\n",
    "    # Make it into a keras model\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "# This updates the weights in a slow manner which keeps stability\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d09a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions, lower_bound, upper_bound, continuous=True,\n",
    "            buffer_capacity=50000, batch_size=64, std_dev=0.2, critic_lr=0.002,\n",
    "            actor_lr=0.001, gamma=0.99, tau=0.005):\n",
    "        \n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # For methods\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        self.continuous = continuous\n",
    "\n",
    "        # This is used to make sure we only sample from used buffer space\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "        self.std_dev = std_dev\n",
    "        self.critic_lr = critic_lr\n",
    "        self.actor_lr = actor_lr\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.actor_model = get_actor(num_states, num_actions, upper_bound, continuous=continuous, layer1=400, layer2=300)\n",
    "        self.critic_model = get_critic(num_states, num_actions, layer1=400, layer2=300)\n",
    "\n",
    "        self.target_actor = get_actor(num_states, num_actions, upper_bound, continuous=continuous, layer1=400, layer2=300)\n",
    "        self.target_critic = get_critic(num_states, num_actions, layer1=400, layer2=300)\n",
    "        \n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_lr,beta_1=0.9,beta_2=0.999,epsilon=1e-07)\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_lr,beta_1=0.9,beta_2=0.999,epsilon=1e-07)\n",
    "        \n",
    "        # Making the weights equal initially\n",
    "        self.target_actor.set_weights(self.actor_model.get_weights())\n",
    "        self.target_critic.set_weights(self.critic_model.get_weights())\n",
    "        \n",
    "        self.ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "    \n",
    "    # Makes a record of the outputted (s,a,r,s') obervation tuple\n",
    "    def record(self, obs_tuple):\n",
    "        # Reuse the same buffer replacing old entries\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "    \n",
    "    # Move the update and learn function from buffer to Agent to \"decrease\" scope\n",
    "    @tf.function\n",
    "    def update(self, state_batch, action_batch, reward_batch, next_state_batch,):\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor(next_state_batch, training=True)\n",
    "            y = reward_batch + self.gamma * self.target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = self.critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, self.critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor_model(state_batch, training=True)\n",
    "            critic_value = self.critic_model([state_batch, actions], training=True)\n",
    "\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, self.actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Sample only valid data\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "        \n",
    "    def policy(self, state, noise_object=0, use_noise=True, noise_mult=1):\n",
    "        # Default noise_object to 0 for when it is not needed\n",
    "        # For doing actions without added noise\n",
    "        if not use_noise:     \n",
    "            sampled_actions = tf.squeeze(self.actor_model(state)).numpy()\n",
    "            legal_action = np.clip(sampled_actions, self.lower_bound, self.upper_bound)\n",
    "\n",
    "            return [np.squeeze(legal_action)]\n",
    "        else:\n",
    "            sampled_actions = tf.squeeze(self.actor_model(state))\n",
    "            noise = noise_object()\n",
    "            # Adding noise to action\n",
    "            sampled_actions = sampled_actions.numpy() + noise * noise_mult\n",
    "\n",
    "            # We make sure action is within bounds\n",
    "            legal_action = np.clip(sampled_actions, self.lower_bound, self.upper_bound)\n",
    "\n",
    "            return [np.squeeze(legal_action)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3f522c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed(x, episode):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42f9f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(total_trials=3, total_episodes=100, \n",
    "            buffer_capacity=50000, batch_size=64, std_dev=0.2, critic_lr=0.002, render=False,\n",
    "            actor_lr=0.001, gamma=0.99, tau=0.005, noise_mult=1, save_weights=False, \n",
    "            directory='Weights/', actor_name='actor', critic_name='critic',\n",
    "            gamma_func=fixed, tau_func=fixed, critic_lr_func=fixed, actor_lr_func=fixed,\n",
    "            noise_mult_func=fixed, std_dev_func=fixed, mean_number=40, output=True,\n",
    "            return_rewards=False, total_time=True, use_guide=False, solved=200,\n",
    "            continuous=True, environment='LunarLander-v2', seed=1453, start_steps=0,\n",
    "            gravity=-10.0, enable_wind=False, wind_power=15.0, turbulence_power=1.5):\n",
    "    tot_time = time.time()\n",
    "    \n",
    "    if environment == 'LunarLander-v2':\n",
    "        env = gym.make(\n",
    "            \"LunarLander-v2\",\n",
    "            continuous=continuous,\n",
    "            gravity=gravity,\n",
    "            enable_wind=enable_wind,\n",
    "            wind_power=wind_power,\n",
    "            turbulence_power=turbulence_power\n",
    "        )\n",
    "    else:\n",
    "        env = gym.make(environment)\n",
    "        \n",
    "    # Apply the seed\n",
    "    _ = env.reset(seed=seed)\n",
    "    \n",
    "    # Stepcount for random start\n",
    "    step = 0\n",
    "        \n",
    "    # This is needed to get the input size for the NN\n",
    "    num_states = env.observation_space.low.shape[0]\n",
    "    num_actions = env.action_space.shape[0]\n",
    "\n",
    "    # Normalize action space according to https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
    "    action_space = spaces.Box(low=-1, high=1, shape=(num_actions,), dtype='float32')\n",
    "\n",
    "    # This is needed to clip the actions within the legal boundaries\n",
    "    upper_bound = action_space.high[0]\n",
    "    lower_bound = action_space.low[0]\n",
    "    \n",
    "    # To store reward history of each episode\n",
    "    ep_reward_list = []\n",
    "    # To store average reward history of last few episodes\n",
    "    avg_reward_list = []\n",
    "    # To separate assisted reward structures from the \"true\"\n",
    "    true_reward_list = []\n",
    "    true_avg_reward_list = []\n",
    "    \n",
    "    for trial in range(total_trials):\n",
    "\n",
    "        # add sublists for each trial\n",
    "        avg_reward_list.append([])\n",
    "        ep_reward_list.append([])\n",
    "        \n",
    "        true_reward_list.append([])\n",
    "        true_avg_reward_list.append([])\n",
    "        \n",
    "        agent = Agent(num_states=num_states, num_actions=num_actions, lower_bound=lower_bound, \n",
    "                upper_bound=upper_bound, continuous=continuous, buffer_capacity=buffer_capacity, \n",
    "                batch_size=batch_size, std_dev=std_dev, critic_lr=critic_lr, actor_lr=actor_lr, \n",
    "                gamma=gamma, tau=tau)\n",
    "\n",
    "        for ep in range(total_episodes):\n",
    "            # functions for different parameters\n",
    "            agent.gamma = gamma_func(gamma, ep)\n",
    "            agent.tau = tau_func(tau, ep)\n",
    "            agent.critic_lr = critic_lr_func(critic_lr, ep)\n",
    "            agent.actor_lr = actor_lr_func(actor_lr, ep)\n",
    "            agent.noise_mult = noise_mult_func(noise_mult, ep)\n",
    "            agent.std_dev = std_dev_func(std_dev, ep)\n",
    "            \n",
    "            # Used for time benchmarking\n",
    "            before = time.time()\n",
    "\n",
    "            prev_state = env.reset()\n",
    "            episodic_reward = 0\n",
    "            true_reward = 0\n",
    "\n",
    "            while True:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                \n",
    "                tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "                if step >= start_steps:\n",
    "                    action = agent.policy(state=tf_prev_state, noise_object=agent.ou_noise, noise_mult=noise_mult)\n",
    "                    # To get the right format\n",
    "                    action = action[0]\n",
    "                else:\n",
    "                    action = env.action_space.sample()\n",
    "                \n",
    "                step += 1\n",
    "                \n",
    "                # Recieve state and reward from environment.\n",
    "                state, reward, done, info = env.step(action)\n",
    "                \n",
    "                # Add this before eventual reward modification\n",
    "                true_reward += reward\n",
    "                \n",
    "                # Reward modification\n",
    "                if use_guide:\n",
    "                    # giving penalty for straying far from flags and having high speed\n",
    "                    # x max\n",
    "#                     reward -= int(abs(state[0]) > 0.15) * 2 * abs(state[0])\n",
    "#                     # y top\n",
    "#                     reward -= int(state[1] > 1) * state[1] / 2\n",
    "#                     # horizontal speed\n",
    "#                     reward -= int(abs(state[2]) > 1) * abs(state[2])\n",
    "#                     # down speed\n",
    "#                     reward -= int(state[3] <  -1) * abs(state[3])\n",
    "#                     # up speed\n",
    "#                     reward -= int(state[3] > 0.1) * 3 * state[3]\n",
    "                    reward -= abs(state[2]/2) + abs(state[3]) + (abs(state[0])) + (abs(state[1])/2)\n",
    "\n",
    "                agent.record((prev_state, action, reward, state))\n",
    "                episodic_reward += reward\n",
    "\n",
    "                agent.learn()\n",
    "                update_target(agent.target_actor.variables, agent.actor_model.variables, agent.tau)\n",
    "                update_target(agent.target_critic.variables, agent.critic_model.variables, agent.tau)\n",
    "\n",
    "                # End this episode if en episode is done\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                prev_state = state\n",
    "\n",
    "            ep_reward_list[trial].append(episodic_reward)\n",
    "            \n",
    "            true_reward_list[trial].append(true_reward)\n",
    "            \n",
    "            true_avg_reward = np.mean(true_reward_list[trial][-mean_number:])\n",
    "            true_avg_reward_list[trial].append(true_avg_reward)\n",
    "\n",
    "            # Mean of last x episodes\n",
    "            avg_reward = np.mean(ep_reward_list[trial][-mean_number:])\n",
    "            if output:\n",
    "                print(\"Ep {} * AvgReward {:.2f} * true AvgReward {:.2f} * Reward {:.2f} * True Reward {:.2f} * time {:.2f} * step {}\"\n",
    "                  .format(ep, avg_reward, true_avg_reward, episodic_reward, true_reward, (time.time() - before), step))\n",
    "            avg_reward_list[trial].append(avg_reward)\n",
    "            \n",
    "            # stop if avg is solved\n",
    "            if true_avg_reward >= solved:\n",
    "                break\n",
    "\n",
    "        if save_weights:\n",
    "            agent.actor_model.save_weights(directory + actor_name + '-trial' + str(trial) + '.h5')\n",
    "            agent.critic_model.save_weights(directory + critic_name + '-trial' + str(trial) + '.h5')\n",
    "    \n",
    "    # Plotting graph\n",
    "    for idx, p in enumerate(true_avg_reward_list):\n",
    "        plt.plot(p, label=str(idx))\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True Avg. Epsiodic Reward (\" + str(mean_number) + \")\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print('total time:',time.time() - tot_time, 's')\n",
    "    \n",
    "    # Return to be able to make graphs etc. later, or use the data for other stuff\n",
    "    if return_rewards:\n",
    "        return true_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a57bcf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(total_episodes=10, actor_weights='Weights/actor-trial0.h5', render=False,\n",
    "        environment=\"LunarLander-v2\", continuous=False, gravity=-10.0, enable_wind=False,\n",
    "        wind_power=15.0, turbulence_power=1.5, seed=1453):\n",
    "    rewards = []\n",
    "    \n",
    "    env = gym.make(\n",
    "        environment,\n",
    "        continuous=continuous,\n",
    "        gravity=gravity,\n",
    "        enable_wind=enable_wind,\n",
    "        wind_power=wind_power,\n",
    "        turbulence_power=turbulence_power\n",
    "    )\n",
    "    \n",
    "    # Apply the seed\n",
    "    _ = env.reset(seed=seed)\n",
    "    \n",
    "    for ep in range(total_episodes):\n",
    "        ep_reward = 0\n",
    "        \n",
    "        # Used for time benchmarking\n",
    "        before = time.time()\n",
    "        \n",
    "        prev_state = env.reset()\n",
    "        agent = Agent(buffer_capacity=0, batch_size=0, std_dev=0, \n",
    "                critic_lr=0, actor_lr=0, gamma=0, tau=0)\n",
    "        agent.actor_model.load_weights(actor_weights)\n",
    "        \n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "            action = agent.policy(state=tf_prev_state, use_noise=False)\n",
    "            action = action[0]\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            print(state)\n",
    "            \n",
    "            ep_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(str(time.time() - before) + 's')\n",
    "                rewards.append(ep_reward)\n",
    "                break\n",
    "\n",
    "            prev_state = state\n",
    "            \n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a90fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random(total_episodes=10, render=False, environment=\"LunarLander-v2\",continuous=False,\n",
    "        gravity=-10.0, enable_wind=False, wind_power=15.0, turbulence_power=1.5, seed=1453):\n",
    "    rewards = []\n",
    "    \n",
    "    env = gym.make(\n",
    "        environment,\n",
    "        continuous=continuous,\n",
    "        gravity=gravity,\n",
    "        enable_wind=enable_wind,\n",
    "        wind_power=wind_power,\n",
    "        turbulence_power=turbulence_power,\n",
    "    )\n",
    "    \n",
    "    # Apply the seed\n",
    "    _ = env.reset(seed=seed)\n",
    "    \n",
    "    for ep in range(total_episodes):\n",
    "        ep_reward = 0\n",
    "        \n",
    "        # Used for time benchmarking\n",
    "        before = time.time()\n",
    "        \n",
    "        prev_state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = env.action_space.sample()\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(str(time.time() - before) + 's')\n",
    "                rewards.append(ep_reward)\n",
    "                break\n",
    "\n",
    "            prev_state = state\n",
    "            \n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83b8ba",
   "metadata": {},
   "source": [
    "---\n",
    "# Runs and tests\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cf6c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a(x, episode):\n",
    "    if (episode % 2 == 1):\n",
    "        return 0.2\n",
    "    else:\n",
    "        return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8b9a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b(x, episode):\n",
    "    if episode < 600:\n",
    "        return 0.015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d6d5c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run(total_trials=3, total_episodes=500, save_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e585e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run(total_trials=2, total_episodes=1000, buffer_capacity=250000, std_dev_func=a, save_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d1cbc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test(render=True, total_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfefd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 0 * AvgReward -409.58 * true AvgReward -409.58 * Reward -409.58 * True Reward -409.58 * time 1.80 * step 180\n",
      "Ep 1 * AvgReward -368.12 * true AvgReward -368.12 * Reward -326.66 * True Reward -326.66 * time 0.64 * step 293\n",
      "Ep 2 * AvgReward -288.98 * true AvgReward -288.98 * Reward -130.71 * True Reward -130.71 * time 1.32 * step 407\n",
      "Ep 3 * AvgReward -237.29 * true AvgReward -237.29 * Reward -82.22 * True Reward -82.22 * time 0.78 * step 476\n",
      "Ep 4 * AvgReward -217.98 * true AvgReward -217.98 * Reward -140.73 * True Reward -140.73 * time 2.06 * step 656\n",
      "Ep 5 * AvgReward -211.46 * true AvgReward -211.46 * Reward -178.86 * True Reward -178.86 * time 0.78 * step 727\n",
      "Ep 6 * AvgReward -192.86 * true AvgReward -192.86 * Reward -81.28 * True Reward -81.28 * time 0.92 * step 804\n",
      "Ep 7 * AvgReward -193.06 * true AvgReward -193.06 * Reward -194.46 * True Reward -194.46 * time 0.99 * step 890\n",
      "Ep 8 * AvgReward -187.38 * true AvgReward -187.38 * Reward -141.90 * True Reward -141.90 * time 1.47 * step 1015\n",
      "Ep 9 * AvgReward -178.72 * true AvgReward -178.72 * Reward -100.83 * True Reward -100.83 * time 1.17 * step 1112\n",
      "Ep 10 * AvgReward -193.97 * true AvgReward -193.97 * Reward -346.45 * True Reward -346.45 * time 1.46 * step 1238\n",
      "Ep 11 * AvgReward -187.28 * true AvgReward -187.28 * Reward -113.63 * True Reward -113.63 * time 1.42 * step 1354\n",
      "Ep 12 * AvgReward -184.58 * true AvgReward -184.58 * Reward -152.20 * True Reward -152.20 * time 1.26 * step 1455\n",
      "Ep 13 * AvgReward -198.57 * true AvgReward -198.57 * Reward -380.44 * True Reward -380.44 * time 1.33 * step 1566\n",
      "Ep 14 * AvgReward -211.37 * true AvgReward -211.37 * Reward -390.59 * True Reward -390.59 * time 1.44 * step 1692\n",
      "Ep 15 * AvgReward -205.28 * true AvgReward -205.28 * Reward -113.86 * True Reward -113.86 * time 1.81 * step 1858\n",
      "Ep 16 * AvgReward -217.20 * true AvgReward -217.20 * Reward -408.07 * True Reward -408.07 * time 1.01 * step 1944\n",
      "Ep 17 * AvgReward -219.33 * true AvgReward -219.33 * Reward -255.41 * True Reward -255.41 * time 1.05 * step 2036\n",
      "Ep 18 * AvgReward -209.11 * true AvgReward -209.11 * Reward -25.29 * True Reward -25.29 * time 1.00 * step 2120\n",
      "Ep 19 * AvgReward -206.71 * true AvgReward -206.71 * Reward -161.09 * True Reward -161.09 * time 1.18 * step 2223\n",
      "Ep 20 * AvgReward -213.07 * true AvgReward -213.07 * Reward -340.30 * True Reward -340.30 * time 1.08 * step 2320\n",
      "Ep 21 * AvgReward -216.89 * true AvgReward -216.89 * Reward -297.04 * True Reward -297.04 * time 1.09 * step 2410\n",
      "Ep 22 * AvgReward -221.54 * true AvgReward -221.54 * Reward -323.82 * True Reward -323.82 * time 1.22 * step 2511\n",
      "Ep 23 * AvgReward -225.09 * true AvgReward -225.09 * Reward -306.66 * True Reward -306.66 * time 11.89 * step 3339\n",
      "Ep 24 * AvgReward -228.28 * true AvgReward -228.28 * Reward -304.85 * True Reward -304.85 * time 0.99 * step 3425\n",
      "Ep 25 * AvgReward -228.28 * true AvgReward -228.28 * Reward -228.41 * True Reward -228.41 * time 1.36 * step 3534\n",
      "Ep 26 * AvgReward -224.26 * true AvgReward -224.26 * Reward -119.73 * True Reward -119.73 * time 1.22 * step 3632\n",
      "Ep 27 * AvgReward -220.08 * true AvgReward -220.08 * Reward -107.19 * True Reward -107.19 * time 1.27 * step 3745\n",
      "Ep 28 * AvgReward -215.99 * true AvgReward -215.99 * Reward -101.40 * True Reward -101.40 * time 1.33 * step 3868\n",
      "Ep 29 * AvgReward -222.26 * true AvgReward -222.26 * Reward -404.17 * True Reward -404.17 * time 0.96 * step 3950\n",
      "Ep 30 * AvgReward -222.36 * true AvgReward -222.36 * Reward -225.31 * True Reward -225.31 * time 1.56 * step 4079\n",
      "Ep 31 * AvgReward -222.63 * true AvgReward -222.63 * Reward -231.13 * True Reward -231.13 * time 1.31 * step 4198\n",
      "Ep 32 * AvgReward -224.11 * true AvgReward -224.11 * Reward -271.30 * True Reward -271.30 * time 1.18 * step 4299\n",
      "Ep 33 * AvgReward -220.35 * true AvgReward -220.35 * Reward -96.44 * True Reward -96.44 * time 0.88 * step 4376\n",
      "Ep 34 * AvgReward -223.61 * true AvgReward -223.61 * Reward -334.39 * True Reward -334.39 * time 0.91 * step 4465\n",
      "Ep 35 * AvgReward -228.43 * true AvgReward -228.43 * Reward -396.94 * True Reward -396.94 * time 1.07 * step 4555\n",
      "Ep 36 * AvgReward -228.33 * true AvgReward -228.33 * Reward -224.89 * True Reward -224.89 * time 1.05 * step 4640\n",
      "Ep 37 * AvgReward -226.77 * true AvgReward -226.77 * Reward -168.87 * True Reward -168.87 * time 1.76 * step 4793\n",
      "Ep 38 * AvgReward -227.72 * true AvgReward -227.72 * Reward -263.80 * True Reward -263.80 * time 1.62 * step 4922\n",
      "Ep 39 * AvgReward -227.45 * true AvgReward -227.45 * Reward -217.11 * True Reward -217.11 * time 0.89 * step 4998\n",
      "Ep 40 * AvgReward -245.18 * true AvgReward -245.18 * Reward -1118.79 * True Reward -1118.79 * time 1.64 * step 5000\n",
      "Ep 41 * AvgReward -255.70 * true AvgReward -255.70 * Reward -747.57 * True Reward -747.57 * time 0.87 * step 5000\n",
      "Ep 42 * AvgReward -287.35 * true AvgReward -287.35 * Reward -1396.66 * True Reward -1396.66 * time 1.64 * step 5000\n",
      "Ep 43 * AvgReward -330.09 * true AvgReward -330.09 * Reward -1791.89 * True Reward -1791.89 * time 1.79 * step 5000\n",
      "Ep 44 * AvgReward -353.94 * true AvgReward -353.94 * Reward -1094.69 * True Reward -1094.69 * time 1.33 * step 5000\n",
      "Ep 45 * AvgReward -372.48 * true AvgReward -372.48 * Reward -920.26 * True Reward -920.26 * time 1.23 * step 5000\n",
      "Ep 46 * AvgReward -400.97 * true AvgReward -400.97 * Reward -1221.12 * True Reward -1221.12 * time 1.63 * step 5000\n",
      "Ep 47 * AvgReward -415.51 * true AvgReward -415.51 * Reward -775.72 * True Reward -775.72 * time 0.78 * step 5000\n",
      "Ep 48 * AvgReward -432.97 * true AvgReward -432.97 * Reward -840.65 * True Reward -840.65 * time 1.05 * step 5000\n",
      "Ep 49 * AvgReward -448.98 * true AvgReward -448.98 * Reward -740.99 * True Reward -740.99 * time 1.15 * step 5000\n",
      "Ep 50 * AvgReward -457.70 * true AvgReward -457.70 * Reward -695.48 * True Reward -695.48 * time 0.82 * step 5000\n",
      "Ep 51 * AvgReward -476.96 * true AvgReward -476.96 * Reward -884.05 * True Reward -884.05 * time 1.16 * step 5000\n",
      "Ep 52 * AvgReward -518.64 * true AvgReward -518.64 * Reward -1819.10 * True Reward -1819.10 * time 2.21 * step 5000\n",
      "Ep 53 * AvgReward -529.27 * true AvgReward -529.27 * Reward -805.74 * True Reward -805.74 * time 1.15 * step 5000\n",
      "Ep 54 * AvgReward -572.58 * true AvgReward -572.58 * Reward -2123.04 * True Reward -2123.04 * time 2.39 * step 5000\n",
      "Ep 55 * AvgReward -618.52 * true AvgReward -618.52 * Reward -1951.40 * True Reward -1951.40 * time 1.93 * step 5000\n",
      "Ep 56 * AvgReward -651.43 * true AvgReward -651.43 * Reward -1724.32 * True Reward -1724.32 * time 1.84 * step 5000\n",
      "Ep 57 * AvgReward -682.09 * true AvgReward -682.09 * Reward -1482.14 * True Reward -1482.14 * time 1.69 * step 5000\n",
      "Ep 58 * AvgReward -705.89 * true AvgReward -705.89 * Reward -977.27 * True Reward -977.27 * time 1.28 * step 5000\n",
      "Ep 59 * AvgReward -723.72 * true AvgReward -723.72 * Reward -874.21 * True Reward -874.21 * time 1.19 * step 5000\n",
      "Ep 60 * AvgReward -745.72 * true AvgReward -745.72 * Reward -1220.10 * True Reward -1220.10 * time 1.45 * step 5000\n",
      "Ep 61 * AvgReward -760.51 * true AvgReward -760.51 * Reward -888.76 * True Reward -888.76 * time 1.09 * step 5000\n",
      "Ep 62 * AvgReward -781.87 * true AvgReward -781.87 * Reward -1178.22 * True Reward -1178.22 * time 1.27 * step 5000\n",
      "Ep 63 * AvgReward -815.13 * true AvgReward -815.13 * Reward -1636.99 * True Reward -1636.99 * time 1.84 * step 5000\n",
      "Ep 64 * AvgReward -828.48 * true AvgReward -828.48 * Reward -838.84 * True Reward -838.84 * time 1.16 * step 5000\n",
      "Ep 65 * AvgReward -843.97 * true AvgReward -843.97 * Reward -847.94 * True Reward -847.94 * time 1.27 * step 5000\n",
      "Ep 66 * AvgReward -863.55 * true AvgReward -863.55 * Reward -902.95 * True Reward -902.95 * time 1.22 * step 5000\n",
      "Ep 67 * AvgReward -884.24 * true AvgReward -884.24 * Reward -934.79 * True Reward -934.79 * time 1.20 * step 5000\n",
      "Ep 68 * AvgReward -912.05 * true AvgReward -912.05 * Reward -1213.82 * True Reward -1213.82 * time 1.55 * step 5000\n",
      "Ep 69 * AvgReward -922.18 * true AvgReward -922.18 * Reward -809.49 * True Reward -809.49 * time 0.96 * step 5000\n",
      "Ep 70 * AvgReward -935.76 * true AvgReward -935.76 * Reward -768.51 * True Reward -768.51 * time 0.83 * step 5000\n",
      "Ep 71 * AvgReward -948.72 * true AvgReward -948.72 * Reward -749.56 * True Reward -749.56 * time 0.88 * step 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 72 * AvgReward -962.62 * true AvgReward -962.62 * Reward -827.39 * True Reward -827.39 * time 0.93 * step 5000\n",
      "Ep 73 * AvgReward -980.07 * true AvgReward -980.07 * Reward -794.43 * True Reward -794.43 * time 1.05 * step 5000\n",
      "Ep 74 * AvgReward -991.64 * true AvgReward -991.64 * Reward -797.17 * True Reward -797.17 * time 0.98 * step 5000\n",
      "Ep 75 * AvgReward -1000.53 * true AvgReward -1000.53 * Reward -752.55 * True Reward -752.55 * time 0.83 * step 5000\n",
      "Ep 76 * AvgReward -1028.60 * true AvgReward -1028.60 * Reward -1347.48 * True Reward -1347.48 * time 1.62 * step 5000\n",
      "Ep 77 * AvgReward -1043.33 * true AvgReward -1043.33 * Reward -758.29 * True Reward -758.29 * time 0.85 * step 5000\n",
      "Ep 78 * AvgReward -1056.60 * true AvgReward -1056.60 * Reward -794.49 * True Reward -794.49 * time 0.95 * step 5000\n",
      "Ep 79 * AvgReward -1071.08 * true AvgReward -1071.08 * Reward -796.47 * True Reward -796.47 * time 1.01 * step 5000\n",
      "Ep 80 * AvgReward -1090.16 * true AvgReward -1090.16 * Reward -1881.76 * True Reward -1881.76 * time 1.81 * step 5000\n",
      "Ep 81 * AvgReward -1089.03 * true AvgReward -1089.03 * Reward -702.30 * True Reward -702.30 * time 0.86 * step 5000\n",
      "Ep 82 * AvgReward -1080.68 * true AvgReward -1080.68 * Reward -1062.92 * True Reward -1062.92 * time 2.01 * step 5000\n",
      "Ep 83 * AvgReward -1056.94 * true AvgReward -1056.94 * Reward -842.09 * True Reward -842.09 * time 1.11 * step 5000\n",
      "Ep 84 * AvgReward -1048.31 * true AvgReward -1048.31 * Reward -749.56 * True Reward -749.56 * time 1.37 * step 5000\n",
      "Ep 85 * AvgReward -1041.91 * true AvgReward -1041.91 * Reward -664.29 * True Reward -664.29 * time 1.20 * step 5000\n",
      "Ep 86 * AvgReward -1028.14 * true AvgReward -1028.14 * Reward -670.22 * True Reward -670.22 * time 0.84 * step 5000\n",
      "Ep 87 * AvgReward -1034.17 * true AvgReward -1034.17 * Reward -1017.05 * True Reward -1017.05 * time 1.94 * step 5000\n",
      "Ep 88 * AvgReward -1034.40 * true AvgReward -1034.40 * Reward -849.81 * True Reward -849.81 * time 1.38 * step 5000\n",
      "Ep 89 * AvgReward -1032.65 * true AvgReward -1032.65 * Reward -670.84 * True Reward -670.84 * time 1.07 * step 5000\n",
      "Ep 90 * AvgReward -1037.62 * true AvgReward -1037.62 * Reward -894.36 * True Reward -894.36 * time 1.20 * step 5000\n",
      "Ep 91 * AvgReward -1049.52 * true AvgReward -1049.52 * Reward -1359.97 * True Reward -1359.97 * time 2.00 * step 5000\n",
      "Ep 92 * AvgReward -1023.82 * true AvgReward -1023.82 * Reward -791.14 * True Reward -791.14 * time 1.14 * step 5000\n",
      "Ep 93 * AvgReward -1034.20 * true AvgReward -1034.20 * Reward -1221.00 * True Reward -1221.00 * time 1.55 * step 5000\n",
      "Ep 94 * AvgReward -991.25 * true AvgReward -991.25 * Reward -405.11 * True Reward -405.11 * time 2.18 * step 5000\n",
      "Ep 95 * AvgReward -945.70 * true AvgReward -945.70 * Reward -129.24 * True Reward -129.24 * time 1.58 * step 5000\n",
      "Ep 96 * AvgReward -906.04 * true AvgReward -906.04 * Reward -138.26 * True Reward -138.26 * time 1.74 * step 5000\n",
      "Ep 97 * AvgReward -888.64 * true AvgReward -888.64 * Reward -786.08 * True Reward -786.08 * time 2.21 * step 5000\n",
      "Ep 98 * AvgReward -867.32 * true AvgReward -867.32 * Reward -124.43 * True Reward -124.43 * time 1.64 * step 5000\n",
      "Ep 99 * AvgReward -846.67 * true AvgReward -846.67 * Reward -48.27 * True Reward -48.27 * time 1.27 * step 5000\n",
      "Ep 100 * AvgReward -819.44 * true AvgReward -819.44 * Reward -130.58 * True Reward -130.58 * time 1.62 * step 5000\n",
      "Ep 101 * AvgReward -802.54 * true AvgReward -802.54 * Reward -212.99 * True Reward -212.99 * time 1.67 * step 5000\n",
      "Ep 102 * AvgReward -775.54 * true AvgReward -775.54 * Reward -98.23 * True Reward -98.23 * time 1.03 * step 5000\n",
      "Ep 103 * AvgReward -739.70 * true AvgReward -739.70 * Reward -203.44 * True Reward -203.44 * time 1.59 * step 5000\n",
      "Ep 104 * AvgReward -726.04 * true AvgReward -726.04 * Reward -292.14 * True Reward -292.14 * time 1.06 * step 5000\n",
      "Ep 105 * AvgReward -710.53 * true AvgReward -710.53 * Reward -227.89 * True Reward -227.89 * time 1.02 * step 5000\n",
      "Ep 106 * AvgReward -694.10 * true AvgReward -694.10 * Reward -245.77 * True Reward -245.77 * time 1.05 * step 5000\n",
      "Ep 107 * AvgReward -676.25 * true AvgReward -676.25 * Reward -220.57 * True Reward -220.57 * time 0.78 * step 5000\n",
      "Ep 108 * AvgReward -652.14 * true AvgReward -652.14 * Reward -249.55 * True Reward -249.55 * time 0.95 * step 5000\n",
      "Ep 109 * AvgReward -637.08 * true AvgReward -637.08 * Reward -207.11 * True Reward -207.11 * time 1.27 * step 5000\n",
      "Ep 110 * AvgReward -617.13 * true AvgReward -617.13 * Reward 29.77 * True Reward 29.77 * time 1.63 * step 5000\n",
      "Ep 111 * AvgReward -604.31 * true AvgReward -604.31 * Reward -237.06 * True Reward -237.06 * time 1.05 * step 5000\n",
      "Ep 112 * AvgReward -591.98 * true AvgReward -591.98 * Reward -334.16 * True Reward -334.16 * time 1.49 * step 5000\n",
      "Ep 113 * AvgReward -577.05 * true AvgReward -577.05 * Reward -197.30 * True Reward -197.30 * time 0.86 * step 5000\n",
      "Ep 114 * AvgReward -564.72 * true AvgReward -564.72 * Reward -303.69 * True Reward -303.69 * time 0.86 * step 5000\n",
      "Ep 115 * AvgReward -551.77 * true AvgReward -551.77 * Reward -234.57 * True Reward -234.57 * time 0.88 * step 5000\n",
      "Ep 116 * AvgReward -526.60 * true AvgReward -526.60 * Reward -340.77 * True Reward -340.77 * time 1.30 * step 5000\n",
      "Ep 117 * AvgReward -521.22 * true AvgReward -521.22 * Reward -543.27 * True Reward -543.27 * time 1.12 * step 5000\n",
      "Ep 118 * AvgReward -509.78 * true AvgReward -509.78 * Reward -336.67 * True Reward -336.67 * time 1.09 * step 5000\n",
      "Ep 119 * AvgReward -490.43 * true AvgReward -490.43 * Reward -22.45 * True Reward -22.45 * time 0.98 * step 5000\n",
      "Ep 120 * AvgReward -449.16 * true AvgReward -449.16 * Reward -231.08 * True Reward -231.08 * time 1.05 * step 5000\n",
      "Ep 121 * AvgReward -439.05 * true AvgReward -439.05 * Reward -297.79 * True Reward -297.79 * time 1.02 * step 5000\n",
      "Ep 122 * AvgReward -424.45 * true AvgReward -424.45 * Reward -478.86 * True Reward -478.86 * time 1.13 * step 5000\n",
      "Ep 123 * AvgReward -418.74 * true AvgReward -418.74 * Reward -613.92 * True Reward -613.92 * time 1.16 * step 5000\n",
      "Ep 124 * AvgReward -409.43 * true AvgReward -409.43 * Reward -377.21 * True Reward -377.21 * time 0.64 * step 5000\n",
      "Ep 125 * AvgReward -403.09 * true AvgReward -403.09 * Reward -410.44 * True Reward -410.44 * time 0.90 * step 5000\n",
      "Ep 126 * AvgReward -397.36 * true AvgReward -397.36 * Reward -440.90 * True Reward -440.90 * time 1.13 * step 5000\n",
      "Ep 127 * AvgReward -385.45 * true AvgReward -385.45 * Reward -540.67 * True Reward -540.67 * time 1.17 * step 5000\n",
      "Ep 128 * AvgReward -377.52 * true AvgReward -377.52 * Reward -532.84 * True Reward -532.84 * time 1.12 * step 5000\n",
      "Ep 129 * AvgReward -374.63 * true AvgReward -374.63 * Reward -555.37 * True Reward -555.37 * time 0.78 * step 5000\n",
      "Ep 130 * AvgReward -366.61 * true AvgReward -366.61 * Reward -573.18 * True Reward -573.18 * time 1.02 * step 5000\n",
      "Ep 131 * AvgReward -345.26 * true AvgReward -345.26 * Reward -506.25 * True Reward -506.25 * time 0.87 * step 5000\n",
      "Ep 132 * AvgReward -337.89 * true AvgReward -337.89 * Reward -496.20 * True Reward -496.20 * time 1.21 * step 5000\n",
      "Ep 133 * AvgReward -309.53 * true AvgReward -309.53 * Reward -86.81 * True Reward -86.81 * time 1.05 * step 5000\n",
      "Ep 134 * AvgReward -309.85 * true AvgReward -309.85 * Reward -417.78 * True Reward -417.78 * time 0.84 * step 5000\n",
      "Ep 135 * AvgReward -316.96 * true AvgReward -316.96 * Reward -413.44 * True Reward -413.44 * time 1.14 * step 5000\n",
      "Ep 136 * AvgReward -320.38 * true AvgReward -320.38 * Reward -275.31 * True Reward -275.31 * time 1.08 * step 5000\n",
      "Ep 137 * AvgReward -307.00 * true AvgReward -307.00 * Reward -250.67 * True Reward -250.67 * time 0.76 * step 5000\n",
      "Ep 138 * AvgReward -315.39 * true AvgReward -315.39 * Reward -460.34 * True Reward -460.34 * time 0.96 * step 5000\n",
      "Ep 139 * AvgReward -327.23 * true AvgReward -327.23 * Reward -521.53 * True Reward -521.53 * time 1.06 * step 5000\n",
      "Ep 140 * AvgReward -331.60 * true AvgReward -331.60 * Reward -305.47 * True Reward -305.47 * time 0.78 * step 5000\n",
      "Ep 141 * AvgReward -339.99 * true AvgReward -339.99 * Reward -548.59 * True Reward -548.59 * time 0.92 * step 5000\n",
      "Ep 142 * AvgReward -350.49 * true AvgReward -350.49 * Reward -518.17 * True Reward -518.17 * time 1.04 * step 5000\n",
      "Ep 143 * AvgReward -355.85 * true AvgReward -355.85 * Reward -417.87 * True Reward -417.87 * time 1.04 * step 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 144 * AvgReward -364.31 * true AvgReward -364.31 * Reward -630.58 * True Reward -630.58 * time 1.14 * step 5000\n",
      "Ep 145 * AvgReward -374.66 * true AvgReward -374.66 * Reward -641.78 * True Reward -641.78 * time 1.08 * step 5000\n",
      "Ep 146 * AvgReward -383.74 * true AvgReward -383.74 * Reward -609.31 * True Reward -609.31 * time 0.90 * step 5000\n",
      "Ep 147 * AvgReward -393.87 * true AvgReward -393.87 * Reward -625.69 * True Reward -625.69 * time 1.22 * step 5000\n",
      "Ep 148 * AvgReward -401.63 * true AvgReward -401.63 * Reward -559.85 * True Reward -559.85 * time 1.06 * step 5000\n",
      "Ep 149 * AvgReward -407.17 * true AvgReward -407.17 * Reward -428.65 * True Reward -428.65 * time 1.07 * step 5000\n",
      "Ep 150 * AvgReward -408.89 * true AvgReward -408.89 * Reward -39.25 * True Reward -39.25 * time 1.13 * step 5000\n",
      "Ep 151 * AvgReward -414.59 * true AvgReward -414.59 * Reward -465.05 * True Reward -465.05 * time 1.16 * step 5000\n",
      "Ep 152 * AvgReward -425.45 * true AvgReward -425.45 * Reward -768.28 * True Reward -768.28 * time 1.37 * step 5000\n",
      "Ep 153 * AvgReward -434.45 * true AvgReward -434.45 * Reward -557.47 * True Reward -557.47 * time 0.99 * step 5000\n",
      "Ep 154 * AvgReward -445.43 * true AvgReward -445.43 * Reward -742.92 * True Reward -742.92 * time 1.42 * step 5000\n",
      "Ep 155 * AvgReward -456.39 * true AvgReward -456.39 * Reward -672.78 * True Reward -672.78 * time 1.39 * step 5000\n",
      "Ep 156 * AvgReward -460.73 * true AvgReward -460.73 * Reward -514.39 * True Reward -514.39 * time 1.28 * step 5000\n",
      "Ep 157 * AvgReward -460.57 * true AvgReward -460.57 * Reward -536.79 * True Reward -536.79 * time 1.21 * step 5000\n",
      "Ep 158 * AvgReward -457.40 * true AvgReward -457.40 * Reward -210.11 * True Reward -210.11 * time 1.40 * step 5000\n",
      "Ep 159 * AvgReward -470.95 * true AvgReward -470.95 * Reward -564.27 * True Reward -564.27 * time 1.65 * step 5000\n",
      "Ep 160 * AvgReward -467.81 * true AvgReward -467.81 * Reward -105.49 * True Reward -105.49 * time 1.09 * step 5000\n",
      "Ep 161 * AvgReward -470.02 * true AvgReward -470.02 * Reward -386.46 * True Reward -386.46 * time 1.59 * step 5000\n",
      "Ep 162 * AvgReward -472.49 * true AvgReward -472.49 * Reward -577.32 * True Reward -577.32 * time 1.38 * step 5000\n",
      "Ep 163 * AvgReward -462.17 * true AvgReward -462.17 * Reward -201.27 * True Reward -201.27 * time 2.03 * step 5000\n",
      "Ep 164 * AvgReward -462.86 * true AvgReward -462.86 * Reward -404.99 * True Reward -404.99 * time 4.09 * step 5000\n",
      "Ep 165 * AvgReward -455.94 * true AvgReward -455.94 * Reward -133.53 * True Reward -133.53 * time 1.63 * step 5000\n",
      "Ep 166 * AvgReward -448.65 * true AvgReward -448.65 * Reward -149.38 * True Reward -149.38 * time 1.78 * step 5000\n",
      "Ep 167 * AvgReward -445.24 * true AvgReward -445.24 * Reward -404.01 * True Reward -404.01 * time 1.25 * step 5000\n",
      "Ep 168 * AvgReward -441.79 * true AvgReward -441.79 * Reward -395.09 * True Reward -395.09 * time 1.23 * step 5000\n",
      "Ep 169 * AvgReward -439.26 * true AvgReward -439.26 * Reward -454.22 * True Reward -454.22 * time 1.29 * step 5000\n",
      "Ep 170 * AvgReward -439.46 * true AvgReward -439.46 * Reward -581.00 * True Reward -581.00 * time 1.68 * step 5000\n",
      "Ep 171 * AvgReward -440.69 * true AvgReward -440.69 * Reward -555.64 * True Reward -555.64 * time 2.58 * step 5000\n",
      "Ep 172 * AvgReward -435.85 * true AvgReward -435.85 * Reward -302.50 * True Reward -302.50 * time 2.85 * step 5000\n",
      "Ep 173 * AvgReward -446.21 * true AvgReward -446.21 * Reward -501.11 * True Reward -501.11 * time 1.80 * step 5000\n",
      "Ep 174 * AvgReward -452.30 * true AvgReward -452.30 * Reward -661.59 * True Reward -661.59 * time 1.63 * step 5000\n",
      "Ep 175 * AvgReward -456.78 * true AvgReward -456.78 * Reward -592.58 * True Reward -592.58 * time 1.26 * step 5000\n",
      "Ep 176 * AvgReward -461.91 * true AvgReward -461.91 * Reward -480.59 * True Reward -480.59 * time 2.42 * step 5000\n",
      "Ep 177 * AvgReward -472.59 * true AvgReward -472.59 * Reward -677.55 * True Reward -677.55 * time 3.86 * step 5000\n",
      "Ep 178 * AvgReward -475.48 * true AvgReward -475.48 * Reward -576.19 * True Reward -576.19 * time 1.23 * step 5000\n",
      "Ep 179 * AvgReward -481.39 * true AvgReward -481.39 * Reward -757.63 * True Reward -757.63 * time 3.94 * step 5000\n",
      "Ep 180 * AvgReward -487.38 * true AvgReward -487.38 * Reward -545.13 * True Reward -545.13 * time 6.03 * step 5000\n",
      "Ep 181 * AvgReward -484.42 * true AvgReward -484.42 * Reward -430.26 * True Reward -430.26 * time 1.25 * step 5000\n",
      "Ep 182 * AvgReward -488.05 * true AvgReward -488.05 * Reward -663.37 * True Reward -663.37 * time 5.21 * step 5000\n",
      "Ep 183 * AvgReward -501.25 * true AvgReward -501.25 * Reward -945.88 * True Reward -945.88 * time 2.20 * step 5000\n",
      "Ep 184 * AvgReward -497.77 * true AvgReward -497.77 * Reward -491.24 * True Reward -491.24 * time 1.42 * step 5000\n",
      "Ep 185 * AvgReward -501.98 * true AvgReward -501.98 * Reward -810.54 * True Reward -810.54 * time 1.76 * step 5000\n",
      "Ep 186 * AvgReward -491.87 * true AvgReward -491.87 * Reward -204.69 * True Reward -204.69 * time 2.70 * step 5000\n",
      "Ep 187 * AvgReward -492.34 * true AvgReward -492.34 * Reward -644.71 * True Reward -644.71 * time 1.12 * step 5000\n",
      "Ep 188 * AvgReward -490.92 * true AvgReward -490.92 * Reward -502.97 * True Reward -502.97 * time 1.00 * step 5000\n",
      "Ep 189 * AvgReward -488.35 * true AvgReward -488.35 * Reward -325.94 * True Reward -325.94 * time 1.37 * step 5000\n",
      "Ep 190 * AvgReward -505.34 * true AvgReward -505.34 * Reward -718.70 * True Reward -718.70 * time 1.33 * step 5000\n",
      "Ep 191 * AvgReward -509.78 * true AvgReward -509.78 * Reward -642.73 * True Reward -642.73 * time 1.15 * step 5000\n",
      "Ep 192 * AvgReward -507.74 * true AvgReward -507.74 * Reward -686.45 * True Reward -686.45 * time 1.11 * step 5000\n",
      "Ep 193 * AvgReward -505.42 * true AvgReward -505.42 * Reward -464.70 * True Reward -464.70 * time 1.61 * step 5000\n",
      "Ep 194 * AvgReward -498.99 * true AvgReward -498.99 * Reward -485.68 * True Reward -485.68 * time 1.38 * step 5000\n",
      "Ep 195 * AvgReward -500.14 * true AvgReward -500.14 * Reward -718.80 * True Reward -718.80 * time 1.53 * step 5000\n",
      "Ep 196 * AvgReward -496.15 * true AvgReward -496.15 * Reward -354.75 * True Reward -354.75 * time 1.39 * step 5000\n",
      "Ep 197 * AvgReward -499.44 * true AvgReward -499.44 * Reward -668.41 * True Reward -668.41 * time 2.01 * step 5000\n",
      "Ep 198 * AvgReward -501.81 * true AvgReward -501.81 * Reward -305.07 * True Reward -305.07 * time 1.55 * step 5000\n",
      "Ep 199 * AvgReward -504.23 * true AvgReward -504.23 * Reward -661.08 * True Reward -661.08 * time 1.93 * step 5000\n",
      "Ep 200 * AvgReward -512.88 * true AvgReward -512.88 * Reward -451.51 * True Reward -451.51 * time 1.88 * step 5000\n",
      "Ep 201 * AvgReward -515.72 * true AvgReward -515.72 * Reward -499.82 * True Reward -499.82 * time 1.22 * step 5000\n",
      "Ep 202 * AvgReward -514.60 * true AvgReward -514.60 * Reward -532.68 * True Reward -532.68 * time 1.05 * step 5000\n",
      "Ep 203 * AvgReward -518.02 * true AvgReward -518.02 * Reward -337.89 * True Reward -337.89 * time 1.69 * step 5000\n",
      "Ep 204 * AvgReward -520.82 * true AvgReward -520.82 * Reward -517.11 * True Reward -517.11 * time 1.50 * step 5000\n",
      "Ep 205 * AvgReward -531.97 * true AvgReward -531.97 * Reward -579.64 * True Reward -579.64 * time 1.42 * step 5000\n",
      "Ep 206 * AvgReward -541.11 * true AvgReward -541.11 * Reward -514.77 * True Reward -514.77 * time 2.19 * step 5000\n",
      "Ep 207 * AvgReward -544.57 * true AvgReward -544.57 * Reward -542.38 * True Reward -542.38 * time 1.21 * step 5000\n",
      "Ep 208 * AvgReward -551.45 * true AvgReward -551.45 * Reward -670.41 * True Reward -670.41 * time 1.07 * step 5000\n",
      "Ep 209 * AvgReward -552.95 * true AvgReward -552.95 * Reward -514.11 * True Reward -514.11 * time 1.30 * step 5000\n",
      "Ep 210 * AvgReward -550.32 * true AvgReward -550.32 * Reward -476.16 * True Reward -476.16 * time 1.15 * step 5000\n",
      "Ep 211 * AvgReward -549.53 * true AvgReward -549.53 * Reward -524.00 * True Reward -524.00 * time 1.44 * step 5000\n",
      "Ep 212 * AvgReward -552.12 * true AvgReward -552.12 * Reward -406.02 * True Reward -406.02 * time 1.13 * step 5000\n",
      "Ep 213 * AvgReward -552.81 * true AvgReward -552.81 * Reward -528.50 * True Reward -528.50 * time 0.99 * step 5000\n",
      "Ep 214 * AvgReward -553.91 * true AvgReward -553.91 * Reward -705.73 * True Reward -705.73 * time 1.19 * step 5000\n",
      "Ep 215 * AvgReward -550.72 * true AvgReward -550.72 * Reward -465.10 * True Reward -465.10 * time 1.18 * step 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 216 * AvgReward -551.43 * true AvgReward -551.43 * Reward -508.74 * True Reward -508.74 * time 1.17 * step 5000\n",
      "Ep 217 * AvgReward -552.31 * true AvgReward -552.31 * Reward -712.83 * True Reward -712.83 * time 0.96 * step 5000\n",
      "Ep 218 * AvgReward -549.79 * true AvgReward -549.79 * Reward -475.31 * True Reward -475.31 * time 1.36 * step 5000\n",
      "Ep 219 * AvgReward -546.78 * true AvgReward -546.78 * Reward -637.18 * True Reward -637.18 * time 0.97 * step 5000\n",
      "Ep 220 * AvgReward -544.83 * true AvgReward -544.83 * Reward -467.39 * True Reward -467.39 * time 1.26 * step 5000\n",
      "Ep 221 * AvgReward -550.05 * true AvgReward -550.05 * Reward -639.08 * True Reward -639.08 * time 1.14 * step 5000\n",
      "Ep 222 * AvgReward -553.16 * true AvgReward -553.16 * Reward -787.70 * True Reward -787.70 * time 1.10 * step 5000\n",
      "Ep 223 * AvgReward -548.31 * true AvgReward -548.31 * Reward -751.99 * True Reward -751.99 * time 1.21 * step 5000\n",
      "Ep 224 * AvgReward -554.31 * true AvgReward -554.31 * Reward -731.11 * True Reward -731.11 * time 1.32 * step 5000\n",
      "Ep 225 * AvgReward -546.14 * true AvgReward -546.14 * Reward -483.82 * True Reward -483.82 * time 1.17 * step 5000\n",
      "Ep 226 * AvgReward -556.64 * true AvgReward -556.64 * Reward -624.50 * True Reward -624.50 * time 1.18 * step 5000\n",
      "Ep 227 * AvgReward -555.88 * true AvgReward -555.88 * Reward -614.30 * True Reward -614.30 * time 0.86 * step 5000\n",
      "Ep 228 * AvgReward -561.49 * true AvgReward -561.49 * Reward -727.37 * True Reward -727.37 * time 0.98 * step 5000\n",
      "Ep 229 * AvgReward -569.33 * true AvgReward -569.33 * Reward -639.58 * True Reward -639.58 * time 1.29 * step 5000\n",
      "Ep 230 * AvgReward -565.93 * true AvgReward -565.93 * Reward -582.76 * True Reward -582.76 * time 1.03 * step 5000\n",
      "Ep 231 * AvgReward -564.98 * true AvgReward -564.98 * Reward -604.80 * True Reward -604.80 * time 1.28 * step 5000\n",
      "Ep 232 * AvgReward -563.26 * true AvgReward -563.26 * Reward -617.79 * True Reward -617.79 * time 1.47 * step 5000\n",
      "Ep 233 * AvgReward -567.99 * true AvgReward -567.99 * Reward -653.71 * True Reward -653.71 * time 1.92 * step 5000\n",
      "Ep 234 * AvgReward -570.52 * true AvgReward -570.52 * Reward -586.92 * True Reward -586.92 * time 1.64 * step 5000\n",
      "Ep 235 * AvgReward -567.78 * true AvgReward -567.78 * Reward -609.16 * True Reward -609.16 * time 1.59 * step 5000\n",
      "Ep 236 * AvgReward -571.00 * true AvgReward -571.00 * Reward -483.49 * True Reward -483.49 * time 1.88 * step 5000\n",
      "Ep 237 * AvgReward -568.39 * true AvgReward -568.39 * Reward -564.07 * True Reward -564.07 * time 1.48 * step 5000\n",
      "Ep 238 * AvgReward -573.51 * true AvgReward -573.51 * Reward -509.74 * True Reward -509.74 * time 1.28 * step 5000\n",
      "Ep 239 * AvgReward -574.45 * true AvgReward -574.45 * Reward -698.96 * True Reward -698.96 * time 1.40 * step 5000\n",
      "Ep 240 * AvgReward -585.10 * true AvgReward -585.10 * Reward -877.42 * True Reward -877.42 * time 1.84 * step 5000\n",
      "Ep 241 * AvgReward -588.83 * true AvgReward -588.83 * Reward -648.88 * True Reward -648.88 * time 1.13 * step 5000\n",
      "Ep 242 * AvgReward -587.42 * true AvgReward -587.42 * Reward -476.34 * True Reward -476.34 * time 1.40 * step 5000\n",
      "Ep 243 * AvgReward -593.23 * true AvgReward -593.23 * Reward -570.45 * True Reward -570.45 * time 1.31 * step 5000\n",
      "Ep 244 * AvgReward -595.10 * true AvgReward -595.10 * Reward -591.86 * True Reward -591.86 * time 2.60 * step 5000\n",
      "Ep 245 * AvgReward -619.19 * true AvgReward -619.19 * Reward -1543.21 * True Reward -1543.21 * time 3.90 * step 5000\n",
      "Ep 246 * AvgReward -640.07 * true AvgReward -640.07 * Reward -1349.82 * True Reward -1349.82 * time 3.72 * step 5000\n",
      "Ep 247 * AvgReward -661.63 * true AvgReward -661.63 * Reward -1405.04 * True Reward -1405.04 * time 3.34 * step 5000\n",
      "Ep 248 * AvgReward -670.78 * true AvgReward -670.78 * Reward -1036.26 * True Reward -1036.26 * time 1.83 * step 5000\n",
      "Ep 249 * AvgReward -685.73 * true AvgReward -685.73 * Reward -1112.00 * True Reward -1112.00 * time 1.50 * step 5000\n",
      "Ep 250 * AvgReward -692.09 * true AvgReward -692.09 * Reward -730.74 * True Reward -730.74 * time 0.93 * step 5000\n",
      "Ep 251 * AvgReward -698.97 * true AvgReward -698.97 * Reward -799.04 * True Reward -799.04 * time 1.32 * step 5000\n",
      "Ep 252 * AvgReward -712.04 * true AvgReward -712.04 * Reward -928.94 * True Reward -928.94 * time 1.18 * step 5000\n",
      "Ep 253 * AvgReward -721.59 * true AvgReward -721.59 * Reward -910.61 * True Reward -910.61 * time 1.22 * step 5000\n",
      "Ep 254 * AvgReward -731.53 * true AvgReward -731.53 * Reward -1103.08 * True Reward -1103.08 * time 1.77 * step 5000\n",
      "Ep 255 * AvgReward -734.82 * true AvgReward -734.82 * Reward -596.91 * True Reward -596.91 * time 1.32 * step 5000\n",
      "Ep 256 * AvgReward -737.94 * true AvgReward -737.94 * Reward -633.45 * True Reward -633.45 * time 1.32 * step 5000\n",
      "Ep 257 * AvgReward -777.32 * true AvgReward -777.32 * Reward -2287.96 * True Reward -2287.96 * time 2.99 * step 5000\n",
      "Ep 258 * AvgReward -785.84 * true AvgReward -785.84 * Reward -816.13 * True Reward -816.13 * time 1.03 * step 5000\n",
      "Ep 259 * AvgReward -790.08 * true AvgReward -790.08 * Reward -806.67 * True Reward -806.67 * time 1.24 * step 5000\n",
      "Ep 260 * AvgReward -809.16 * true AvgReward -809.16 * Reward -1230.91 * True Reward -1230.91 * time 1.56 * step 5000\n",
      "Ep 261 * AvgReward -825.32 * true AvgReward -825.32 * Reward -1285.24 * True Reward -1285.24 * time 1.51 * step 5000\n",
      "Ep 262 * AvgReward -827.73 * true AvgReward -827.73 * Reward -884.23 * True Reward -884.23 * time 1.24 * step 5000\n",
      "Ep 263 * AvgReward -829.49 * true AvgReward -829.49 * Reward -822.23 * True Reward -822.23 * time 1.50 * step 5000\n",
      "Ep 264 * AvgReward -844.32 * true AvgReward -844.32 * Reward -1324.46 * True Reward -1324.46 * time 1.67 * step 5000\n",
      "Ep 265 * AvgReward -859.92 * true AvgReward -859.92 * Reward -1107.95 * True Reward -1107.95 * time 1.35 * step 5000\n",
      "Ep 266 * AvgReward -861.61 * true AvgReward -861.61 * Reward -691.95 * True Reward -691.95 * time 0.89 * step 5000\n",
      "Ep 267 * AvgReward -869.56 * true AvgReward -869.56 * Reward -932.39 * True Reward -932.39 * time 1.72 * step 5000\n",
      "Ep 268 * AvgReward -864.86 * true AvgReward -864.86 * Reward -539.08 * True Reward -539.08 * time 1.18 * step 5000\n",
      "Ep 269 * AvgReward -857.25 * true AvgReward -857.25 * Reward -335.27 * True Reward -335.27 * time 1.34 * step 5000\n",
      "Ep 270 * AvgReward -928.10 * true AvgReward -928.10 * Reward -3416.72 * True Reward -3416.72 * time 5.26 * step 5000\n",
      "Ep 271 * AvgReward -937.87 * true AvgReward -937.87 * Reward -995.64 * True Reward -995.64 * time 2.15 * step 5000\n",
      "Ep 272 * AvgReward -932.75 * true AvgReward -932.75 * Reward -412.99 * True Reward -412.99 * time 1.16 * step 5000\n",
      "Ep 273 * AvgReward -930.02 * true AvgReward -930.02 * Reward -544.49 * True Reward -544.49 * time 1.09 * step 5000\n",
      "Ep 274 * AvgReward -934.51 * true AvgReward -934.51 * Reward -766.51 * True Reward -766.51 * time 1.28 * step 5000\n",
      "Ep 275 * AvgReward -962.99 * true AvgReward -962.99 * Reward -1748.63 * True Reward -1748.63 * time 3.25 * step 5000\n",
      "Ep 276 * AvgReward -976.50 * true AvgReward -976.50 * Reward -1023.64 * True Reward -1023.64 * time 1.59 * step 5000\n",
      "Ep 277 * AvgReward -978.62 * true AvgReward -978.62 * Reward -649.08 * True Reward -649.08 * time 0.71 * step 5000\n",
      "Ep 278 * AvgReward -982.01 * true AvgReward -982.01 * Reward -645.43 * True Reward -645.43 * time 1.05 * step 5000\n",
      "Ep 279 * AvgReward -976.89 * true AvgReward -976.89 * Reward -494.14 * True Reward -494.14 * time 1.04 * step 5000\n",
      "Ep 280 * AvgReward -971.58 * true AvgReward -971.58 * Reward -664.92 * True Reward -664.92 * time 0.93 * step 5000\n",
      "Ep 281 * AvgReward -986.64 * true AvgReward -986.64 * Reward -1251.09 * True Reward -1251.09 * time 1.53 * step 5000\n",
      "Ep 282 * AvgReward -995.50 * true AvgReward -995.50 * Reward -830.78 * True Reward -830.78 * time 0.94 * step 5000\n",
      "Ep 283 * AvgReward -1000.54 * true AvgReward -1000.54 * Reward -772.13 * True Reward -772.13 * time 0.77 * step 5000\n",
      "Ep 284 * AvgReward -1017.54 * true AvgReward -1017.54 * Reward -1271.83 * True Reward -1271.83 * time 1.30 * step 5000\n",
      "Ep 285 * AvgReward -997.49 * true AvgReward -997.49 * Reward -741.22 * True Reward -741.22 * time 0.75 * step 5000\n",
      "Ep 286 * AvgReward -983.45 * true AvgReward -983.45 * Reward -788.17 * True Reward -788.17 * time 0.79 * step 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 287 * AvgReward -968.10 * true AvgReward -968.10 * Reward -791.01 * True Reward -791.01 * time 0.88 * step 5000\n",
      "Ep 288 * AvgReward -962.40 * true AvgReward -962.40 * Reward -808.53 * True Reward -808.53 * time 1.25 * step 5000\n",
      "Ep 289 * AvgReward -959.04 * true AvgReward -959.04 * Reward -977.43 * True Reward -977.43 * time 1.23 * step 5000\n",
      "Ep 290 * AvgReward -963.21 * true AvgReward -963.21 * Reward -897.48 * True Reward -897.48 * time 1.11 * step 5000\n",
      "Ep 291 * AvgReward -972.95 * true AvgReward -972.95 * Reward -1188.78 * True Reward -1188.78 * time 1.38 * step 5000\n",
      "Ep 292 * AvgReward -985.30 * true AvgReward -985.30 * Reward -1422.90 * True Reward -1422.90 * time 1.69 * step 5000\n",
      "Ep 293 * AvgReward -990.12 * true AvgReward -990.12 * Reward -1103.33 * True Reward -1103.33 * time 1.25 * step 5000\n",
      "Ep 294 * AvgReward -988.56 * true AvgReward -988.56 * Reward -1040.65 * True Reward -1040.65 * time 1.36 * step 5000\n",
      "Ep 295 * AvgReward -997.47 * true AvgReward -997.47 * Reward -953.48 * True Reward -953.48 * time 1.25 * step 5000\n",
      "Ep 296 * AvgReward -1017.77 * true AvgReward -1017.77 * Reward -1445.27 * True Reward -1445.27 * time 1.81 * step 5000\n",
      "Ep 297 * AvgReward -1007.76 * true AvgReward -1007.76 * Reward -1887.65 * True Reward -1887.65 * time 2.18 * step 5000\n",
      "Ep 298 * AvgReward -1008.05 * true AvgReward -1008.05 * Reward -827.65 * True Reward -827.65 * time 1.24 * step 5000\n",
      "Ep 299 * AvgReward -1006.79 * true AvgReward -1006.79 * Reward -756.36 * True Reward -756.36 * time 0.93 * step 5000\n",
      "Ep 300 * AvgReward -1007.99 * true AvgReward -1007.99 * Reward -1278.98 * True Reward -1278.98 * time 1.12 * step 5000\n",
      "Ep 301 * AvgReward -995.30 * true AvgReward -995.30 * Reward -777.71 * True Reward -777.71 * time 0.76 * step 5000\n",
      "Ep 302 * AvgReward -998.93 * true AvgReward -998.93 * Reward -1029.34 * True Reward -1029.34 * time 1.27 * step 5000\n",
      "Ep 303 * AvgReward -998.97 * true AvgReward -998.97 * Reward -823.75 * True Reward -823.75 * time 0.94 * step 5000\n",
      "Ep 304 * AvgReward -995.01 * true AvgReward -995.01 * Reward -1166.15 * True Reward -1166.15 * time 1.18 * step 5000\n",
      "Ep 305 * AvgReward -1009.85 * true AvgReward -1009.85 * Reward -1701.39 * True Reward -1701.39 * time 1.73 * step 5000\n",
      "Ep 306 * AvgReward -1024.94 * true AvgReward -1024.94 * Reward -1295.74 * True Reward -1295.74 * time 1.46 * step 5000\n",
      "Ep 307 * AvgReward -1020.41 * true AvgReward -1020.41 * Reward -751.08 * True Reward -751.08 * time 1.04 * step 5000\n",
      "Ep 308 * AvgReward -1036.35 * true AvgReward -1036.35 * Reward -1176.57 * True Reward -1176.57 * time 1.56 * step 5000\n",
      "Ep 309 * AvgReward -1048.15 * true AvgReward -1048.15 * Reward -807.56 * True Reward -807.56 * time 0.86 * step 5000\n",
      "Ep 310 * AvgReward -999.09 * true AvgReward -999.09 * Reward -1454.19 * True Reward -1454.19 * time 1.52 * step 5000\n",
      "Ep 311 * AvgReward -998.66 * true AvgReward -998.66 * Reward -978.45 * True Reward -978.45 * time 1.09 * step 5000\n",
      "Ep 312 * AvgReward -1007.20 * true AvgReward -1007.20 * Reward -754.66 * True Reward -754.66 * time 0.69 * step 5000\n",
      "Ep 313 * AvgReward -1030.22 * true AvgReward -1030.22 * Reward -1464.96 * True Reward -1464.96 * time 1.36 * step 5000\n",
      "Ep 314 * AvgReward -1040.92 * true AvgReward -1040.92 * Reward -1194.81 * True Reward -1194.81 * time 1.50 * step 5000\n",
      "Ep 315 * AvgReward -1035.84 * true AvgReward -1035.84 * Reward -1545.43 * True Reward -1545.43 * time 1.78 * step 5000\n",
      "Ep 316 * AvgReward -1028.48 * true AvgReward -1028.48 * Reward -729.01 * True Reward -729.01 * time 0.90 * step 5000\n",
      "Ep 317 * AvgReward -1039.99 * true AvgReward -1039.99 * Reward -1109.70 * True Reward -1109.70 * time 1.52 * step 5000\n",
      "Ep 318 * AvgReward -1045.74 * true AvgReward -1045.74 * Reward -875.47 * True Reward -875.47 * time 1.16 * step 5000\n",
      "Ep 319 * AvgReward -1058.66 * true AvgReward -1058.66 * Reward -1010.91 * True Reward -1010.91 * time 1.27 * step 5000\n",
      "Ep 320 * AvgReward -1066.99 * true AvgReward -1066.99 * Reward -997.82 * True Reward -997.82 * time 1.35 * step 5000\n",
      "Ep 321 * AvgReward -1072.99 * true AvgReward -1072.99 * Reward -1491.16 * True Reward -1491.16 * time 2.02 * step 5000\n",
      "Ep 322 * AvgReward -1069.67 * true AvgReward -1069.67 * Reward -698.10 * True Reward -698.10 * time 0.97 * step 5000\n",
      "Ep 323 * AvgReward -1068.37 * true AvgReward -1068.37 * Reward -720.01 * True Reward -720.01 * time 1.04 * step 5000\n",
      "Ep 324 * AvgReward -1059.53 * true AvgReward -1059.53 * Reward -918.52 * True Reward -918.52 * time 1.23 * step 5000\n",
      "Ep 325 * AvgReward -1059.58 * true AvgReward -1059.58 * Reward -743.22 * True Reward -743.22 * time 0.84 * step 5000\n",
      "Ep 326 * AvgReward -1062.68 * true AvgReward -1062.68 * Reward -911.90 * True Reward -911.90 * time 1.38 * step 5000\n",
      "Ep 327 * AvgReward -1061.28 * true AvgReward -1061.28 * Reward -734.94 * True Reward -734.94 * time 0.86 * step 5000\n",
      "Ep 328 * AvgReward -1059.73 * true AvgReward -1059.73 * Reward -746.73 * True Reward -746.73 * time 0.90 * step 5000\n",
      "Ep 329 * AvgReward -1068.38 * true AvgReward -1068.38 * Reward -1323.44 * True Reward -1323.44 * time 1.55 * step 5000\n",
      "Ep 330 * AvgReward -1067.87 * true AvgReward -1067.87 * Reward -876.96 * True Reward -876.96 * time 1.13 * step 5000\n",
      "Ep 331 * AvgReward -1058.01 * true AvgReward -1058.01 * Reward -794.57 * True Reward -794.57 * time 0.99 * step 5000\n",
      "Ep 332 * AvgReward -1045.51 * true AvgReward -1045.51 * Reward -922.93 * True Reward -922.93 * time 1.17 * step 5000\n",
      "Ep 333 * AvgReward -1038.38 * true AvgReward -1038.38 * Reward -818.04 * True Reward -818.04 * time 1.01 * step 5000\n",
      "Ep 334 * AvgReward -1030.72 * true AvgReward -1030.72 * Reward -733.98 * True Reward -733.98 * time 0.96 * step 5000\n",
      "Ep 335 * AvgReward -1023.87 * true AvgReward -1023.87 * Reward -679.74 * True Reward -679.74 * time 1.20 * step 5000\n",
      "Ep 336 * AvgReward -1011.87 * true AvgReward -1011.87 * Reward -965.08 * True Reward -965.08 * time 1.15 * step 5000\n",
      "Ep 337 * AvgReward -1006.65 * true AvgReward -1006.65 * Reward -1678.90 * True Reward -1678.90 * time 1.92 * step 5000\n",
      "Ep 338 * AvgReward -1009.95 * true AvgReward -1009.95 * Reward -959.55 * True Reward -959.55 * time 1.27 * step 5000\n",
      "Ep 339 * AvgReward -1015.21 * true AvgReward -1015.21 * Reward -966.88 * True Reward -966.88 * time 1.33 * step 5000\n",
      "Ep 340 * AvgReward -1016.15 * true AvgReward -1016.15 * Reward -1316.73 * True Reward -1316.73 * time 1.72 * step 5000\n",
      "Ep 341 * AvgReward -1016.12 * true AvgReward -1016.12 * Reward -776.27 * True Reward -776.27 * time 1.24 * step 5000\n",
      "Ep 342 * AvgReward -1010.17 * true AvgReward -1010.17 * Reward -791.60 * True Reward -791.60 * time 0.81 * step 5000\n",
      "Ep 343 * AvgReward -1008.35 * true AvgReward -1008.35 * Reward -750.91 * True Reward -750.91 * time 0.96 * step 5000\n",
      "Ep 344 * AvgReward -1019.20 * true AvgReward -1019.20 * Reward -1600.23 * True Reward -1600.23 * time 1.92 * step 5000\n",
      "Ep 345 * AvgReward -1001.54 * true AvgReward -1001.54 * Reward -994.64 * True Reward -994.64 * time 1.43 * step 5000\n",
      "Ep 346 * AvgReward -988.30 * true AvgReward -988.30 * Reward -766.50 * True Reward -766.50 * time 1.12 * step 5000\n",
      "Ep 347 * AvgReward -990.03 * true AvgReward -990.03 * Reward -820.10 * True Reward -820.10 * time 1.07 * step 5000\n",
      "Ep 348 * AvgReward -994.51 * true AvgReward -994.51 * Reward -1355.69 * True Reward -1355.69 * time 1.99 * step 5000\n",
      "Ep 349 * AvgReward -995.88 * true AvgReward -995.88 * Reward -862.43 * True Reward -862.43 * time 1.44 * step 5000\n",
      "Ep 350 * AvgReward -977.61 * true AvgReward -977.61 * Reward -723.29 * True Reward -723.29 * time 1.44 * step 5000\n",
      "Ep 351 * AvgReward -968.46 * true AvgReward -968.46 * Reward -612.74 * True Reward -612.74 * time 1.02 * step 5000\n",
      "Ep 352 * AvgReward -965.18 * true AvgReward -965.18 * Reward -623.34 * True Reward -623.34 * time 1.06 * step 5000\n",
      "Ep 353 * AvgReward -947.56 * true AvgReward -947.56 * Reward -760.25 * True Reward -760.25 * time 1.02 * step 5000\n",
      "Ep 354 * AvgReward -939.91 * true AvgReward -939.91 * Reward -888.87 * True Reward -888.87 * time 1.28 * step 5000\n",
      "Ep 355 * AvgReward -924.22 * true AvgReward -924.22 * Reward -917.51 * True Reward -917.51 * time 1.28 * step 5000\n",
      "Ep 356 * AvgReward -927.58 * true AvgReward -927.58 * Reward -863.58 * True Reward -863.58 * time 1.17 * step 5000\n",
      "Ep 357 * AvgReward -930.92 * true AvgReward -930.92 * Reward -1243.44 * True Reward -1243.44 * time 1.52 * step 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 358 * AvgReward -941.12 * true AvgReward -941.12 * Reward -1283.19 * True Reward -1283.19 * time 1.52 * step 5000\n",
      "Ep 359 * AvgReward -958.18 * true AvgReward -958.18 * Reward -1693.22 * True Reward -1693.22 * time 2.11 * step 5000\n",
      "Ep 360 * AvgReward -955.78 * true AvgReward -955.78 * Reward -902.00 * True Reward -902.00 * time 1.15 * step 5000\n",
      "Ep 361 * AvgReward -935.94 * true AvgReward -935.94 * Reward -697.68 * True Reward -697.68 * time 0.87 * step 5000\n",
      "Ep 362 * AvgReward -942.83 * true AvgReward -942.83 * Reward -973.66 * True Reward -973.66 * time 1.46 * step 5000\n",
      "Ep 363 * AvgReward -943.53 * true AvgReward -943.53 * Reward -747.82 * True Reward -747.82 * time 1.01 * step 5000\n",
      "Ep 364 * AvgReward -946.70 * true AvgReward -946.70 * Reward -1045.61 * True Reward -1045.61 * time 1.37 * step 5000\n",
      "Ep 365 * AvgReward -955.24 * true AvgReward -955.24 * Reward -1084.74 * True Reward -1084.74 * time 1.34 * step 5000\n",
      "Ep 366 * AvgReward -962.84 * true AvgReward -962.84 * Reward -1215.77 * True Reward -1215.77 * time 1.34 * step 5000\n",
      "Ep 367 * AvgReward -966.21 * true AvgReward -966.21 * Reward -869.89 * True Reward -869.89 * time 1.02 * step 5000\n",
      "Ep 368 * AvgReward -966.83 * true AvgReward -966.83 * Reward -771.33 * True Reward -771.33 * time 0.95 * step 5000\n",
      "Ep 369 * AvgReward -971.24 * true AvgReward -971.24 * Reward -1500.05 * True Reward -1500.05 * time 1.85 * step 5000\n",
      "Ep 370 * AvgReward -975.12 * true AvgReward -975.12 * Reward -1032.23 * True Reward -1032.23 * time 1.42 * step 5000\n",
      "Ep 371 * AvgReward -983.28 * true AvgReward -983.28 * Reward -1120.85 * True Reward -1120.85 * time 1.74 * step 5000\n",
      "Ep 372 * AvgReward -978.85 * true AvgReward -978.85 * Reward -745.47 * True Reward -745.47 * time 0.77 * step 5000\n",
      "Ep 373 * AvgReward -977.35 * true AvgReward -977.35 * Reward -758.18 * True Reward -758.18 * time 0.99 * step 5000\n",
      "Ep 374 * AvgReward -976.51 * true AvgReward -976.51 * Reward -700.45 * True Reward -700.45 * time 1.10 * step 5000\n",
      "Ep 375 * AvgReward -978.50 * true AvgReward -978.50 * Reward -759.32 * True Reward -759.32 * time 0.84 * step 5000\n",
      "Ep 376 * AvgReward -991.31 * true AvgReward -991.31 * Reward -1477.34 * True Reward -1477.34 * time 1.73 * step 5000\n",
      "Ep 377 * AvgReward -987.51 * true AvgReward -987.51 * Reward -1526.98 * True Reward -1526.98 * time 1.61 * step 5000\n",
      "Ep 378 * AvgReward -980.49 * true AvgReward -980.49 * Reward -678.74 * True Reward -678.74 * time 1.32 * step 5000\n",
      "Ep 379 * AvgReward -986.95 * true AvgReward -986.95 * Reward -1225.20 * True Reward -1225.20 * time 1.48 * step 5000\n",
      "Ep 380 * AvgReward -990.96 * true AvgReward -990.96 * Reward -1477.48 * True Reward -1477.48 * time 1.78 * step 5000\n",
      "Ep 381 * AvgReward -1004.94 * true AvgReward -1004.94 * Reward -1335.31 * True Reward -1335.31 * time 1.60 * step 5000\n",
      "Ep 382 * AvgReward -1002.67 * true AvgReward -1002.67 * Reward -700.60 * True Reward -700.60 * time 1.17 * step 5000\n",
      "Ep 383 * AvgReward -1021.13 * true AvgReward -1021.13 * Reward -1489.36 * True Reward -1489.36 * time 1.78 * step 5000\n",
      "Ep 384 * AvgReward -999.70 * true AvgReward -999.70 * Reward -742.96 * True Reward -742.96 * time 0.90 * step 5000\n",
      "Ep 385 * AvgReward -1005.25 * true AvgReward -1005.25 * Reward -1216.78 * True Reward -1216.78 * time 1.77 * step 5000\n",
      "Ep 386 * AvgReward -1019.31 * true AvgReward -1019.31 * Reward -1328.93 * True Reward -1328.93 * time 1.48 * step 5000\n",
      "Ep 387 * AvgReward -1015.59 * true AvgReward -1015.59 * Reward -671.44 * True Reward -671.44 * time 1.21 * step 5000\n",
      "Ep 388 * AvgReward -1015.65 * true AvgReward -1015.65 * Reward -1357.97 * True Reward -1357.97 * time 1.56 * step 5000\n",
      "Ep 389 * AvgReward -1016.72 * true AvgReward -1016.72 * Reward -905.35 * True Reward -905.35 * time 1.21 * step 5000\n",
      "Ep 390 * AvgReward -1018.61 * true AvgReward -1018.61 * Reward -798.59 * True Reward -798.59 * time 1.04 * step 5000\n",
      "Ep 391 * AvgReward -1027.18 * true AvgReward -1027.18 * Reward -955.52 * True Reward -955.52 * time 1.44 * step 5000\n",
      "Ep 392 * AvgReward -1030.20 * true AvgReward -1030.20 * Reward -744.24 * True Reward -744.24 * time 0.99 * step 5000\n",
      "Ep 393 * AvgReward -1030.94 * true AvgReward -1030.94 * Reward -789.93 * True Reward -789.93 * time 1.00 * step 5000\n",
      "Ep 394 * AvgReward -1026.47 * true AvgReward -1026.47 * Reward -710.07 * True Reward -710.07 * time 0.91 * step 5000\n",
      "Ep 395 * AvgReward -1035.85 * true AvgReward -1035.85 * Reward -1292.80 * True Reward -1292.80 * time 1.46 * step 5000\n",
      "Ep 396 * AvgReward -1050.60 * true AvgReward -1050.60 * Reward -1453.69 * True Reward -1453.69 * time 1.74 * step 5000\n",
      "Ep 397 * AvgReward -1039.04 * true AvgReward -1039.04 * Reward -780.84 * True Reward -780.84 * time 0.97 * step 5000\n",
      "Ep 398 * AvgReward -1025.65 * true AvgReward -1025.65 * Reward -747.64 * True Reward -747.64 * time 0.89 * step 5000\n",
      "Ep 399 * AvgReward -1024.55 * true AvgReward -1024.55 * Reward -1649.19 * True Reward -1649.19 * time 2.02 * step 5000\n",
      "Ep 0 * AvgReward -742.40 * true AvgReward -742.40 * Reward -742.40 * True Reward -742.40 * time 1.75 * step 5000\n",
      "Ep 1 * AvgReward -590.52 * true AvgReward -590.52 * Reward -438.65 * True Reward -438.65 * time 1.17 * step 5000\n",
      "Ep 2 * AvgReward -456.43 * true AvgReward -456.43 * Reward -188.25 * True Reward -188.25 * time 1.87 * step 5000\n",
      "Ep 3 * AvgReward -409.44 * true AvgReward -409.44 * Reward -268.44 * True Reward -268.44 * time 1.45 * step 5000\n",
      "Ep 4 * AvgReward -410.90 * true AvgReward -410.90 * Reward -416.77 * True Reward -416.77 * time 1.29 * step 5000\n",
      "Ep 5 * AvgReward -363.08 * true AvgReward -363.08 * Reward -123.93 * True Reward -123.93 * time 1.23 * step 5000\n",
      "Ep 6 * AvgReward -332.64 * true AvgReward -332.64 * Reward -150.05 * True Reward -150.05 * time 1.02 * step 5000\n",
      "Ep 7 * AvgReward -324.20 * true AvgReward -324.20 * Reward -265.11 * True Reward -265.11 * time 1.32 * step 5000\n",
      "Ep 8 * AvgReward -299.35 * true AvgReward -299.35 * Reward -100.55 * True Reward -100.55 * time 0.78 * step 5000\n",
      "Ep 9 * AvgReward -291.24 * true AvgReward -291.24 * Reward -218.26 * True Reward -218.26 * time 0.96 * step 5000\n",
      "Ep 10 * AvgReward -289.35 * true AvgReward -289.35 * Reward -270.40 * True Reward -270.40 * time 1.31 * step 5000\n",
      "Ep 11 * AvgReward -287.47 * true AvgReward -287.47 * Reward -266.87 * True Reward -266.87 * time 1.72 * step 5000\n",
      "Ep 12 * AvgReward -282.00 * true AvgReward -282.00 * Reward -216.34 * True Reward -216.34 * time 1.55 * step 5000\n",
      "Ep 13 * AvgReward -286.82 * true AvgReward -286.82 * Reward -349.41 * True Reward -349.41 * time 2.01 * step 5000\n",
      "Ep 14 * AvgReward -276.89 * true AvgReward -276.89 * Reward -137.88 * True Reward -137.88 * time 1.46 * step 5000\n",
      "Ep 15 * AvgReward -287.91 * true AvgReward -287.91 * Reward -453.15 * True Reward -453.15 * time 1.59 * step 5000\n",
      "Ep 16 * AvgReward -292.07 * true AvgReward -292.07 * Reward -358.68 * True Reward -358.68 * time 1.22 * step 5000\n",
      "Ep 17 * AvgReward -296.41 * true AvgReward -296.41 * Reward -370.17 * True Reward -370.17 * time 1.27 * step 5000\n",
      "Ep 18 * AvgReward -314.48 * true AvgReward -314.48 * Reward -639.78 * True Reward -639.78 * time 1.16 * step 5000\n",
      "Ep 19 * AvgReward -318.99 * true AvgReward -318.99 * Reward -404.62 * True Reward -404.62 * time 1.49 * step 5000\n",
      "Ep 20 * AvgReward -321.44 * true AvgReward -321.44 * Reward -370.55 * True Reward -370.55 * time 1.63 * step 5000\n",
      "Ep 21 * AvgReward -321.23 * true AvgReward -321.23 * Reward -316.74 * True Reward -316.74 * time 1.20 * step 5000\n",
      "Ep 22 * AvgReward -317.28 * true AvgReward -317.28 * Reward -230.41 * True Reward -230.41 * time 1.24 * step 5000\n",
      "Ep 23 * AvgReward -317.89 * true AvgReward -317.89 * Reward -331.91 * True Reward -331.91 * time 1.43 * step 5000\n",
      "Ep 24 * AvgReward -312.26 * true AvgReward -312.26 * Reward -177.22 * True Reward -177.22 * time 1.30 * step 5000\n",
      "Ep 25 * AvgReward -303.77 * true AvgReward -303.77 * Reward -91.41 * True Reward -91.41 * time 1.23 * step 5000\n",
      "Ep 26 * AvgReward -305.01 * true AvgReward -305.01 * Reward -337.28 * True Reward -337.28 * time 1.09 * step 5000\n",
      "Ep 27 * AvgReward -303.17 * true AvgReward -303.17 * Reward -253.43 * True Reward -253.43 * time 1.68 * step 5000\n",
      "Ep 28 * AvgReward -299.57 * true AvgReward -299.57 * Reward -198.84 * True Reward -198.84 * time 1.23 * step 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 29 * AvgReward -290.79 * true AvgReward -290.79 * Reward -36.32 * True Reward -36.32 * time 1.92 * step 5000\n",
      "Ep 30 * AvgReward -291.24 * true AvgReward -291.24 * Reward -304.64 * True Reward -304.64 * time 1.73 * step 5000\n",
      "Ep 31 * AvgReward -288.95 * true AvgReward -288.95 * Reward -217.96 * True Reward -217.96 * time 2.59 * step 5000\n",
      "Ep 32 * AvgReward -292.21 * true AvgReward -292.21 * Reward -396.51 * True Reward -396.51 * time 1.47 * step 5000\n",
      "Ep 33 * AvgReward -291.07 * true AvgReward -291.07 * Reward -253.46 * True Reward -253.46 * time 4.45 * step 5000\n",
      "Ep 34 * AvgReward -305.32 * true AvgReward -305.32 * Reward -789.71 * True Reward -789.71 * time 3.37 * step 5000\n",
      "Ep 35 * AvgReward -308.51 * true AvgReward -308.51 * Reward -420.42 * True Reward -420.42 * time 1.20 * step 5000\n",
      "Ep 36 * AvgReward -316.44 * true AvgReward -316.44 * Reward -601.61 * True Reward -601.61 * time 1.10 * step 5000\n"
     ]
    }
   ],
   "source": [
    "run(total_trials=2, total_episodes=500, start_steps=10000, save_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3671867f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
