{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7764110b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\flatbuffers\\compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  'hamming': pil_image.HAMMING,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  'box': pil_image.BOX,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  'lanczos': pil_image.LANCZOS,\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6470ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.gymlibrary.ml/environments/box2d/lunar_lander/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f235a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OU Noise\n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n",
    "\n",
    "def get_actor(num_states, num_actions, upper_bound, continuous=True, layer1=400, layer2=300):\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(layer1, activation=\"relu\")(inputs)\n",
    "    out = layers.Dense(layer2, activation=\"relu\")(out)\n",
    "    \n",
    "    # Different output activation based on discrete or continous version\n",
    "    if continuous:\n",
    "        outputs = layers.Dense(num_actions, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "    else:\n",
    "        outputs = layers.Dense(num_actions, activation=\"softmax\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Multiply to fill the whole action space which should be equal around 0\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def get_critic(num_states, num_actions, layer1=400, layer2=300):\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(layer1, activation=\"relu\")(concat)\n",
    "    out = layers.Dense(layer2, activation=\"relu\")(out)\n",
    "\n",
    "    outputs = layers.Dense(num_actions)(out)\n",
    "\n",
    "    # Make it into a keras model\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "# This updates the weights in a slow manner which keeps stability\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d09a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions, lower_bound, upper_bound, continuous=True,\n",
    "            buffer_capacity=50000, batch_size=64, std_dev=0.2, critic_lr=0.002,\n",
    "            actor_lr=0.001, gamma=0.99, tau=0.005):\n",
    "        \n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # For methods\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        self.continuous = continuous\n",
    "\n",
    "        # This is used to make sure we only sample from used buffer space\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "        # have to change type for gradient calculation\n",
    "        self.done_buffer = np.zeros((self.buffer_capacity, 1)).astype(np.float32)\n",
    "        \n",
    "        self.std_dev = std_dev\n",
    "        self.critic_lr = critic_lr\n",
    "        self.actor_lr = actor_lr\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.actor_model = get_actor(num_states, num_actions, upper_bound, continuous=continuous, layer1=400, layer2=300)\n",
    "        self.critic_model = get_critic(num_states, num_actions, layer1=400, layer2=300)\n",
    "\n",
    "        self.target_actor = get_actor(num_states, num_actions, upper_bound, continuous=continuous, layer1=400, layer2=300)\n",
    "        self.target_critic = get_critic(num_states, num_actions, layer1=400, layer2=300)\n",
    "        \n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_lr,beta_1=0.9,beta_2=0.999,epsilon=1e-07)\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_lr,beta_1=0.9,beta_2=0.999,epsilon=1e-07)\n",
    "        \n",
    "        # Making the weights equal initially\n",
    "        self.target_actor.set_weights(self.actor_model.get_weights())\n",
    "        self.target_critic.set_weights(self.critic_model.get_weights())\n",
    "        \n",
    "        self.ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "    \n",
    "    # Makes a record of the outputted (s,a,r,s') obervation tuple + terminal state\n",
    "    def record(self, obs_tuple):\n",
    "        # Reuse the same buffer replacing old entries\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        \n",
    "        self.done_buffer[index] = obs_tuple[4]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "    \n",
    "    # Move the update and learn function from buffer to Agent to \"decrease\" scope\n",
    "    @tf.function\n",
    "    def update(self, state_batch, action_batch, reward_batch, next_state_batch, done_batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor(next_state_batch, training=True)\n",
    "            # Add done_batch to y function for terminal state\n",
    "            y = reward_batch + done_batch * self.gamma * self.target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = self.critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
    "\n",
    "        #critic_grad = tf.concat([tf.reshape(gradient, [-1]) for gradient in critic_grad], axis=0)\n",
    "        \n",
    "        # clip gradient to avoid big gradients\n",
    "        #critic_grad = tf.clip_by_value(critic_grad, clip_value_min=-1, clip_value_max=1)\n",
    "        \n",
    "        #critic_variables = tf.concat([tf.reshape(v, [-1]) for v in self.critic_model.trainable_variables], axis=0)\n",
    "        \n",
    "        self.critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, self.critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor_model(state_batch, training=True)\n",
    "            critic_value = self.critic_model([state_batch, actions], training=True)\n",
    "\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
    "        # clip actor too\n",
    "        #actor_grad = tf.concat([tf.reshape(g, [-1]) for g in actor_grad], axis=0)\n",
    "        #actor_grad = tf.clip_by_value(actor_grad, clip_value_min=-1, clip_value_max=1)\n",
    "        self.actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, self.actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Sample only valid data\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        # Add done_batch for terminal state\n",
    "        done_batch = tf.convert_to_tensor(self.done_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
    "        \n",
    "    def policy(self, state, noise_object=0, use_noise=True, noise_mult=1):\n",
    "        # Default noise_object to 0 for when it is not needed\n",
    "        # For doing actions without added noise\n",
    "        if not use_noise:     \n",
    "            sampled_actions = tf.squeeze(self.actor_model(state)).numpy()\n",
    "            legal_action = np.clip(sampled_actions, self.lower_bound, self.upper_bound)\n",
    "        else:\n",
    "            sampled_actions = tf.squeeze(self.actor_model(state))\n",
    "            noise = noise_object()\n",
    "            # Adding noise to action\n",
    "            sampled_actions = sampled_actions.numpy() + noise * noise_mult\n",
    "\n",
    "            # We make sure action is within bounds\n",
    "            legal_action = np.clip(sampled_actions, self.lower_bound, self.upper_bound)\n",
    "            \n",
    "        if self.continuous:\n",
    "            return [np.squeeze(legal_action)]\n",
    "        else:\n",
    "            clip_disc = max(0, int(np.squeeze(legal_action)))\n",
    "            clip_disc = min(3, clip_disc)\n",
    "            return clip_disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f522c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed(x, episode):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42f9f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(total_trials=1, total_episodes=100, \n",
    "            buffer_capacity=50000, batch_size=64, std_dev=0.3, critic_lr=0.003, render=False,\n",
    "            actor_lr=0.002, gamma=0.99, tau=0.005, noise_mult=1, save_weights=True, \n",
    "            directory='Weights/', actor_name='actor', critic_name='critic',\n",
    "            gamma_func=fixed, tau_func=fixed, critic_lr_func=fixed, actor_lr_func=fixed,\n",
    "            noise_mult_func=fixed, std_dev_func=fixed, mean_number=20, output=True,\n",
    "            return_rewards=False, total_time=True, use_guide=False, solved=200,\n",
    "            continuous=True, environment='LunarLander-v2', seed=1453, start_steps=0,\n",
    "            gravity=-10.0, enable_wind=False, wind_power=15.0, turbulence_power=1.5):\n",
    "    tot_time = time.time()\n",
    "    \n",
    "    if environment == 'LunarLander-v2':\n",
    "        env = gym.make(\n",
    "            \"LunarLander-v2\",\n",
    "            continuous=continuous,\n",
    "            gravity=gravity,\n",
    "            enable_wind=enable_wind,\n",
    "            wind_power=wind_power,\n",
    "            turbulence_power=turbulence_power\n",
    "        )\n",
    "    else:\n",
    "        env = gym.make(environment)\n",
    "        \n",
    "    # Apply the seed\n",
    "    _ = env.reset(seed=seed)\n",
    "        \n",
    "    # This is needed to get the input size for the NN\n",
    "    num_states = env.observation_space.low.shape[0]\n",
    "    if continuous:\n",
    "        num_actions = env.action_space.shape[0]\n",
    "    else:\n",
    "        num_actions = 1\n",
    "\n",
    "    # Normalize action space according to https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
    "    action_space = spaces.Box(low=-1, high=1, shape=(num_actions,), dtype='float32')\n",
    "\n",
    "    # This is needed to clip the actions within the legal boundaries\n",
    "    upper_bound = action_space.high[0]\n",
    "    lower_bound = action_space.low[0]\n",
    "    \n",
    "    # To store reward history of each episode\n",
    "    ep_reward_list = []\n",
    "    # To store average reward history of last few episodes\n",
    "    avg_reward_list = []\n",
    "    # To separate assisted reward structures from the \"true\"\n",
    "    true_reward_list = []\n",
    "    true_avg_reward_list = []\n",
    "    \n",
    "    for trial in range(total_trials):\n",
    "        \n",
    "        # Stepcount for random start\n",
    "        step = 0\n",
    "\n",
    "        # add sublists for each trial\n",
    "        avg_reward_list.append([])\n",
    "        ep_reward_list.append([])\n",
    "        \n",
    "        true_reward_list.append([])\n",
    "        true_avg_reward_list.append([])\n",
    "        \n",
    "        agent = Agent(num_states=num_states, num_actions=num_actions, lower_bound=lower_bound, \n",
    "                upper_bound=upper_bound, continuous=continuous, buffer_capacity=buffer_capacity, \n",
    "                batch_size=batch_size, std_dev=std_dev, critic_lr=critic_lr, actor_lr=actor_lr, \n",
    "                gamma=gamma, tau=tau)\n",
    "\n",
    "        for ep in range(total_episodes):\n",
    "            # functions for different parameters\n",
    "            agent.gamma = gamma_func(gamma, ep)\n",
    "            agent.tau = tau_func(tau, ep)\n",
    "            agent.critic_lr = critic_lr_func(critic_lr, ep)\n",
    "            agent.actor_lr = actor_lr_func(actor_lr, ep)\n",
    "            agent.noise_mult = noise_mult_func(noise_mult, ep)\n",
    "            agent.std_dev = std_dev_func(std_dev, ep)\n",
    "            \n",
    "            # Used for time benchmarking\n",
    "            before = time.time()\n",
    "\n",
    "            prev_state = env.reset()\n",
    "            episodic_reward = 0\n",
    "            true_reward = 0\n",
    "\n",
    "            while True:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                \n",
    "                tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "                if step >= start_steps:\n",
    "                    action = agent.policy(state=tf_prev_state, noise_object=agent.ou_noise, noise_mult=noise_mult)\n",
    "                    # To get the right format\n",
    "                    if continuous:\n",
    "                        action = action[0]\n",
    "                else:\n",
    "                    action = env.action_space.sample()\n",
    "                \n",
    "                step += 1\n",
    "                \n",
    "                # Recieve state and reward from environment.\n",
    "                state, reward, done, info = env.step(action)\n",
    "                \n",
    "                # Add this before eventual reward modification\n",
    "                true_reward += reward\n",
    "                \n",
    "                # Reward modification\n",
    "                if use_guide:\n",
    "                    # giving penalty for straying far from flags and having high speed\n",
    "                    # x max\n",
    "#                     reward -= int(abs(state[0]) > 0.15) * 2 * abs(state[0])\n",
    "#                     # y top\n",
    "#                     reward -= int(state[1] > 1) * state[1] / 2\n",
    "#                     # horizontal speed\n",
    "#                     reward -= int(abs(state[2]) > 1) * abs(state[2])\n",
    "#                     # down speed\n",
    "#                     reward -= int(state[3] <  -1) * abs(state[3])\n",
    "#                     # up speed\n",
    "#                     reward -= int(state[3] > 0.1) * 3 * state[3]\n",
    "                    reward -= abs(state[2]/2) + abs(state[3]) + (abs(state[0])) + (abs(state[1])/2)\n",
    "\n",
    "                # Add terminal state for when it has landed. Just look at legs on the ground\n",
    "                terminal_state = int(state[6] and state[7])\n",
    "                \n",
    "                agent.record((prev_state, action, reward, state, terminal_state))\n",
    "                episodic_reward += reward\n",
    "\n",
    "                agent.learn()\n",
    "                update_target(agent.target_actor.variables, agent.actor_model.variables, agent.tau)\n",
    "                update_target(agent.target_critic.variables, agent.critic_model.variables, agent.tau)\n",
    "\n",
    "                # End this episode if en episode is done\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                prev_state = state\n",
    "\n",
    "            ep_reward_list[trial].append(episodic_reward)\n",
    "            \n",
    "            true_reward_list[trial].append(true_reward)\n",
    "            \n",
    "            true_avg_reward = np.mean(true_reward_list[trial][-mean_number:])\n",
    "            true_avg_reward_list[trial].append(true_avg_reward)\n",
    "\n",
    "            # Mean of last x episodes\n",
    "            avg_reward = np.mean(ep_reward_list[trial][-mean_number:])\n",
    "            if output:\n",
    "                print(\"Ep {} * AvgReward {:.2f} * true AvgReward {:.2f} * Reward {:.2f} * True Reward {:.2f} * time {:.2f} * step {}\"\n",
    "                  .format(ep, avg_reward, true_avg_reward, episodic_reward, true_reward, (time.time() - before), step))\n",
    "            avg_reward_list[trial].append(avg_reward)\n",
    "            \n",
    "            # stop if avg is solved\n",
    "            if true_avg_reward >= solved:\n",
    "                break\n",
    "\n",
    "        # Save weights naming\n",
    "        now = datetime.datetime.now()\n",
    "        timestamp = \"{}.{}.{}.{}.{}.{}\".format(now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "        save_name = \"{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}\".format(\n",
    "            environment, total_episodes, \n",
    "            buffer_capacity, batch_size, \n",
    "            std_dev, critic_lr, actor_lr, \n",
    "            gamma, tau, noise_mult, \n",
    "            gamma_func.__name__, tau_func.__name__, \n",
    "            critic_lr_func.__name__, actor_lr_func.__name__, \n",
    "            noise_mult_func.__name__, std_dev_func.__name__, \n",
    "            mean_number, use_guide, \n",
    "            solved, continuous, \n",
    "            start_steps, gravity, \n",
    "            enable_wind, wind_power, \n",
    "            turbulence_power, timestamp\n",
    "        )\n",
    "        if save_weights:\n",
    "            try:\n",
    "                agent.actor_model.save_weights(directory + actor_name + '-trial' + str(trial) + '_' + save_name + '.h5')\n",
    "            except:\n",
    "                print('actor save fail')\n",
    "            try:\n",
    "                agent.critic_model.save_weights(directory + critic_name + '-trial' + str(trial) + '_' + save_name + '.h5')\n",
    "            except:\n",
    "                print('critic save fail')\n",
    "    \n",
    "    # Plotting graph\n",
    "    for idx, p in enumerate(true_avg_reward_list):\n",
    "        plt.plot(p, label=str(idx))\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True Avg. Epsiodic Reward (\" + str(mean_number) + \")\")\n",
    "    plt.legend()\n",
    "    plt.savefig('Graphs/' + save_name + '.png')\n",
    "    plt.show()\n",
    "    \n",
    "    print('total time:',time.time() - tot_time, 's')\n",
    "    \n",
    "    # Return to be able to make graphs etc. later, or use the data for other stuff\n",
    "    if return_rewards:\n",
    "        return true_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a57bcf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(total_episodes=10, actor_weights='Weights/actor-trial0.h5', render=False,\n",
    "        environment=\"LunarLander-v2\", continuous=True, gravity=-10.0, enable_wind=False,\n",
    "        wind_power=15.0, turbulence_power=1.5, seed=1453):\n",
    "    rewards = []\n",
    "    \n",
    "    env = gym.make(\n",
    "        environment,\n",
    "        continuous=continuous,\n",
    "        gravity=gravity,\n",
    "        enable_wind=enable_wind,\n",
    "        wind_power=wind_power,\n",
    "        turbulence_power=turbulence_power\n",
    "    )\n",
    "    \n",
    "        # This is needed to get the input size for the NN\n",
    "    num_states = env.observation_space.low.shape[0]\n",
    "    if continuous:\n",
    "        num_actions = env.action_space.shape[0]\n",
    "    else:\n",
    "        num_actions = 1\n",
    "\n",
    "    # Normalize action space according to https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
    "    action_space = spaces.Box(low=-1, high=1, shape=(num_actions,), dtype='float32')\n",
    "\n",
    "    # This is needed to clip the actions within the legal boundaries\n",
    "    upper_bound = action_space.high[0]\n",
    "    lower_bound = action_space.low[0]\n",
    "    \n",
    "    # Apply the seed\n",
    "    _ = env.reset(seed=seed)\n",
    "    \n",
    "    for ep in range(total_episodes):\n",
    "        ep_reward = 0\n",
    "        \n",
    "        # Used for time benchmarking\n",
    "        before = time.time()\n",
    "        \n",
    "        prev_state = env.reset()\n",
    "        agent = Agent(num_states=num_states, num_actions=num_actions, lower_bound=lower_bound, \n",
    "                upper_bound=upper_bound, continuous=continuous, buffer_capacity=0, batch_size=0, \n",
    "                std_dev=0, critic_lr=0, actor_lr=0, gamma=0, tau=0)\n",
    "        agent.actor_model.load_weights(actor_weights)\n",
    "        \n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "            action = agent.policy(state=tf_prev_state, use_noise=False)\n",
    "            if continuous:\n",
    "                action = action[0]\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            ep_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(str(time.time() - before) + 's')\n",
    "                rewards.append(ep_reward)\n",
    "                break\n",
    "\n",
    "            prev_state = state\n",
    "            \n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a90fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random(total_episodes=10, render=False, environment=\"LunarLander-v2\", continuous=True,\n",
    "        gravity=-10.0, enable_wind=False, wind_power=15.0, turbulence_power=1.5, seed=1453):\n",
    "    rewards = []\n",
    "    \n",
    "    env = gym.make(\n",
    "        environment,\n",
    "        continuous=continuous,\n",
    "        gravity=gravity,\n",
    "        enable_wind=enable_wind,\n",
    "        wind_power=wind_power,\n",
    "        turbulence_power=turbulence_power,\n",
    "    )\n",
    "    \n",
    "    # Apply the seed\n",
    "    _ = env.reset(seed=seed)\n",
    "    \n",
    "    for ep in range(total_episodes):\n",
    "        ep_reward = 0\n",
    "        \n",
    "        # Used for time benchmarking\n",
    "        before = time.time()\n",
    "        \n",
    "        prev_state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = env.action_space.sample()\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(str(time.time() - before) + 's')\n",
    "                rewards.append(ep_reward)\n",
    "                break\n",
    "\n",
    "            prev_state = state\n",
    "            \n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83b8ba",
   "metadata": {},
   "source": [
    "---\n",
    "# Runs and tests\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d14ddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xax = [x for x in range(-600,200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cf6c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decreasing_std(x, episode):\n",
    "    return x/(1+(episode/500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "058e4b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decreasing_lr(x, episode):\n",
    "    return x/(1+(episode/500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "890d4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decreasing_std(x, episode):\n",
    "    return abs(min(1.5,(0.7/(0.005*max(x+400,1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1c80631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n"
     ]
    }
   ],
   "source": [
    "print(decreasing_std(-900,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab85e9ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25e860c8ca0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xax,[decreasing_std(x,1) for x in range(-600,200)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adcbbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 0 * AvgReward -511.91 * true AvgReward -511.91 * Reward -511.91 * True Reward -511.91 * time 1.67 * step 117\n",
      "Ep 1 * AvgReward -325.95 * true AvgReward -325.95 * Reward -139.98 * True Reward -139.98 * time 2.73 * step 318\n",
      "Ep 2 * AvgReward -384.90 * true AvgReward -384.90 * Reward -502.80 * True Reward -502.80 * time 1.86 * step 450\n",
      "Ep 3 * AvgReward -311.50 * true AvgReward -311.50 * Reward -91.31 * True Reward -91.31 * time 1.84 * step 595\n",
      "Ep 4 * AvgReward -260.22 * true AvgReward -260.22 * Reward -55.12 * True Reward -55.12 * time 1.03 * step 683\n",
      "Ep 5 * AvgReward -237.17 * true AvgReward -237.17 * Reward -121.90 * True Reward -121.90 * time 2.18 * step 851\n",
      "Ep 6 * AvgReward -221.12 * true AvgReward -221.12 * Reward -124.81 * True Reward -124.81 * time 1.18 * step 948\n",
      "Ep 7 * AvgReward -231.94 * true AvgReward -231.94 * Reward -307.67 * True Reward -307.67 * time 1.03 * step 1039\n",
      "Ep 8 * AvgReward -214.54 * true AvgReward -214.54 * Reward -75.36 * True Reward -75.36 * time 0.81 * step 1108\n",
      "Ep 9 * AvgReward -198.89 * true AvgReward -198.89 * Reward -58.09 * True Reward -58.09 * time 0.82 * step 1176\n",
      "Ep 10 * AvgReward -210.61 * true AvgReward -210.61 * Reward -327.73 * True Reward -327.73 * time 1.51 * step 1311\n",
      "Ep 11 * AvgReward -214.23 * true AvgReward -214.23 * Reward -254.04 * True Reward -254.04 * time 1.06 * step 1403\n",
      "Ep 12 * AvgReward -208.83 * true AvgReward -208.83 * Reward -144.11 * True Reward -144.11 * time 1.82 * step 1558\n",
      "Ep 13 * AvgReward -201.88 * true AvgReward -201.88 * Reward -111.51 * True Reward -111.51 * time 1.70 * step 1692\n",
      "Ep 14 * AvgReward -191.30 * true AvgReward -191.30 * Reward -43.16 * True Reward -43.16 * time 0.92 * step 1764\n",
      "Ep 15 * AvgReward -182.19 * true AvgReward -182.19 * Reward -45.49 * True Reward -45.49 * time 1.02 * step 1836\n",
      "Ep 16 * AvgReward -166.46 * true AvgReward -166.46 * Reward 85.18 * True Reward 85.18 * time 15.43 * step 2836\n",
      "Ep 17 * AvgReward -171.52 * true AvgReward -171.52 * Reward -257.59 * True Reward -257.59 * time 1.85 * step 3001\n",
      "Ep 18 * AvgReward -172.85 * true AvgReward -172.85 * Reward -196.70 * True Reward -196.70 * time 1.08 * step 3078\n",
      "Ep 19 * AvgReward -165.07 * true AvgReward -165.07 * Reward -17.32 * True Reward -17.32 * time 1.67 * step 3198\n",
      "Ep 20 * AvgReward -156.17 * true AvgReward -156.17 * Reward -333.79 * True Reward -333.79 * time 1.38 * step 3315\n",
      "Ep 21 * AvgReward -158.73 * true AvgReward -158.73 * Reward -191.31 * True Reward -191.31 * time 1.20 * step 3419\n",
      "Ep 22 * AvgReward -141.07 * true AvgReward -141.07 * Reward -149.62 * True Reward -149.62 * time 2.21 * step 3596\n",
      "Ep 23 * AvgReward -149.58 * true AvgReward -149.58 * Reward -261.49 * True Reward -261.49 * time 1.06 * step 3699\n",
      "Ep 24 * AvgReward -166.19 * true AvgReward -166.19 * Reward -387.36 * True Reward -387.36 * time 0.97 * step 3793\n",
      "Ep 25 * AvgReward -165.64 * true AvgReward -165.64 * Reward -110.88 * True Reward -110.88 * time 1.05 * step 3881\n",
      "Ep 26 * AvgReward -158.93 * true AvgReward -158.93 * Reward 9.50 * True Reward 9.50 * time 2.47 * step 4088\n",
      "Ep 27 * AvgReward -153.59 * true AvgReward -153.59 * Reward -200.94 * True Reward -200.94 * time 0.80 * step 4159\n",
      "Ep 28 * AvgReward -165.65 * true AvgReward -165.65 * Reward -316.48 * True Reward -316.48 * time 1.09 * step 4251\n",
      "Ep 29 * AvgReward -166.36 * true AvgReward -166.36 * Reward -72.31 * True Reward -72.31 * time 0.87 * step 4326\n",
      "Ep 30 * AvgReward -155.52 * true AvgReward -155.52 * Reward -110.93 * True Reward -110.93 * time 1.66 * step 4460\n",
      "Ep 31 * AvgReward -157.73 * true AvgReward -157.73 * Reward -298.20 * True Reward -298.20 * time 1.18 * step 4560\n",
      "Ep 32 * AvgReward -165.71 * true AvgReward -165.71 * Reward -303.86 * True Reward -303.86 * time 1.78 * step 4716\n",
      "Ep 33 * AvgReward -166.49 * true AvgReward -166.49 * Reward -126.98 * True Reward -126.98 * time 1.77 * step 4867\n",
      "Ep 34 * AvgReward -168.73 * true AvgReward -168.73 * Reward -88.05 * True Reward -88.05 * time 1.85 * step 5019\n",
      "Ep 35 * AvgReward -175.76 * true AvgReward -175.76 * Reward -186.02 * True Reward -186.02 * time 1.86 * step 5171\n",
      "Ep 36 * AvgReward -183.98 * true AvgReward -183.98 * Reward -79.32 * True Reward -79.32 * time 1.48 * step 5297\n",
      "Ep 37 * AvgReward -182.57 * true AvgReward -182.57 * Reward -229.28 * True Reward -229.28 * time 1.16 * step 5391\n",
      "Ep 38 * AvgReward -174.31 * true AvgReward -174.31 * Reward -31.57 * True Reward -31.57 * time 0.99 * step 5476\n",
      "Ep 39 * AvgReward -178.32 * true AvgReward -178.32 * Reward -97.56 * True Reward -97.56 * time 0.88 * step 5550\n",
      "Ep 40 * AvgReward -160.12 * true AvgReward -160.12 * Reward 30.24 * True Reward 30.24 * time 1.20 * step 5654\n",
      "Ep 41 * AvgReward -162.46 * true AvgReward -162.46 * Reward -238.01 * True Reward -238.01 * time 1.01 * step 5745\n",
      "Ep 42 * AvgReward -159.38 * true AvgReward -159.38 * Reward -88.02 * True Reward -88.02 * time 0.92 * step 5822\n",
      "Ep 43 * AvgReward -155.05 * true AvgReward -155.05 * Reward -174.97 * True Reward -174.97 * time 1.07 * step 5922\n",
      "Ep 44 * AvgReward -152.54 * true AvgReward -152.54 * Reward -337.23 * True Reward -337.23 * time 0.97 * step 6014\n",
      "Ep 45 * AvgReward -152.07 * true AvgReward -152.07 * Reward -101.47 * True Reward -101.47 * time 0.95 * step 6096\n",
      "Ep 46 * AvgReward -168.57 * true AvgReward -168.57 * Reward -320.35 * True Reward -320.35 * time 1.26 * step 6214\n",
      "Ep 47 * AvgReward -181.85 * true AvgReward -181.85 * Reward -466.67 * True Reward -466.67 * time 1.88 * step 6369\n",
      "Ep 48 * AvgReward -175.50 * true AvgReward -175.50 * Reward -189.43 * True Reward -189.43 * time 1.61 * step 6504\n",
      "Ep 49 * AvgReward -173.57 * true AvgReward -173.57 * Reward -33.77 * True Reward -33.77 * time 1.34 * step 6624\n",
      "Ep 50 * AvgReward -177.60 * true AvgReward -177.60 * Reward -191.41 * True Reward -191.41 * time 1.94 * step 6784\n",
      "Ep 51 * AvgReward -164.26 * true AvgReward -164.26 * Reward -31.44 * True Reward -31.44 * time 0.92 * step 6859\n",
      "Ep 52 * AvgReward -151.94 * true AvgReward -151.94 * Reward -57.54 * True Reward -57.54 * time 1.09 * step 6951\n",
      "Ep 53 * AvgReward -149.47 * true AvgReward -149.47 * Reward -77.47 * True Reward -77.47 * time 1.01 * step 7044\n",
      "Ep 54 * AvgReward -161.27 * true AvgReward -161.27 * Reward -324.16 * True Reward -324.16 * time 1.24 * step 7131\n",
      "Ep 55 * AvgReward -167.66 * true AvgReward -167.66 * Reward -313.79 * True Reward -313.79 * time 1.88 * step 7277\n",
      "Ep 56 * AvgReward -171.44 * true AvgReward -171.44 * Reward -154.98 * True Reward -154.98 * time 1.03 * step 7364\n",
      "Ep 57 * AvgReward -175.38 * true AvgReward -175.38 * Reward -308.08 * True Reward -308.08 * time 1.11 * step 7460\n",
      "Ep 58 * AvgReward -178.70 * true AvgReward -178.70 * Reward -97.92 * True Reward -97.92 * time 1.66 * step 7605\n",
      "Ep 59 * AvgReward -179.95 * true AvgReward -179.95 * Reward -122.46 * True Reward -122.46 * time 0.72 * step 7668\n",
      "Ep 60 * AvgReward -187.13 * true AvgReward -187.13 * Reward -113.40 * True Reward -113.40 * time 1.67 * step 7813\n",
      "Ep 61 * AvgReward -178.20 * true AvgReward -178.20 * Reward -59.37 * True Reward -59.37 * time 0.89 * step 7886\n",
      "Ep 62 * AvgReward -180.96 * true AvgReward -180.96 * Reward -143.21 * True Reward -143.21 * time 0.95 * step 7973\n",
      "Ep 63 * AvgReward -187.46 * true AvgReward -187.46 * Reward -304.98 * True Reward -304.98 * time 1.02 * step 8062\n",
      "Ep 64 * AvgReward -197.07 * true AvgReward -197.07 * Reward -529.45 * True Reward -529.45 * time 1.68 * step 8203\n",
      "Ep 65 * AvgReward -197.21 * true AvgReward -197.21 * Reward -104.39 * True Reward -104.39 * time 1.07 * step 8292\n",
      "Ep 66 * AvgReward -199.33 * true AvgReward -199.33 * Reward -362.69 * True Reward -362.69 * time 1.28 * step 8398\n",
      "Ep 67 * AvgReward -191.22 * true AvgReward -191.22 * Reward -304.50 * True Reward -304.50 * time 1.20 * step 8502\n",
      "Ep 68 * AvgReward -198.71 * true AvgReward -198.71 * Reward -339.15 * True Reward -339.15 * time 1.07 * step 8598\n",
      "Ep 69 * AvgReward -221.86 * true AvgReward -221.86 * Reward -496.75 * True Reward -496.75 * time 1.13 * step 8700\n",
      "Ep 70 * AvgReward -216.98 * true AvgReward -216.98 * Reward -93.90 * True Reward -93.90 * time 1.02 * step 8785\n",
      "Ep 71 * AvgReward -215.95 * true AvgReward -215.95 * Reward -10.81 * True Reward -10.81 * time 1.89 * step 8938\n",
      "Ep 72 * AvgReward -218.92 * true AvgReward -218.92 * Reward -116.93 * True Reward -116.93 * time 1.21 * step 9038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 73 * AvgReward -222.76 * true AvgReward -222.76 * Reward -154.28 * True Reward -154.28 * time 1.67 * step 9178\n",
      "Ep 74 * AvgReward -218.08 * true AvgReward -218.08 * Reward -230.52 * True Reward -230.52 * time 1.86 * step 9337\n",
      "Ep 75 * AvgReward -213.11 * true AvgReward -213.11 * Reward -214.52 * True Reward -214.52 * time 1.05 * step 9428\n",
      "Ep 76 * AvgReward -230.08 * true AvgReward -230.08 * Reward -494.23 * True Reward -494.23 * time 2.12 * step 9616\n",
      "Ep 77 * AvgReward -218.37 * true AvgReward -218.37 * Reward -73.90 * True Reward -73.90 * time 0.79 * step 9690\n",
      "Ep 78 * AvgReward -232.00 * true AvgReward -232.00 * Reward -370.56 * True Reward -370.56 * time 1.10 * step 9783\n"
     ]
    }
   ],
   "source": [
    "run(total_trials=2, total_episodes=1000, buffer_capacity=300000, tau=0.001, critic_lr=0.0002, \n",
    "    actor_lr=0.0001, start_steps=10000, continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc15eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test(render=True, actor_weights='Weights/actor-trial1_LunarLander-v2_2500_50000_64_0.3_0.003_0.002_0.99_0.005_1_fixed_fixed_fixed_fixed_fixed_fixed_20_False_200_True_10000_-10.0_False_15.0_1.5_2022.8.1.8.55.0.h5', continuous=True, total_episodes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7440126e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
